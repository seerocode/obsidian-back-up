 @import url(https://themes.googleusercontent.com/fonts/css?kit=sCR6ORJJ\_1QHAOsqev1h5GRYh9syjTFm17fR6pfMaDM);

Updated: 2023-04-05T16:10:16.000Z

# Toward User-Driven Algorithm Auditing: Investigating users’ strategies for uncovering harmful algorithmic behavior

devos2022userdriven

Alicia DeVos, Aditi Dhabalia, Hong Shen, Kenneth Holstein, and Motahhare Eslami. 2022. Toward User-Driven Algorithm Auditing: Investigating users’ strategies for uncovering harmful algorithmic behavior. In \_Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems\_ (CHI ’22), Association for Computing Machinery, New York, NY, USA, 1–19. DOI:<https://doi.org/10.1145/3491102.3517441>

### Relevance to recikCGKqx5PlZq9X:

It covers how people's situated knowledge helps them uncover algorithmic harms and how to move their work forward and due to the journals, there is some sense of users doing more of an organic audit

### Summary:

* * *

### Abstract Breakdown:

### Thesis/Main Research Question(s):

\*\*How do people find, make sense of, and evaluate potentially harmful algorithmic behavior?\*\*

### Motivation:

Gap: There is limited literature on: (1) understanding what makes user-driven algorithm auditing effective and how people go about searching for and making sense of harmful algorithmic behavior (2) what support could be provided for users to make their auditing more effective.  

Background: "the absence of the actual context of use and everyday users in the auditing process can still result in major blindspots in practice. As well as experts’ cultural blindspots, social dynamics, changing norms, and new circumstances can hinder experts’ detection of many types of algorithmic biases and harms". Past literature has generally focused on expert-led algorithm auditing approaches that are performed outside the context of everyday use of an algorithmic system. However, this approach may fail when those conducting an audit lack the relevant cultural background and lived experience to recognize or know where to look for harmful behaviors

### Contribution:

The authors put forward a three-stage model capturing how users uncover algorithmic harms:

1\. Search inspiration - > Two main themes related to search inspiration emerged through our analysis: noticing patterns (or lack thereof) and drawing upon prior exposures to societal biases.

2\. sensemaking -> involves the ways that participants understood and evaluated algorithmic behaviors that might be harmfully biased, and 3) remediation covers participant actions and desires to mitigate the ramifications of harmful algorithmic bias. Connected to each stage of this process, we uncovered two common influences on the process across participants: knowledge and beliefs, and platform affordances. Participants used their prior knowledge and beliefs related to biased and harmful algorithmic behavior (i.e., their conception of bias and harm, exposure and experience, expectations and values, and folk theories) to make sense of problematic algorithmic behaviors they encountered. Participants’ abilities to detect and make sense of such behaviors were enabled and guided by platform affordances.

3\. Remediation -> The third stage describes the actions that people take to avoid or counter harmful biases.

### Methods: Diaries

### Methodology:

Authors recruited 23 participants via social media, community mailing lists, and job postings.

\- Authors conducted a three-phase study with 23 everyday users of algorithmic system

    - In the first phase, they conducted think-aloud interview sessions in which individual participants were tasked with searching for images that the authors believed displayed biased results such as a) a librarian, b) a thug, c) one person nagging another, d) a beautiful woman, and e) a wedding and asked to interpret the results as a way to \*\*evaluate existing biases\*\*. Secondly, participants were asked to find new examples of image search queries that might yield problematic image search results \*\*\_(they were probed to do this)\_\*\*.

    - participants then took part in a 14-day diary study in which they were asked to report potentially harmful algorithmic behaviors that they found both through active searching and incidentally during the course of their day-to-day interactions with algorithmic system

    - Finally, we invited participants to participate in group workshop sessions and work collectively to interpret and discuss some of the cases that were uncovered during the diary study toward a) \*\*understanding how people work together to evaluate user reports of potentially harmful algorithmic behaviors\*\* and b) \*\*beginning to understand how we might design to facilitate such collective activities. They\*\* workshop participants to evaluate these individual user reports and work together to create a collective report that provides insights for someone who might be able to act on them.

### Results/Findings:

The authors found that the strategies users employ for auditing are\*\* heavily guided by their personal experiences with and exposures to societal bias\*\* and \*\*collective sensemaking\*\* amongst multiple users is invaluable in user-driven algorithm audits.

participants submitted a total of 160 evidence-supported reports of potential biases or other harmful algorithmic behaviors in the diary study, spanning gender, sexual orientation, race/ethnicity, socioeconomic class, age, body type, disability, religion, politics, and combinations of these.

Participants were able to find additional potential biases in image search results that the authors hadn't previously identified.

\*\*Almost all participants (n=22) revealed that they held pre-existing knowledge and beliefs about biased and harmful algorithmic behavior that influenced the search and sensemaking process.\*\*

Most participants (n=18) mentioned folk theories about why algorithms behave in the ways that they do, which informed their search and sensemaking process.

Most participants (n=20) anticipated how an algorithm would behave in a particular situation, forming expectations about algorithmic behavior. In addition to predictions of how an algorithm will behave, participants also expressed normative expectations, or thoughts about how an algorithm should behave, which they linked to their general values. Participants began searching with expectations, then compared search results with their expectations to refine their conceptions of bias and harm (n=17).

All three stages of participants’ bias search and sensemaking process — search inspiration, sensemaking, and remediation — appeared to be influenced by platform affordances such as autocomplete on google image search.

participants often reflected on missing contextual information needed to properly evaluate a given user report. In some cases, participants recognized that they lacked crucial information needed to determine whether a reported issue represented biased and/or harmful algorithmic behavior

Participants used prior knowledge and beliefs about biased and harmful algorithmic behavior to make sense of potentially harmful algorithmic behaviors they encountered, and they gained new knowledge and beliefs along the way. Participant capabilities were made possible and guided by platform affordances such as aggregate views in interface presentations and autocomplete suggestions by search engines. In the next sections, we first present a high-level description of the influences, then delve into reporting of different stages of the process

resistance, awareness, representation improvement, exterting pressure, communication between users and platforms

### Limitations/Weaknesses:

\- What question was asked to recruit people?

\- recognized distinct challenge involves situations in which those targeted and potentially harmed by algorithms are not the the direct users who interact with these systems day-to-day. such as welfare ADS

\- They don't really go into a deep comparison or make explicit how exactly user's strategies  are often able to be so effective as they indicated in their introduction

\- They briefly covered the different types of auditing even though a big part of their argument is how user-driven auditing is more effective than formal methods but how they are more effective isn't really compared and in what ways they are more effective (where specifically users could bolster those audits).

\- What constitutes effectiveness?

### Future Work:

\- Assigning people to particular auditing roles based on their specific exposure and experience could boost future auditing efforts

\- We can leverage our knowledge that platform affordances influence the process by providing straightforward ways to report bias, either to the platform or beyond, through the design and mechanisms of the platform itself.

\- algorithmic counterpublics: There is great opportunity in encouraging users to come together within their auditing activities. Though users can and do individually audit algorithms, oftentimes it is only together that can they conduct tests as different, authentic users to gather robust evidence. Only together can they discuss and provide general consensus or disagreement, or raise awareness at the scale required to put pressure on companies and hold them accountable.

\- not all algorithmic behaviors are equally visible, and sometimes people are harmed by algorithmic behaviors without their knowledge. In this paper we have largely focused on harmful algorithmic behaviors that people have opportunities to encounter in their day-to-day lives and that (at least some) people can plausibly be expected to recognize as harmful. Not all algorithmic harms fit this description. The creation of new platforms like these apps, with affordances that support collective investigation and sensemaking, can provide avenues for the recognition of harmful algorithmic behaviors that may otherwise remain invisible.

The author argues that users on a platform would need to be primed to some extent like they did with the google search image tasks and recognized risks in doing so as it could influence their decisioning and sensemaking behavior

When fixing bias is neither desirable nor possible, promoting awareness of algorithmic biases and other harmful behaviors can be especially valuable. User awareness helps users adapt their behaviors in and around algorithmic systems, in turn fostering userdriven auditing and supporting remediation of harmful issues by other means. -> refusal instead of repair

"At what point in a harmful algorithm’s existence do we mitigate the harm by stopping the algorithm altogether rather than just attempting to mitigate specific issues?"