---
year: 2021
title: Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors
authors: Hong Shen, Alicia DeVos, Motahhare Eslami, Kenneth Holstein
tag: literaturenote, zotero
---

### Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors 

``` ad-info
title: Citation
[1]

Hong Shen, Alicia DeVos, Motahhare Eslami, and Kenneth Holstein. 2021. Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors. _Proc. ACM Hum.-Comput. Interact._ 5, CSCW2 (October 2021), 433:1-433:29. DOI:[https://doi.org/10.1145/3479577](https://doi.org/10.1145/3479577)
```

``` ad-info
title: Metadata
- **Title**: Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors
- **CiteKey**: shen2021everyday
- **Author**: Hong Shen, Alicia DeVos, Motahhare Eslami, Kenneth Holstein
- **Year**: 1634529600000 
- **Type**: journalArticle 
- **DOI**: 10.1145/3479577
```

```ad-quote
title: Abstract
A growing body of literature has proposed formal approaches to audit algorithmic systems for biased and harmful behaviors. While formal auditing approaches have been greatly impactful, they often suffer major blindspots, with critical issues surfacing only in the context of everyday use once systems are deployed. Recent years have seen many cases in which everyday users of algorithmic systems detect and raise awareness about harmful behaviors that they encounter in the course of their everyday interactions with these systems. However, to date little academic attention has been granted to these bottom-up, user-driven auditing processes. In this paper, we propose and explore the concept of everyday algorithm auditing, a process in which users detect, understand, and interrogate problematic machine behaviors via their day-to-day interactions with algorithmic systems. We argue that everyday users are powerful in surfacing problematic machine behaviors that may elude detection via more centrally-organized forms of auditing, regardless of users' knowledge about the underlying algorithms. We analyze several real-world cases of everyday algorithm auditing, drawing lessons from these cases for the design of future platforms and tools that facilitate such auditing behaviors. Finally, we discuss work that lies ahead, toward bridging the gaps between formal auditing approaches and the organic auditing behaviors that emerge in everyday use of algorithmic systems.
```

```ad-abstract
title: Files and Links
- **Ref**: [[@shen2021everyday]]
- **DOI**: 10.1145/3479577
- **Url**: https://doi.org/10.1145/3479577
- **Uri**: http://zotero.org/users/10910988/items/M7LBVZ82
- **File**: 
- **Local Library**: [Zotero]()
- **PDF**: [Zotero]([shen2021everyday_Everyday Algorithm Auditing.pdf](zotero://select/library/items/2ICHUGFG))
```

```ad-note
title: Tags and Collections
- **Keywords**: 
- **Collections**: 
```

### <mark style='background:#4b6584'>Notes</mark> 


#### Interesting

##### Interesting, Page 2, Yellow, Highlight, 2023-03-21:

> literature often occur outside the context of everyday use of an
> algorithmic system or platform: auditors, such as researchers,
> algorithmic experts, and activists, initiate, conduct, or
> orchestrate the auditing process

More of a systematic approach

***

##### Interesting, Page 2, Yellow, Highlight, 2023-03-21:

> hey often fail to surface critical issues

***

##### Interesting, Page 2, Yellow, Highlight, 2023-03-21:

> everyday algorithm auditing: a process in which users detect,
> interrogate, and understand problematic machine behaviors via
> their daily interactions with algorithmic system

***

##### Interesting, Page 2, Yellow, Highlight, 2023-03-21:

> everyday algorithm auditing occurs in the context of “everyday
> use” of an algorithmic system or platform

***

##### Interesting, Page 4, Yellow, Highlight, 2023-03-21:

> most algorithm auditing approaches that have been discussed in
> the literature require individuals with some level of technical
> expertise to initiate, conduct, or direct the entire process.

***

##### Interesting, Page 4, Yellow, Highlight, 2023-03-21:

> still the researchers’ responsibility to design and initiate the
> audit by assigning crowdworkers speci
c tasks, and then to
> convert the crowd’s outputs on distributed tasks into meaningful
> insights about an algorithmic system’s behavior. However, such
> centrally-organized, formal audits often fail to surface serious
> issues that everyday users of algorithmic systems are quickly
> able to detect once a system is deployed in the wild.

***

##### Interesting, Page 7, Yellow, Highlight, 2023-03-21:

> bottom-up, user-driven auditing behavior

***

##### Interesting, Page 7, Yellow, Highlight, 2023-03-21:

> Sometimes, everyday audits are conducted by users who have a
> high degree of relevant technical knowledge. For example,
> Sweeney,

***

##### Interesting, Page 7, Yellow, Highlight, 2023-03-21:

> In other cases, everyday users may have little algorithmic
> expertise. For example, many small business owners collectively
> initiated an everyday audit of Yelp’s algorithm after noticing
> that positive reviews were not showing prominently on their
> business pages [29].

***

##### Interesting, Page 7, Yellow, Highlight, 2023-03-21:

> Collectiveness: Everyday algorithm audits can also vary in the
> extent to which users work together collectively. Some cases are
> more individually led, such as the case when Alciné discovered
> that Google’s image recognition algorithm labelled some

***

##### Interesting, Page 7, Yellow, Highlight, 2023-03-21:

> It is worth noting that having a large number of users involved
> does not always mean an everyday audit is collective.

***

##### Interesting, Page 8, Yellow, Highlight, 2023-03-21:

> Organicness: Though everyday algorithm auditing is based on
> everyday use, in practice the process can be more or less
> organic.

***

##### Interesting, Page 8, Yellow, Highlight, 2023-03-21:

> By contrast, other everyday audits start organically but later
> turn highly organized.

***

##### Interesting, Page 8, Yellow, Highlight, 2023-03-21:

> Still other everyday audits are organic throughout the entire
> process.

***

##### Interesting, Page 8, Yellow, Highlight, 2023-03-21:

> they foreground a di
erent type of resistance that is less
> visible and more incidental but nevertheless constantly
> contesting the existing power relations in everyday forms

Resistance as power

***

##### Interesting, Page 9, Yellow, Highlight, 2023-03-21:

> [97] discussed how 
nstas — secondary Instagram accounts that
> feature content often unacceptable to users’ primary accounts —
> have developed forms of counterpublics for young people to share
> emotional or vulnerable content with their close friends.

finsta as counter public

***

##### Interesting, Page 16, Yellow, Highlight, 2023-03-21:

> Everyday audits are often initiated by incidental exposure to
> problematic machine behaviors: in other words, users may not
> necessarily intend to conduct an audit from the start

***

#### Questions

##### Questions, Page 9, Blue, Highlight, 2023-03-21:

> have developed forms of counterpublics

How do we do this in organic audits! #Ideas

***

#### Look Up

##### Look Up, Page 2, Green, Highlight, 2023-03-21:

> (e.g., [12, 65, 84, 100])

***

##### Look Up, Page 2, Green, Highlight, 2023-03-21:

> social or cultural contexts [17, 47, 78, 99

***

##### Look Up, Page 3, Green, Highlight, 2023-03-21:

> “parallel discursive arenas” [32], where communities impacted by
> harmful algorithmic behaviors come together and participate in
> their own forms of collective sensemaking, hypothesis
> development, and algorithm bias detection.

What users are doing during organic audits

***

##### Look Up, Page 4, Green, Highlight, 2023-03-21:

> assessing the alignment of deployed algorithmic systems with
> laws and regulations, societal values, ethical desiderata, or
> industry standards (e.g., [12, 25, 70, 74]).

***

##### Look Up, Page 4, Green, Highlight, 2023-03-21:

> internal algorithm audits, conducted by ML teams themselves,
> with the aim of detecting and mitigating misalignments prior to
> deployment (e.g., [17, 48, 57, 71]).

***

##### Look Up, Page 4, Green, Highlight, 2023-03-21:

> Sandvig et al. [74] proposed a taxonomy to summarize di
erent
> algorithm auditing methods and research designs, including (1)
> code audits, (2) noninvasive user audits, (3) scraping audits,
> (4) sock puppet audits, and (5) crowdsourced/collaborative
> audits.

***

##### Look Up, Page 4, Green, Highlight, 2023-03-21:

> or example, certain algorithmic behaviors may only arise — or
> may only be recognized as harmful — when a system is used in the
> presence of particular real-world social or cultural dynamics
> [48, 60, 78, 80]

***

##### Look Up, Page 5, Green, Highlight, 2023-03-21:

> Other harmful behaviors may only emerge when a system is used in
> unanticipated ways or in unanticipated contexts, perhaps due to
> changing norms and practices surrounding the use of a given
> algorithmic system over time [17, 80].

***

##### Look Up, Page 5, Green, Highlight, 2023-03-21:

> broad notion of “emergent bias,” discussed in Friedman and
> Nissenbaum’s seminal work on bias in computer systems [33].

***

##### Look Up, Page 5, Green, Highlight, 2023-03-21:

> [83],

***

##### Look Up, Page 8, Green, Highlight, 2023-03-21:

> an audit [28],

Look up! These are closed audits

***

##### Look Up, Page 8, Green, Highlight, 2023-03-21:

> However, after the tool was released to the public, users had
> autonomy in deciding how to use it. Collective auditing
> behaviors emerged organically through discussions on social
> media, as users shared 
ndings and hypotheses and sometimes
> built upon each other’s explorations [19, 62].

#Ideas

***

##### Look Up, Page 8, Green, Highlight, 2023-03-21:

> he proposes that counterpublics can be understood as “parallel
> discursive arenas” [32], where members of often

***

##### Look Up, Page 9, Green, Highlight, 2023-03-21:

> In everyday algorithm auditing, we saw similar counterpublics
> emerging. Indeed, many in
u- ential cases started with
> individual users complaining about certain socially harmful
> machine results in their everyday lives, but ended up with a
> large group of users working together for a collective and
> collaborative action. For example, in 2019 a group of YouTubers
> came together to show that the YouTube recommendation algorithm
> demonetizes LGBTQ+ content, resulting in a huge loss of
> advertising revenue for LGBTQ+ content creators [73]

***

##### Look Up, Page 17, Green, Highlight, 2023-03-21:

> folk theories”: non-authoritative theories a user forms to
> explain and understand how a technological system works [23,
> 27].

***

##### Look Up, Page 21, Green, Highlight, 2023-03-21:

> 13, 26]

***

##### Look Up, Page 21, Green, Highlight, 2023-03-21:

> As a foundation for such interfaces, it may be possible to build
> upon emerging algorithmic techniques for crowd-in-the-loop
> detection of “unknown unknowns” in ML models (e.g., [6, 56, 59,
> 64, 87, 101])

***

##### Look Up, Page 23, Green, Highlight, 2023-03-21:

> Holstein et al. (2019) found that commercial product teams,
> fearful of negative public attention, are often motivated to
> uncover harmful algorithmic behaviors as early in the process as
> possible

***

##### Look Up, Page 23, Green, Highlight, 2023-03-21:

> Acknowledging the limitations of existing auditing approaches,
> several industry practitioners interviewed in this research
> expressed interest in im- plementing mechanisms for user-led
> auditing [48]. Some product teams were motivated enough to
> experiment with developing their own collective auditing tools
> and processes.

***

##### Look Up, Page 23, Green, Highlight, 2023-03-21:

> public awareness raising around harmful behaviors in commercial
> AI systems can motivate target companies to prioritize
> remediation [70].

***

#### Results

##### Results, Page 3, Purple, Highlight, 2023-03-21:

> Analyzing these cases and comparing their dynamics led us to a
> process-oriented view of everyday algorithm audits encompassing
> (1) initiation of an audit to (2) raising awareness of observed
> issues to (3) hypothesizing about observed behaviors and testing
> an algorithmic system to ideally (4) some form of remediation.

***

##### Results, Page 3, Purple, Highlight, 2023-03-21:

> We outline 
ve broad categories of potential design
> interventions to support everyday audits — (a) community
> guidance, (b) expert guidance, (c) algorithmic guidance, (d)
> organizational guidance, and (e) incentivization — and discuss
> potential design trade-o
s

***

##### Results, Page 5, Purple, Highlight, 2023-03-21:

> everyday audits are more collectively organized and organic, and
> require less algorithmic expertise on the part of those
> organizing and enacting the audit.

***

##### Results, Page 5, Purple, Highlight, 2023-03-21:

> Users’ autonomy and agency are central to the concept of
> everyday algorithm auditing.

***

##### Results, Page 5, Purple, Highlight, 2023-03-21:

> We de
ne everyday algorithm auditing as the ways everyday users
> detect, understand, and/or interrogate problematic machine
> behaviors via their day-to-day interactions with algorithmic
> systems.

***

##### Results, Page 5, Purple, Highlight, 2023-03-21:

> Following [74], we use “auditing” to refer to behaviors
> undertaken, whether formally or informally, to test and
> investigate whether a given algorithmic system operates in a
> socially harmful way, such as behaving in ways that produce
> inequitable outcomes along lines of class, race, or gender.

***

##### Results, Page 7, Purple, Highlight, 2023-03-21:

> we aim to bridge the gap between algorithmic auditing approaches
> proposed in the academic research literature and auditing
> behaviors that users exhibit day-to-day.

***

##### Results, Page 10, Purple, Highlight, 2023-03-21:

> our aim is to begin formalizing this concept, to help guide
> future empirical and design research in this space.

***

##### Results, Page 19, Purple, Highlight, 2023-03-21:

> Below we outline 
ve broad categories of potential design
> interventions — (a) community guid- ance, (b) expert guidance,
> (c) algorithmic guidance, (d) organizational guidance, and (e)
> incentiviza- tion — and discuss potential design trade-o
s.

***

##### Results, Page 20, Purple, Highlight, 2023-03-21:

> One way designers might support collective auditing behavior is
> to o
er mechanisms that help everyday auditors guide each
> other’s e
orts. At minimum, this might include providing spaces
> for community discussion so that everyday auditors can discuss,
> raise awareness about, and build upon each other’s 
ndings, as
> we have already observed in the Yelp and Twitter cases

***

##### Results, Page 20, Purple, Highlight, 2023-03-21:

> Beyond simply providing spaces for discussion, we might imagine
> tailoring or augmenting discussion channels with mechanisms that
> are explicitly designed to support everyday auditing. For
> example, a discussion platform designed for everyday auditing
> might allow community members to upvote speci
c algorithmic
> behaviors that other community members have reported, in order
> to collectively surface the most severe reports, or reports that
> may bene
t from further discussion.

Discussion spaces designed for everyday audits

***

##### Results, Page 20, Purple, Highlight, 2023-03-21:

> Furthermore, when an everyday auditor selects a particular
> report, the discussion platform might provide an overview of
> relevant hypotheses and 
ndings that other auditors have
> generated so far, to help the auditor build upon previous
> contributions instead of retreading old ground.

Platform ideas

***

##### Results, Page 20, Purple, Highlight, 2023-03-21:

> we should also consider the possibilities of leveraging existing
> platforms rather than creating entirely new ones

***

##### Results, Page 20, Purple, Highlight, 2023-03-21:

> A potential trade-o
 is that hosting community discussion within
> the same platforms that are under audit may lead to concerns
> about platform-driven censorship or suppression, given uneven
> power dynamics and the potential for value con
icts between
> justice-oriented audits and pro
t-oriented platforms. We
> anticipate that in certain cases, everyday auditors may feel
> more comfortable hosting these discussions elsewhere.

***

##### Results, Page 20, Purple, Highlight, 2023-03-21:

> We expect that everyday algorithm audits can bene
t from expert
> input, whether from system developers, researchers, or
> regulators. However, it remains an open question what forms and
> degrees of expert engagement may be most helpful in facilitating
> everyday audits

#Ideas

***

##### Results, Page 21, Purple, Highlight, 2023-03-21:

> To help steer everyday auditors’ ongoing e
orts in more
> productive directions, designers might invite relevant experts
> to continuously monitor community discussions and intervene as
> needed [13, 26].

***

##### Results, Page 21, Purple, Highlight, 2023-03-21:

> f this is not handled carefully, experts’ involvement may risk
> diminishing everyday auditors’ sense of community and autonomy

Limitations of expert involvement

***

##### Results, Page 21, Purple, Highlight, 2023-03-21:

> Relevant experts may also provide guidance to users at the
> initiation stage. For example, to complement their own internal
> auditing e
orts, a product team may actively draw users’
> attention to suspected issues or components of a system that
> they would like users to audit, then suggest testing strategies
> for users to try [48].

***

##### Results, Page 21, Purple, Highlight, 2023-03-21:

> The design of bidirectional feedback mechanisms between
> developers and everyday users of algorithmic systems represents
> a critical direction for future research. In addition to the
> need for systems that help developers provide e
ective feedback
> and guidance to users who are engaged in everyday auditing
> behaviors, there are opportunities to design systems that
> sca
old users in providing more useful, actionable feedback to
> developers.

#Ideas

***

##### Results, Page 21, Purple, Highlight, 2023-03-21:

> For certain auditing tasks, such as exploring a system’s
> behavior, gathering evidence, or testing hypotheses, everyday
> auditors may bene
t from algorithmic assistance

***

##### Results, Page 21, Purple, Highlight, 2023-03-21:

> These methods focus on surfacing regions of a model’s error
> space in which the model is highly con
dent yet incorrect [56].
> This form of algorithmic guidance could supplement guidance that
> users may receive from each other (e.g., via upvoting
> mechanisms) and from expert guidance (e.g., suggestions for how
> everyday auditors should direct their search e
orts),
> potentially surfacing cases that these other forms of guidance
> would miss.

***

##### Results, Page 21, Purple, Highlight, 2023-03-21:

> We distinguish between two major categories of auditing
> behaviors: “instance-based” and “comparison-based” auditing.
> Instance-based auditing applies to cases where the observation
> of an isolated harmful algorithmic behavior is su
cient to
> ground an argument for remediation.

Instance based auditing

***

##### Results, Page 21, Purple, Highlight, 2023-03-21:

> By contrast, comparison-based auditing applies to cases where,
> in order to establish that a harmful bias truly exists and is
> worth addressing, it is necessary to compare multiple instances
> and demonstrate the existence of statistical patterns.

Comparison auditing

***

##### Results, Page 22, Purple, Highlight, 2023-03-21:

> Designers could help guide the division of labor through
> interventions that encourage everyday auditors to take on
> particular roles during an auditing process, including roles
> that are expected to be important and might not arise
> organically.

Design for organizational guidance

***

##### Results, Page 22, Purple, Highlight, 2023-03-21:

> For example, one potential design direction is to o
er an
> “algorithmic bias bounty program,” drawing inspiration from “bug
> bounty” programs in cybersecurity

Incentivization #ideas

***

##### Results, Page 22, Purple, Highlight, 2023-03-21:

> Similarly, in the context of everyday algorithm audits, we could
> encourage users to directly report detected biases to
> organizations and platforms, o
ering social incentives (e.g.,
> reputations and points) and monetary incentives accordingly. A
> potential risk is that, if not designed carefully, the provision
> of extrinsic rewards for algorithm auditing may reduce useful
> spontaneity or may inadvertently diminish users’ existing
> intrinsic motivations to engage in everyday auditing behaviors.

Risks of algorithm bug county

***

##### Results, Page 23, Purple, Highlight, 2023-03-21:

> In addition to incentivizing auditors, there is also a need to
> incentivize developers and platforms both to implement these
> measures (e.g., supporting user-led audits on embedded
> discussion forums) and to remediate issues that users uncover

Developers want this!

***

##### Results, Page 23, Purple, Highlight, 2023-03-21:

> Similarly, recent work from Vincent et al. [93] discusses
> potential ways for the public to withhold their data
> contributions as leverage, reducing the e
ectiveness of speci
c
> data-driven technologies in order to motivate companies to
> address users’ concerns.

Use your data as power! #Ideas

***

##### Results, Page 23, Purple, Highlight, 2023-03-21:

> Finally, given the uneven power dynam- ics between users and
> platforms, structural interventions are also needed (e.g.,
> policy and legal interventions) to ensure external
> accountability.

***

##### Results, Page 25, Purple, Highlight, 2023-03-21:

> We have drawn lessons from these cases for future research and
> design around everyday algorithm auditing, outlining 
ve broad
> categories of potential design interventions to support everyday
> audits — (a) commu- nity guidance, (b) expert guidance, (c)
> algorithmic guidance, (d) organizational guidance, and (e)
> incentivization — along with potential trade-o
s.

***

#### Discussion

##### Discussion, Page 3, Orange, Highlight, 2023-03-21:

> Furthermore, we suggest that everyday algorithm auditing may be
> most powerful when audits are conducted collectively, through
> interactive discussions among users with a diverse range of
> experiences.

***

##### Discussion, Page 3, Orange, Highlight, 2023-03-21:

> bridge the gap between existing algorithm auditing approaches in
> academia and industry versus everyday auditing behaviors that
> emerge in day-to-day use of algorithmic systems.

***

##### Discussion, Page 5, Orange, Highlight, 2023-03-21:

> approaches can fail to detect serious issues is that those
> involved in an audit may lack the relevant cultural backgrounds
> and lived experiences to recognize or know where to look for
> harmful behaviors [99].

***

##### Discussion, Page 5, Orange, Highlight, 2023-03-21:

> Without engaging diverse, contextually situated users in the
> process of auditing complex algorithmic systems, existing
> approaches are likely to su
er major blindspots, with critical
> issues surfacing only post-deployment (e.g., [17, 29, 33, 48,
> 80]).

***

##### Discussion, Page 5, Orange, Highlight, 2023-03-21:

> we emphasize the situatedness [83] of everyday algorithm audits
> within everyday use as a central factor that distinguishes this
> concept from other approaches.

***

##### Discussion, Page 5, Orange, Highlight, 2023-03-21:

> everyday algorithm auditing practices are situated actions in
> which everyday users encounter problematic machine behaviors via
> their routine interactions with algorithmic systems, then adjust
> their behaviors to understand and act on what they encountered.

***

##### Discussion, Page 8, Orange, Highlight, 2023-03-21:

> We argue that everyday algorithm auditing can be thought of as a
> form of everyday algorithmic resistance [89]. In their seminal
> work, Certeau and Scott [22, 77] developed the idea of “everyday
> resistance” to examine the ways in which people exercise their
> agency in front of dominant power structures and hegemonic
> culture formats in their everyday lives

***

##### Discussion, Page 8, Orange, Highlight, 2023-03-21:

> Instead of characterizing the relationship between users and
> algorithms as passive consumers versus omnipotent machines, this
> line of research helps us foreground users’ agency, capacity,
> and tactics in their regular interactions and contestations with
> algorithms.

***

##### Discussion, Page 8, Orange, Highlight, 2023-03-21:

> We argue that everyday algorithm auditing, when performed by a
> group of users collectively, can also be viewed as a form of
> counterpublics

***

##### Discussion, Page 18, Orange, Highlight, 2023-03-21:

> We argue that everyday algorithm auditing can receive — and
> often bene
t from — di
erent degrees and forms of external
> intervention. For instance, in some cases, developers with
> insight into the inner workings of a given system may be able to
> provide useful feedback to guide hypothesis formation, such as
> in the Twitter cropping case when an engineering team lead
> chimed in to let audit participants know that facial recognition
> is not part of the cropping algorithm.30 To the extent that
> external interventions represent guidance for an otherwise
> collectively-led audit, rather than directing or orchestrating
> the collective’s activities as in a crowdsourced/collective
> audit, we do not view such interventions as invalidating the
> everyday nature of the audit.

#Ideas

***

##### Discussion, Page 23, Orange, Highlight, 2023-03-21:

> We argue that such “everyday algorithm auditing” is especially
> powerful in detecting harmful behaviors that are challenging for
> existing auditing approaches discussed in the academic
> literature.

***

##### Discussion, Page 23, Orange, Highlight, 2023-03-21:

> First, everyday algorithm auditing leverages the lived
> experiences of everyday users. Past literature suggests that
> existing approaches often fail to involve auditors with relevant
> cultural backgrounds and lived experience that are critical to
> detect sensitive harms [99]

***

##### Discussion, Page 23, Orange, Highlight, 2023-03-21:

> Everyday algorithm auditing, with contextually situated users,
> is especially powerful to appropriately identify the problems at
> hand.

***

##### Discussion, Page 24, Orange, Highlight, 2023-03-21:

> Second, everyday algorithm auditing harnesses the situated
> knowledge of everyday users. As past literature suggests, many
> harmful machine behaviors are challenging to detect outside of
> situated contexts of use [17, 33, 48, 60, 78].

***

##### Discussion, Page 24, Orange, Highlight, 2023-03-21:

> Through their day-to-day interactions with an algorithmic
> system, everyday users are particularly well positioned to
> detect these types of behaviors that emerge in real-world
> contexts of use, in the presence of complex social dynamics, and
> in the changing norms and practices of using algorithmic systems
> over time.

***

##### Discussion, Page 24, Orange, Highlight, 2023-03-21:

> hird, in an everyday algorithm audit, users are able to form
> counterpublics via di
erent social media channels or community
> forums and participate in their own collective sensemaking and
> consensus building.

***

##### Discussion, Page 24, Orange, Highlight, 2023-03-21:

> his is especially powerful since only in such collective
> attempts, will users be able to build on each others’
> contributions, test each others’ hypotheses, and support each
> other in di
erent forms (e.g., helping publicize their e
orts).
> In contrast, in a crowdsourced/collaborative audit [74], users
> are often working on individualized tasks that have been
> assigned to them, without participating in discussion.

***

##### Discussion, Page 24, Orange, Highlight, 2023-03-21:

> Finally, in an everyday algorithm audit, users have control over
> the auditing process. This di
ers from a
> crowdsourced/collaborative audit [74], in which users are hired
> via crowdsourcing platforms to work on decomposed subtasks
> determined entirely by outside parties; in an everyday algorithm
> auditing, users — often collectively — decide their own course
> of action. Their autonomy and agency help steer the audit in the
> directions that are most useful and meaningful for them, which
> are often overlooked by existing approaches.

Autonomy and agency is important

***

##### Discussion, Page 24, Orange, Highlight, 2023-03-21:

> However, currently we have very limited understanding of the
> appropriate timing for intervention. For example, how can we
> maintain the organic nature of everyday algorithm auditing,
> especially at the initiation stage, but also prompt a group of
> people to start auditing 
rst, which might inevitably remove
> some of the organic nature?

#Ideas

***

##### Discussion, Page 24, Orange, Highlight, 2023-03-21:

> Currently we are only at the starting point to understand and
> explore the appropriate degrees of these interventions. How much
> inter- vention is too much? What are the trade-o
s in moving
> from more organic to more organized a

***

#### Method

##### Method, Page 2, Pink, Highlight, 2023-03-21:

> Through online discussions, they built upon one another’s
> 
ndings to surface similar biases or to present evidence or
> counter-evidence for a pattern discovered by another person

***

##### Method, Page 2, Pink, Highlight, 2023-03-21:

> Twitter’s testing procedures failed to detect this bias because
> during real world usage, users were interacting with the
> cropping algorithm in ways the team did not anticipate up front

Users have situated knowledge

***

##### Method, Page 2, Pink, Highlight, 2023-03-21:

> We draw insights from past literature in “everyday resistance”
> [22, 77] and “algorithmic resistance” [89] to theorize such
> behaviors. We interpret everyday algorithmic auditing e
orts as
> a form of everyday resistance by which users actively,
> continuously question and repurpose mass cultural products in

***

##### Method, Page 3, Pink, Highlight, 2023-03-21:

> We adopt an exploratory case study approach

***

##### Method, Page 3, Pink, Highlight, 2023-03-21:

> we examine two types of algorithmic sociotechnical platforms,
> the Twitter cropping algorithm and online rating algorithms,
> that have been the target of everyday algorithm audits in recent
> years.

This doesn't include closed systems

***

##### Method, Page 6, Pink, Highlight, 2023-03-21:

> business owners on Yelp came together

It's the collective testing that is important!

***

##### Method, Page 9, Pink, Highlight, 2023-03-21:

> Thus, we asked: How can we better understand the
> characteristics, dynamics, and progression of everyday auditing
> practices? How can we support everyday users in detecting,
> reporting, and theorizing about problematic machine behaviors
> during the course of their interactions with algorithmic
> systems?

Questions authors asked

***

##### Method, Page 9, Pink, Highlight, 2023-03-21:

> we adopted an exploratory case study approach [69], examining
> several recent, prominent cases

***

##### Method, Page 9, Pink, Highlight, 2023-03-21:

> We iteratively reviewed and discussed these known cases to form
> the initial idea of “everyday algorithm auditing”.

***

##### Method, Page 9, Pink, Highlight, 2023-03-21:

> we generated related keywords (e.g., “auditing,” “testing,”
> “algorithm,” “algorithmic,” “bias,” “harm,” “users,”
> “collective,” and “community”).

***

##### Method, Page 9, Pink, Highlight, 2023-03-21:

> we searched news media (e.g., Google News, Google Search) and
> social media (e.g., Twitter, Facebook) using combinations of
> these keywords to get a rough sense for the scope of this
> phenomenon.

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> de
nition for everyday algorithm auditing: one or more everyday
> users act to detect, understand, and/or interrogate biased and
> harmful algorithmic behaviors through their everyday use of a
> platform.

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> These cases span multiple algorithmic do- mains, including image
> captioning, image search, machine translation, online
> rating/review, image cropping, credit scoring, and advertising
> and recommendation systems.

Scope should be more specific for domains

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> Case Selection: To support depth as well as breadth in our
> analysis of everyday algorithm audits, we chose a small subset
> of four cases to examine in greater detail

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> we iteratively extracted patterns from our initial dataset
> through a series of discussions among our research team.

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> set out to choose a set of cases that a) span multiple domains
> and vary along the three dimensions outlined in the Scope and
> Boundaries section (algorithmic expertise, collectiveness,
> organicness) and b) were accessible to us via multiple data
> sources (e.g., user discussion posts, news articles, research
> studies), enabling a rich analysis.

Their prerequisites for selecting cases

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> we chose two di
erent domains that each have each been the
> target of everyday algorithm audits in recent years: image
> cropping algorithms and rating platform algorithms

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> Figure 1 summarizes the four cases, visualizing varying degrees
> along each dimension. For example, there are highly
> collaborative, less collaborative, and individual levels of
> collectiveness among the four cases.

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> For each of our four primary cases, we gathered and reviewed
> discussion threads from the platforms where the everyday audits
> took place, then supplemented this information by drawing upon
> relevant media (e.g., news articles, academic publications)
> collected in our earlier search

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> For each platform, we examine two di
erent categories of biases
> that were detected via everyday algorithm auditing, as well as
> the di
erent paths users took to conduct these audits.

***

##### Method, Page 10, Pink, Highlight, 2023-03-21:

> We compare these cases with each other to understand
> similarities and di
erences between everyday audits, and we
> consider how the paths taken contribute to their impacts

***

##### Method, Page 11, Pink, Square, 2023-03-21:

![Scope and boundaries](assets/Page11Image50.05204733096091_423.5119376197563-394.30275552970204_219.67306415275198.png)

***

##### Method, Page 15, Pink, Highlight, 2023-03-21:

> Unlike the Yelp case, this everyday algorithm audit stayed
> highly individualized. After Book- ing.com users reported the
> issue they observed through reviews, there was no on-platform
> way for the users to communicate with each other, in contrast to
> the Yelp case where users could talk on the Yelp platform.

No way for users to talk about the issue

***

##### Method, Page 15, Pink, Highlight, 2023-03-21:

> Users attempted to warn others of both the bad hotel experience
> and the unrepresentative algorithm behaviors, using review
> writing as a method for raising awareness since this was the
> only way to communicate on the platform.

How users attempted to share their findings

***

##### Method, Page 16, Pink, Highlight, 2023-03-21:

> Or perhaps more collective everyday audits invite publicity, as
> the collective nature means that not only are more people aware
> of the audit occurring, but also they are more aware of it as
> they actively participate in sensemaking discussions and
> actions. While Booking.com’s audit participants lacked
> collectiveness, they still changed their behaviors to try to
> make changes to the system; that is, they signaled to other
> users a more accurate presentation of their hotel stays, and
> they trusted the overall platform ratings less [28].

***

##### Method, Page 16, Pink, Highlight, 2023-03-21:

> ather, users often stumble upon concerning algorithmic behaviors
> in the midst of their day-to-day use of a system.

Spontaneity

***

##### Method, Page 17, Pink, Highlight, 2023-03-21:

> After detecting a problematic algorithmic behavior, the
> initiator or initiators broadcast what has been discovered to
> others. For example, in both Twitter cropping cases, this took
> the form of a tweet to other users. Others can then spread the
> word to even more people

***

##### Method, Page 17, Pink, Highlight, 2023-03-21:

> Promoting the audit is a valuable part of the process, as it can
> bring more people into the discussion and increases visibility
> of the issue at hand.

Why having a forum matters!

***

##### Method, Page 17, Pink, Highlight, 2023-03-21:

> the more users promote the audit, the more new people notice its
> happening and are then more likely to promote the audit
> themselves. Externally, sharing information about the audit
> makes it more likely that the awareness will eventually extend
> beyond the audit, as will be discussed more in Remediation
> below.

***

##### Method, Page 17, Pink, Highlight, 2023-03-21:

> 6.3 Hypothesizing & Testing Following initiation and/or
> awareness raising, an everyday algorithm audit may progress
> toward users forming hypotheses for the observed behavior and
> intentionally testing an algorithmic system to collect further
> evidence. This process may involve just the individual who
> initiated the audit or may extend to many users if the initiator
> raised awareness and inspired others to get involved. Individual
> users who proceed alone continue with little support either from
> other situated users or from external experts.

***

##### Method, Page 17, Pink, Highlight, 2023-03-21:

> In more collective everyday algorithm audits, a group of
> everyday users organizes and builds on each others’ e
orts in
> the process. The users collectively decide on and steer the
> process, often through online discussion.

***

##### Method, Page 17, Pink, Highlight, 2023-03-21:

> When users start hypothesizing about the potential biases of a
> system and testing their hypothe- ses, they usually develop
> “folk theories”: non-authoritative theories a user forms to
> explain and understand how a technological system works [23, 27]

***

##### Method, Page 17, Pink, Highlight, 2023-03-21:

> n an everyday algorithm audit, these theories help users to form
> hypotheses about the sources of a bias, then test them.
> Sometimes these tests fail, such as switching the tie colors of
> McConnell and Obama in the Twitter racial cropping algorithm
> case to test the hypothesis that the algorithm might prioritize
> red over blue;29

***

##### Method, Page 18, Pink, Highlight, 2023-03-21:

> Hypothesizing and testing often involves di
erent degrees and
> forms of collaboration. In cases where users perform the
> auditing in an individual endeavor, they might spot an issue and
> quickly report it via social media channels or the platforms’
> internal discussion forum or, if coming from a background of
> high technical expertise, they might conduct the auditing in a
> more systematic way alone. In other more collective cases, users
> come together to make sense of how the system actually produces
> such biased results.

***

##### Method, Page 18, Pink, Highlight, 2023-03-21:

> During this process, they might raise new evidence, test each
> other’s evidence, and collectively search for patterns that
> emerged from their discussion

***

##### Method, Page 18, Pink, Highlight, 2023-03-21:

> he hypothesizing and testing process can range from organized to
> organic. More organized testing often involves external
> stakeholders (e.g., researchers, developers, journalists) who
> intervene in the process, while more organic testing often
> arises naturally without outside intervention as everyday users
> detect and assess algorithmic behaviors and play an active role
> in determining the course the audit takes.

***

##### Method, Page 18, Pink, Highlight, 2023-03-21:

> .4 Remediation At its core, an everyday algorithm audit has a
> singular objective: instigate change based on the issues
> identi
ed and investigated.

***

##### Method, Page 18, Pink, Highlight, 2023-03-21:

> One type of change creates or increases external awareness
> through publicity

***

##### Method, Page 18, Pink, Highlight, 2023-03-21:

> Another type of change comes in the form of legal action

***

##### Method, Page 19, Pink, Highlight, 2023-03-21:

> Perhaps the most e
ective type of change is at the platform
> level. Sometimes when an everyday algorithm audit brings a
> problem to the forefront, the creators or employers of the
> algorithm take notice and agree that a change should occur.

Most effective change for organic audits

***

##### Method, Page 25, Pink, Highlight, 2023-03-21:

> y comparing the lifetime and dynamics of several cases of
> everyday algo- rithm auditing, we have proposed a
> process-oriented view of everyday algorithm audits, involving
> initiation, awareness raising, testing and hypothesizing, and
> remediation

***

#### Other

##### Other, Page 2, Other, Highlight, 2023-03-21:

> What factors facilitate “successful” user-driven algorithmic
> audits, and what factors prevent such audits from having an
> impact? How might we e
ectively harness everyday users’
> motivation and strengths in surfacing harmful algorithmic
> behaviors, so as to overcome the limitations of existing
> auditing approaches?

Contribution

***

##### Other, Page 3, Other, Text, 2023-03-21:

Covered existing approaches to algorithm auditing

***

##### Other, Page 6, Other, Text, 2023-03-21:

The domains are limited in being too broad and not including closed systems

***

##### Other, Page 25, Other, Highlight, 2023-03-21:

> Future research should conduct in-depth follow-up studies with
> users to investigate the practices and strategies they use to
> 
nd and make sense of harmful algorithmic behaviors. Another
> fruitful, complementary area for future research includes an
> exploration of the design opportunities discussed above. For
> example, how can we design platforms that support users in
> conducting more e
ective everyday algorithm audits, both
> individually and collectively

Two opportunities for extending this work #ideas

***




[Page11Image50.05204733096091_423.5119376197563-394.30275552970204_219.67306415275198.png]: assets/Page11Image50.05204733096091_423.5119376197563-394.30275552970204_219.67306415275198.png
