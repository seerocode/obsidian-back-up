{"path":".obsidian/plugins/text-extractor/cache/hu/zoterolibrary-files-3890-hu-chen-2020-fair-classification-and-social-welfare-pdf-a561600add228fd4f85fe5768e8ed5d1.json","text":"Fair Classification and Social Welfare Lily Hu lilyhu@g.harvard.edu Harvard University Cambridge, MA Yiling Chen yiling@seas.harvard.edu Harvard University Cambridge, MA ABSTRACT Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of a￿airs, important questions follow. How do leading notions of fair- ness as de￿ned by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classi￿cation regimes. Our main ￿ndings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of \u0000\u0000 perturbations to a fairness parameter \u0000 in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and e￿cient computation of “fairness-to-welfare” solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classi￿cation outcomes that make groups better-o￿. Our analyses show that applying stricter fairness criteria codi￿ed as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring “more fair” clas- si￿ers does not abide by the Pareto Principle—a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their e￿ectiveness as a means to ensure fairness and justice. ACM Reference Format: Lily Hu and Yiling Chen. 2020. Fair Classi￿cation and Social Welfare. In Conference on Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 17 pages. https: //doi.org/10.1145/3351095.3372857 1 INTRODUCTION In his 1979 Tanner Lectures, Amartya Sen noted that since nearly all egalitarian theories are founded on an equality of some sort, the heart of the issue rests on clarifying the “equality of what?” problem [1]. The ￿eld of fair machine learning has not escaped this essential question. Does machine learning have an obligation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro￿t or commercial advantage and that copies bear this notice and the full citation on the ￿rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a fee. Request permissions from permissions@acm.org. FAT* ’20, January 27–30, 2020, Barcelona, Spain © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00 https://doi.org/10.1145/3351095.3372857 to assure probabilistic equality of outcomes across various social groups [2, 3]? Or does it simply owe an equality of treatment [4]? Does fairness demand that individuals (or groups) be subject to equal mistreatment rates [5, 6]? Or does being fair refer only to avoiding some intolerable level of algorithmic error? Currently, the task of accounting for fair machine learning cashes out in the comparison of myriad metrics—probability distributions, error likelihoods, classi￿cation rates—sliced up every way possible to reveal the range of inequalities that may arise before, during, and after the learning process. But as shown in work by Chouldechova [7] and Kleinberg et al. [8], fundamental statistical incompatibilities rule out any solution that can satisfy all parity metrics. Fairness- constrained loss minimization o￿ers little guidance on its own for choosing among the fairness desiderata, which appear incommen- surable and result in di￿erent impacts on di￿erent individuals and groups. We are thus left with the harsh but unavoidable task of adjudicating between these measures and methods. How ought we decide? For a given application, who actually bene￿ts from the operationalization of a certain fairness constraint? This is a basic but critical question that must be answered if we are to under- stand the impact that fairness constraints have on classi￿cation outcomes. Much research in fairness has been motivated by the well-documented negative impacts that these systems can have on already structurally disadvantaged groups. But do fairness con- straints as currently formulated in fact earn their reputation as serving to improve the welfares of marginalized social groups? When algorithms are adopted in social environments—consider, for example, the use of predictive systems in the ￿nancial services industry—classi￿cation outcomes directly bear on individuals’ ma- terial well-beings. We, thus, view predictions as resource allocations awarded to individuals and by extension, to various social groups. In this paper, we build out a method of analysis that takes in generic fair learning regimes and analyzes them from a welfare perspective. Our main contributions, presented in Section 3, are methodolog- ical as well as substantive in the ￿eld of algorithmic fairness. We show that how “fair” a classi￿er is—how well it accords with a group parity constraint such as “equality of opportunity” or “balance for false positives”—does not neatly translate into statements about di￿erent groups’ welfares are a￿ected. Drawing on techniques from parametric programming and ￿nding a SVM’s regularization path, our method of analysis ￿nds the optimal \u0000-fair Soft-Margin SVM solution for all values of a fairness tolerance parameter \u0000 2 [0, 1]. We track the welfares of individuals and groups as a function of \u0000 and identify those ranges of \u0000 values that support solutions that are Pareto-dominated by neighboring \u0000 values. Further, the algorithmic implementation of our analyses is computationally e￿cient, with a complexity on the same order as current standard SVM solvers that ￿t a single SVM model, and is thus practical as a procedure that translates fairness constraints into welfare e￿ects for all \u0000. 535 FAT* ’20, January 27–30, 2020, Barcelona, Spain Lily Hu and Yiling Chen Our substantive results show that a classi￿er that abides by a stricter fairness standard does not necessarily issue improved outcomes for the disadvantaged group. In particular, we prove two results: ￿rst, starting at any nonzero \u0000-fair optimal SVM solution, we express the range of \u0000\u0000 < 0 perturbations that tighten the fairness constraint and lead to classi￿er-output allocations that are weakly Pareto dominated by those issued by the “less fair” original classi￿er. Second, there are nonzero \u0000-fair optimal SVM solutions, such that there exist \u0000\u0000 < 0 perturbations that yield classi￿cations that are strongly Pareto dominated by those issued by the “less fair” original classi￿er. We demonstrate these ￿ndings on the Adult dataset. In general, our results show that when notions of fairness rest entirely on leading parity-based notions, always preferring more fair machine learning classi￿ers does not accord with the Pareto Principle, an axiom typically seen as fundamental in social choice theory and welfare economics. The purposes of our paper are twofold. The ￿rst is simply to encourage a welfare-centric understanding of algorithmic fairness. Whenever machine learning is deployed within important social and economic processes, concerns for fairness arise when societal ideals are in tension with a decision-maker’s interests. Most leading methodologies have focused on optimization of utility or welfare to the vendor but have rarely awarded those individuals and groups who are subject to these systems the same kind of attention to welfare e￿ects. Our work explicitly focuses its analysis on the latter. We also seek to highlight the limits of conceptualizing fairness only in terms of group-based parity measures. Our results show that at current, making a system “more fair” as de￿ned by popular met- rics can harm the vulnerable social populations that were ostensibly meant to be served by the imposition of such constraints. Though the Pareto Principle is not without faults, the frequency with which “more fair” classi￿cation outcomes are welfare-wise dominated by “less fair” ones occurs is troublesome and should lead scholars to reevaluate popular methodologies by which we understand the impact of machine learning on di￿erent social populations. 1.1 Related Work Research in fair machine learning has largely centered on com- putationally de￿ning “fairness” as a property of a classi￿er and then showing that techniques can be invented to satisfy such a notion [2–5, 5, 6, 9–18]. Since most methods are meant to apply to learning problems generally, many such notions of fairness center on parity-based metrics about a classi￿er’s behavior on various legally protected social groups rather than on matters of welfare. Most of the works that do look toward a welfare-based frame- work for interpreting appeals to fairness sit at the intersection of computing and economics. Mullainathan [19] also makes a com- parison between policies as set by machine learning systems and policies as set by a social planner. He argues that systems that make explicit their description of a global welfare function are less likely to perpetrate biased outcomes and are more successful at ameliorat- ing social inequities. Heidari et al. [20] propose using social welfare functions as fairness constraints on loss minimization programs. They suggest that a learner ought to optimize her classi￿er while in Rawls’ original position. As a result, their approach to social welfare is closely tied with considerations of risk. Rather than integrate social welfare functions into the supervised learning pipeline, we claim that the result of an algorithmic classi￿cation system can itself be considered a welfare-impacting allocation. Thus, our work simply takes a generic \u0000-fair learning problem as-is, and then con- siders the welfare implications of its full path of outcomes for all \u0000 2 [0, 1] on individuals as well as groups. Attention to the potential harms of machine learning systems is not new, of course. Within the fairness literature, Corbett-Davies & Goel [21] and Liu et al. [22] devote most of their analyses to the person-impacting e￿ects of algorithmic systems. We agree that these e￿ects are relevant to the question of fairness, but our results di￿er in their methodologi- cal focus: we introduce a technique that derives the full range of welfare e￿ects achieved by a fair classi￿cation algorithm. The techniques that we use to translate fair learning outcomes into welfare paths are related to a number of existing works. The proxy fairness constraint in our instantiation of the \u0000-fair SVM problem original appeared in Zafar et al.’s work on restricting the disparate impact of machine classi￿ers [5]. Their research intro- duces this particular proxy fairness constrained program and shows that it can be e￿ciently solved and well approximates target fair- ness constraints. We use the constraint to demonstrate our overall ￿ndings about the e￿ect of fairness criteria on individual and group welfares. We share some of the preliminary formulations of the fair SVM problem with Donini et al. [17] though they focus on the statistical and fairness guarantees of the generalized ERM program. Lastly, though work on tuning hyperparameters of SVMs and the solution paths that result seem far a￿eld from questions of fairness and welfare, our analysis on the e￿ect of \u0000\u0000 fairness perturbations on welfare take advantage of methods in that line of work [23–27]. 2 PROBLEM FORMALIZATION Our framework and results are motivated by those algorithmic use cases in which considerations of fairness and welfare stand alongside those of e￿ciency. Because our paper connects machine classi￿cation and notions of algorithmic fairness with conceptions of social welfare, we ￿rst provide an overview of the notation and assumptions that feature throughout our work. In the empirical loss minimization problem, a learner seeks a classi￿er h that issues the most accurate predictions when trained on set of n data points {xi , zi , \u0000i }n i=1. Each triple gives an individ- ual’s feature vector xi 2 X, protected class attribute zi 2 {0, 1}, and true label \u0000i 2 {\u00001, +1}.1 A classi￿er that assigns an incorrect label h(xi ) , \u0000i incurs a penalty. The empirical risk minimizing predictor is given by h⇤ := arg min h 2H nX i=1 `(h(xi), \u0000i ) where hypothesis h : X ! R gives a learner’s model, the loss func- tion ` : R⇥ {\u00001, +1} ! R gives the penalty incurred by a prediction, and H is the hypothesis class under the learner’s consideration. Binary classi￿cation systems issue predictions h(x) 2 {\u00001, +1}. Notions of fairness have been formalized in a variety of ways in the machine learning literature. Though Dwork et al.’s [4] initial conceptualization remains prominent and in￿uential, much work 1Though individuals in a dataset will typically be coded with many protected class attributes, in this paper we will consider only a single sensitive attribute of focus. 536 Fair Classification and Social Welfare FAT* ’20, January 27–30, 2020, Barcelona, Spain has since de￿ned fairness as a parity notion applied across di￿erent protected class groups [3, 5, 7, 8, 17, 18]. The following de￿nition gives the general form of these types of fairness criteria. De￿nition 2.1. A classi￿er h satis￿es a general group-based no- tion of \u0000-fairness if \u0000\u0000E[\u0000(`, h, xi , \u0000i )|Ezi =1] \u0000 E[\u0000(`, h, xi , \u0000i )|Ezi =0]\u0000\u0000  \u0000 (1) where \u0000 is some function of classi￿er h performance, and Ezi =0 and Ezi =1 are events that occur with respect to groups z = 0 and z = 1 respectively. Further speci￿cations of the function \u0000 and the events E instan- tiate particular group-based fairness notions. For example, when \u0000(`, h, xi , \u0000i ) = h(xi ) and Ezi refers to the events in which \u0000i =+1 for each group zi 2 {0, 1}, De￿nition 2.1 gives an \u0000-approximation of equality of opportunity [3]. When \u0000(`, h, xi , \u0000i ) = `(h(xi ), \u0000i ) and Ezi refers to all classi￿cation events for each group zi , De￿nition 2.1 gives the notion of \u0000-approximation of overall error rate balance [7]. Notice that as \u0000 increases, the constraint loosens, and the solu- tion is considered “less fair.” As \u0000 decreases, the fairness constraint becomes more strict, and the solution is considered “more fair.” Mapping classi￿cation outcomes to changes in individuals’ wel- fares gives a useful method of analysis for many data-based algorith- mic systems that are involved in resource distribution pipelines. In particular, we consider tools that issue outcomes uniformly ranked, or preferred, by those individuals who are the subjects of the system. That is, individuals agree on which outcome is preferred. Examples of such systems abound: applicants for credit generally want to be found eligible; candidates for jobs generally want to be hired, or at least ranked highly in their pool. These realms are precisely those in which fairness considerations are urgent and where fairness- adjusted learning methods are most likely to be adopted. 3 WELFARE IMPACTS OF FAIRNESS CONSTRAINTS The central inquiry of our work asks how fairness constraints as popularized in the algorithmic fairness community relate to welfare- based analyses that are dominant in economics and policy-making circles. Do fairness-adjusted optimization problems actually make marginalized groups better-o￿ in terms of welfare? In this section, we work from an empirical risk minimization (ERM) program with generic fairness constraints parametrized by a tolerance parameter \u0000 > 0 and trace individuals’ and groups’ welfares as a function of \u0000. We assume that an individual bene￿ts from receiving a positive classi￿cation, and thus we de￿ne group welfare as Wk = 1 nk X i |zi =k h(xi ) + 1 2 , k 2 {0, 1} (2) where nk give the number of individuals in group z = k. We note that Wk can be de￿ned in ways other than (2), which assumes that positive classi￿cation are always and only welfare-enhancing. Other work has considered the possibility that positive classi￿cations may in fact make individuals worse-o￿ if they are false positives [22]. The de￿nition of Wk can be generalized to account for these cases. First, in Section 3.1, we present an instantiation of the \u0000-fair ERM problem with a fairness constraint proposed in prior work in algorithmic fairness. We work from the Soft-Margin SVM program and derive the various dual formulations that will be of use in the following analyses. In Section 3.2, we move on to show how \u0000\u0000 per- turbations to the fairness constraint in the \u0000-fair ERM problem yield changes in classi￿cation outcomes for individuals and by extension, how they impact a group’s overall welfare. Our approach, which draws a connection between fairness perturbations and searches for an optimal SVM regularization parameter, tracks changes in an individual’s classi￿cation by taking advantage of the codependence of variables in the dual of the SVM. By perturbing the fairness constraint, we observe changes in not its own corresponding dual variable but in the corresponding dual of the margin constraints, which relay the classi￿cation fates of data points. Leveraging this technique, we plot the “solution paths” of the dual variable as a function of \u0000, which in turn allows us to compute group welfares as a function of \u0000 and draw out substantive results on the dynamics of how classi￿cation outcomes change in response to \u0000-fair learning. We prove that stricter fairness standards do not necessarily support welfare-enhancing outcomes for the disadvan- taged group. In many such cases, the learning goal of ensuring group-based fairness is incompatible with the Pareto Principle. De￿nition 3.1 (Pareto Principle). Let x, \u0000 be two social alternatives. Let ⌫i be the preference ordering of individuals i 2 [n], and ⌫P be the preference ordering of a social planner. The planner abides by the Pareto Principle if x ⌫P \u0000 whenever x ⌫i \u0000 for all i. In welfare economics, the Pareto Principle is a standard require- ment of social welfare functionals—it would appear that the se- lection of an allocation that is Pareto dominated by an available alternative would be undesirable and even irresponsible! Neverthe- less, we show that applying fairness criteria to loss minimization tasks in some cases do just that. We perform our analysis on the Soft-Margin SVM optimization problem and, for concreteness, work with a well-known fairness formulation in the literature. However, we note that our methods and results apply to fairness-constrained convex loss minimization programs more generally. We also show that this method of analysis can form practical tools. In Section 3.3, we present a computationally e￿cient algo- rithmic implementation of our analyses, ￿tting full welfare solution paths for all \u0000 2 [0, 1] values in a time complexity that is on the same order as that of a single SVM ￿t. We close this section by work- ing from the shadow price of the fairness constraint to derive local and global sensitivities of the optimal solution to \u0000\u0000 perturbations. 3.1 Setting up the \u0000-fair ERM program The general fairness-constrained empirical loss minimization pro- gram can be written as minimize h 2 H `(h(x), \u0000) subject to fh (x, \u0000)  \u0000 (3) where `(h(x), \u0000) gives the empirical loss of a classi￿er h 2 H on the dataset X. To maximize accuracy, the learner ought to minimize 0-1 loss; however because the loss function `0\u00001 is non-convex, a convex surrogate loss such as hinge loss (`h ) or log loss (`log) is frequently substituted in its place to ensure that globally optimal solutions may be e￿ciently found. fh (x, \u0000)  \u0000 gives a group-based 537 FAT* ’20, January 27–30, 2020, Barcelona, Spain Lily Hu and Yiling Chen fairness constraint of the type given in De￿nition 2.1, where \u0000 > 0 is the unfairness “tolerance parameter”—a greater \u0000 permits a greater group disparity on a metric of interest; a smaller \u0000 more tightly restricts the level of permissible disparity. We examine the behavior of fairness-constrained linear SVM classi￿ers, though we note that our techniques generalize to nonlin- ear kernels SVMs, since interpretations of the dual of the SVM and the full SVM regularization path are the same with kernels [24]. Our learner minimizes hinge loss with L1 regularization; equivalently, she seeks a Soft-Margin SVM that is “\u0000-fair.” Both SVM models and “fair training” approaches are in broad circulation. The fair empirical risk minimization program is thus given as minimize \u0000, b 1 2 k\u0000 k2 + C nX i=1 \u0000i subject to \u0000i (\u0000 |xi + b) \u0000 1 + \u0000i \u0000 0, (\u0000-fair Soft-SVM) \u0000i \u0000 0, f\u0000,b (x, \u0000)  \u0000 where the learner seeks SVM parameters \u0000, b; \u0000i are non-negative slack variables that violate the margin constraint in the Hard- Margin SVM problem \u0000i (\u0000 |xi + b) \u0000 1 \u0000 0, and C > 0 is a hyper- parameter tunable by the learner to express the trade-o￿ between preferring a larger margin and penalizing violations of the margin. f\u0000,b (x, \u0000) is the group parity-based fairness constraint. The abundant literature on algorithmic fairness presents a long menu of options for the various forms that f\u0000,b could take, but generally speaking, the constraints are non-convex. As such, much work has enlisted methods that depart from directly pursuing e￿- cient constraint-based convex programming techniques in order to solve them [5, 6, 9, 16, 18]. Researchers have also devised convex proxy alternatives, which have been shown to approximate the intended outcomes of original fairness constraints well [5, 17, 28]. In particular, in this paper, we work with the proxy constraint proposed by Zafar et al. [5], which constrains disparities in covari- ance between group membership and the (signed) distance between individuals’ feature vectors and the hyperplane decision boundary: f\u0000,b (x, \u0000) = \u0000\u0000\u0000\u0000\u0000\u0000 1 n nX i=1 (zi \u0000 ¯z)(\u0000 |xi + b)\u0000\u0000\u0000\u0000\u0000\u0000  \u0000 (4) ¯z re￿ects the bias in the demographic makeup of X: ¯z = 1 n Pn i=1 zi . Let (\u0000-fair-SVM1-P) be the Soft-Margin SVM program with this covariance constraint. The corresponding Lagrangian is LP (\u0000, b,\u0000 , \u0000, µ, \u00001, \u00002) = 1 2 k\u0000 k2 + C nX i=1 \u0000i \u0000 nX i=1 \u0000i \u0000 nX i=1 µi (\u0000i (\u0000 |xi + b) \u0000 1 + \u0000i ) \u0000 \u00001 ⇣ \u0000 \u0000 1 n nX i=1 (zi \u0000 ¯z)(\u0000 |xi + b)⌘ (\u0000-fair-SVM1-L) \u0000 \u00002 ⇣ \u0000 \u0000 1 n nX i=1 (¯z \u0000 zi )(\u0000 |xi + b)⌘ where \u0000 2 Rd , b 2 R, \u0000 2 Rn are primal variables. The (non- negative) Lagrange multipliers \u0000, µ 2 Rn correspond to the n non- negativity constraints \u0000i \u0000 0 and the margin-slack constraints \u0000i (\u0000 |xi + b) \u0000 1 + \u0000i \u0000 0 respectively. The multipliers \u00001, \u00002 2 R correspond to the two linearized forms of the absolute value fair- ness constraint. By complementary slackness, dual variables reveal information about the satisfaction or violation of their correspond- ing constraints. The analyses in the subsequent two subsections will focus on these interpretations. By the Karush-Kuhn-Tucker (KKT) conditions, at the solution of the convex program, the gradients of L with respect to \u0000 , b, and \u0000i are zero. Plugging in these conditions, the dual Lagrangian is LD (µ, \u0000 ) = \u0000 1 2 \u0000\u0000\u0000\u0000\u0000\u0000 nX i=1 µi\u0000i xi \u0000 \u0000 n nX i=1 (zi \u0000 ¯z)xi \u0000\u0000\u0000\u0000\u0000\u0000 2 + nX i=1 µi \u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 (5) where \u0000 = \u00001 \u0000 \u00002. The dual maximizes this objective subject to the constraints µi 2 [0, C] for all i 2 [n] and P i=1 µi\u0000i = 0. We thus arrive at the Wolfe dual problem maximize µ, \u0000 , V \u0000 1 2 \u0000\u0000\u0000\u0000\u0000\u0000 nX i=1 µi\u0000i xi \u0000 \u0000 n nX i=1 (zi \u0000 ¯z)xi \u0000\u0000\u0000\u0000\u0000\u0000 2 + nX i=1 µi \u0000 V \u0000 subject to µi 2 [0, C], i = 1,. . . , n,(\u0000-fair-SVM1-D) nX i=1 µi\u0000i = 0, \u0000 2 [\u0000V , V ] where we have introduced the variable V to eliminate the absolute value function \u0000\u0000\u0000 \u0000\u0000 in the objective. Notice that when \u0000 = 0 and neither of the constraints bind, we recover the standard dual SVM program. Since we are concerned with fair learning that does alter an optimal solution, we consider cases where V is strictly positive. We introduce additional dual variables \u0000\u0000 and \u0000+, corresponding to the \u0000 2 [\u0000V , V ] constraint and derive the Lagrangian L(µ, \u0000 , V , \u0000\u0000, \u0000+) = \u0000 1 2 \u0000\u0000\u0000\u0000\u0000\u0000 nX i=1 µi\u0000i xi \u0000 \u0000 n nX i=1 (zi \u0000 ¯z)xi \u0000\u0000\u0000\u0000\u0000\u0000 2 + nX i=1 µi \u0000 V \u0000 + \u0000 (\u0000\u0000 \u0000 \u0000+) + V (\u0000\u0000 + \u0000+) Under KKT conditions, \u0000\u0000 + \u0000+ = \u0000 and \u0000 ⇤ = n(n(\u0000\u0000 \u0000 \u0000+) + Pn i=1 µi\u0000i hxi , ui) kuk2 (6) where u = Pn i=1 (zi \u0000 ¯z)xi geometrically gives some group-sensitive “average\" of x 2 X. We can now rewrite (\u0000-fair-SVM1-D) as maximize µ, \u0000\u0000, \u0000+ \u0000 1 2 \u0000\u0000\u0000\u0000\u0000\u0000 nX i=1 µi\u0000i (I \u0000 Pu)xi \u0000\u0000\u0000\u0000\u0000\u0000 2 + nX i=1 µi + 2n P i µi\u0000i hxi , ui + n2 (\u0000\u0000 \u0000 \u0000+) 2kuk2 (\u0000\u0000 \u0000 \u0000+) subject to µi 2 [0, C], i = 1,. . . , n, nX i=1 µi\u0000i = 0, (\u0000-fair SVM2-D) \u0000\u0000, \u0000+ \u0000 0, \u0000\u0000 + \u0000+ = \u0000 538 Fair Classification and Social Welfare FAT* ’20, January 27–30, 2020, Barcelona, Spain where I, Pu 2 Rd⇥d . The former is the identity matrix, and the latter is the projection matrix onto the vector u. As was also observed by Donini et al., the \u0000 = 0 version of (\u0000-fair SVM2-D) is equivalent to the standard formulation of the dual SVM program with Kernel K (xi , xj ) = h(I \u0000 Pu)xi , (I \u0000 Pu)xj i [17]. Since we are interested in the welfare impacts of fair learning when fairness constraints do have an impact on optimal solutions, we will assume that the fairness constraint binds. For clarity of exposition, we assume that the positive covariance constraint binds, and thus that \u0000\u0000 = 0 and \u0000+ = \u0000 in (\u0000-fair SVM2-D). This is without loss of generalization—the same analyses apply when the negative covariance constraint binds. The dual \u0000-fair SVM program becomes minimize µ 1 2 \u0000\u0000\u0000\u0000\u0000\u0000 nX i=1 µi\u0000i (I \u0000 Pu)xi \u0000\u0000\u0000\u0000\u0000\u0000 2 \u0000 nX i=1 µi + n\u0000 (2 P i µi\u0000i hxi , ui\u0000 n\u0000 ) 2kuk2 subject to µi 2 [0, C], i = 1,. . . , n, (\u0000-fair SVM-D) nX i=1 µi\u0000i = 0 We will work from this formulation of the constrained optimization problem for the remainder of the paper. 3.2 Impact of Fair Learning on Individuals’ Welfares We now move on to investigate the e￿ects of perturbing a ￿xed \u0000-fair SVM by some \u0000\u0000 on the classi￿cation outcomes that are is- sued. We ask, “How are individuals and groups’ classi￿cations, and thus their welfares, impacted when a learner tightens or loosens a fairness constraint?” The key insight that drives our methods and results is that rather than perform sensitivity analysis directly on the dual variable corresponding to the fairness constraint—which, as we will see in Section 3.4, only gives information about the change in the learner’s objective value—we track changes in the classi￿er’s behavior by analyzing the e￿ect of \u0000\u0000 perturbations on another set of dual variables: µi that correspond to the primal margin constraints. Each of these n dual variables indicate whether its corresponding vector xi is correctly classi￿ed, lies in the margin, or incorrectly classi￿ed. Leveraging how these µi change as a func- tion fo \u0000 thereby allows us to track the solution paths of individual points and by extension, compute group welfare paths. De￿ne a function p(\u0000 ) : R ! R that gives the optimal value of the \u0000-fair loss minimizing program in (\u0000-fair SVM1-P), which by duality is also the optimal value of (\u0000-fair SVM-D). We begin at a solution p(\u0000 ) and consider changes in classi￿cations at the solution p(\u0000 + \u0000\u0000 ), where \u0000\u0000 are perturbations can be positive or negative, so long as \u0000 + \u0000\u0000 > 0. At an optimal solution, the classi￿cation fate of each data point xi is encoded in the dual variable µ⇤ i , which is a function of \u0000. µi (\u0000 ) is the \u0000-parameterized solution path of µi such that at any particular solution p(\u0000 ), the optimal value of the dual variable µ⇤ i = µi (\u0000 ). As a slight abuse of notation, we reserve notation µi (\u0000 ) for the functional form of the solution path and write µ\u0000 i ; to refer to the value of the dual variable at a given \u0000. L￿￿￿￿ 3.2. The dual variable paths µi (\u0000 ) for all i 2 [n] are piecewise linear in \u0000. Though this lemma seems merely of technical interest, it is a workhorse result for both our methodological contributions— our analytical results and our computationally e￿cient algorithm, which converts fairness constraints to welfare paths—as well as our substantive fairness results about how fairness perturbations impact individual and learner welfares. The algorithm we present in Section 3.3, performs full welfare analysis for all values of \u0000 in a computationally e￿cient manner by taking advantage of the piecewise linear form of individual and group welfares. Piecewise linearity also sets the stage for the later substantive results about the tension between fairness improvements and the Pareto Prin- ciple. We thus walk through the longer proof of this key result in the main text of the paper as it provides important exposition, de￿nitions, and derivations for subsequent results. P￿￿￿￿. Let D\u0000 be the value of the objective function in (\u0000-fair SVM-D). By the dual formulation of the Soft-Margin SVM, we can use the value of @D \u0000 @µ j to partition the set of indices j 2 [n] in a way that corresponds to the classi￿cation fates of individual vectors xj at the optimal solution: @D\u0000 @µj > 0 \u0000! µ\u0000 j = 0, and j 2 F \u0000 (7) @D\u0000 @µj = 0 \u0000! µ\u0000 j 2 [0, C], and j 2 M\u0000 (8) @D\u0000 @µj < 0 \u0000! µ\u0000 j = C, and j 2 E\u0000 (9) Hence, xj are either correctly classi￿ed free vectors (7), vectors in the margin (8), or error vectors (9). We track membership in these sets by letting {F , M, E}\u0000 be the index set partition at the \u0000-fair solution. To analyze the impact that applying a fairness constraint has on individuals’ or groups’ welfares, we track the behavior of @D \u0000 @µ j and observe how vector index membership in sets F \u0000 , M\u0000 , and E\u0000 change under a perturbation to \u0000. This information will in turn reveal how classi￿cations change or remain stable upon tightening or loosening the fairness constraint. Fairness perturbations do not always shu￿e data points across the di￿erent membership sets F \u0000 , M\u0000 , and E\u0000 . It is clear that for j 2 {F , E}\u0000 , so long as a perturbation of \u0000\u0000 does not cause @D \u0000 @µ j to ￿ip signs or to vanish to 0, j will belong to the same set and h\u0000 (xj ) = h\u0000 +\u0000\u0000 (xj ) where h\u0000 (xj ) gives the \u0000-fair classi￿cation outcome for xj . In these cases, an individual’s welfare is una￿ected by the change in the fairness tolerance level from \u0000 to \u0000 + \u0000\u0000. In contrast, vectors xj with j 2 M\u0000 are subject to a di￿erent condition to ensure that they stay in the margin: @D \u0000 @µ j = @D \u0000 +\u0000\u0000 @µ j = 0, i.e., perturbing by \u0000\u0000 does not lead to any changes in @D \u0000 @µ j : @D\u0000 @µj = nX i=1 µi\u0000i (I \u0000 Pu)xi\u0000j (I \u0000 Pu)xj + n\u0000\u0000j hxj , ui kuk2 + b\u0000j \u0000 1 = 0 (10) for all j 2 M\u0000 . Let r \u0000, \u0000\u0000 j be the change in µ\u0000 j upon perturbing \u0000 by \u0000\u0000, then we have µ\u0000 +\u0000\u0000 j = µ\u0000 j + r \u0000, \u0000\u0000 j (11) 539 FAT* ’20, January 27–30, 2020, Barcelona, Spain Lily Hu and Yiling Chen recalling that µ\u0000 j is the value of µj at the optimal solution p(\u0000 ). Let r \u0000, \u0000\u0000 2 Rn+1 be the vector of µ\u0000 i sensitivities to perturbations \u0000\u0000 with r \u0000, \u0000\u0000 0 as the change in the o￿set b. For all unshu￿ed j 2 M\u0000 , we can compute r \u0000, \u0000\u0000 j by taking the ￿nite di￿erence of (10) with respect to a \u0000\u0000 perturbation, nX i=1 r \u0000, \u0000\u0000 i \u0000i\u0000j h(I \u0000 Pu)xi , (I \u0000 Pu)xj i + r \u0000, \u0000\u0000 0 \u0000j = \u0000n\u0000j \u0000\u0000 kuk2 hu, xj i It is clear that r \u0000, \u0000\u0000 i = 0 for all i that are left unshu￿ed in the partition {F , E}\u0000 . For these “stable ranges” where no i changes its index set membership, we can simplify the previous expression by summing over only those r \u0000, \u0000\u0000 i where i 2 M\u0000 : X i 2M\u0000 r \u0000, \u0000\u0000 i \u0000i\u0000j h(I \u0000 Pu)xi , (I \u0000 Pu)xj i + r \u0000, \u0000\u0000 0 \u0000j = \u0000n\u0000j \u0000\u0000 kuk2 hu, xj i Thus we can compute r \u0000, \u0000\u0000 i by inverting the matrix K \u0000 = *................... , 0 \u00001 \u00002 .. . \u0000 |M\u0000 | \u00001 ... \u0000i\u0000j h(I \u0000 Pu)xi , (I \u0000 Pu)xj i \u00002 \u0000 |M\u0000 | +/////////////////// - 2 R( |M\u0000 |+1)⇥( |M\u0000 |+1) (12) where indices are renumbered to only re￿ect i, j 2 M\u0000 . This matrix is invertible so long as the margin is not empty and the Kernel K (xi , xj ) = h(I \u0000 Pu)xi , (I \u0000 Pu)xj i forms a positive de￿nite matrix. Since the objective function in (\u0000-fair SVM-D) is quadratic, a su￿- cient condition for K \u0000 to be invertible is that the objective is strictly convex—we assume this as a technical condition.2 The sensitivities of µ\u0000 j for j 2 M\u0000 to \u0000\u0000 perturbations are given by r \u0000, \u0000\u0000 = (K \u0000 )\u00001 ✓ \u0000n kuk2 v◆ | {z } r \u0000 \u0000\u0000, where v = 2666666666664 0 ... \u0000j hu, xj i ... 3777777777775 2 R|M\u0000 |+1 (13) Plugging this back into (11), we have µ\u0000 +\u0000\u0000 j = µ\u0000 j + ✓⇣K \u0000 ⌘\u00001 ⇣ \u0000n kuk2 v⌘◆ j | {z } r \u0000 j \u0000\u0000 (14) Hence, for all j 2 M\u0000 that stay in the margin, the solution path function µj (\u0000 ) is linear in \u0000. For j 2 {F , E}\u0000 that stay in their partition sets, µj (\u0000 + \u0000\u0000 ) = µj (\u0000 ), so the function is constant. 2We mention the case in which the margin is empty in Section 3.3, though we refer the interested reader to the Appendix for a full exposition of how µ \u0000 j are updated when the margin is empty and as a result, we cannot compute how i move across index sets via the sensitivities r . When \u0000\u0000 perturbations do result in changes in the partition, there are four ways that indices could be shu￿ed across sets: (1) j 2 E\u0000 moves into M\u0000 +\u0000\u0000 (2) j 2 F \u0000 moves into M\u0000 +\u0000\u0000 (3) j 2 M\u0000 moves into F \u0000 +\u0000\u0000 (4) j 2 M\u0000 moves into E\u0000 +\u0000\u0000 Since index transitions only occur by way of changes to the margin, we need now only con￿rm that each of these transitions maintains continuous µj (\u0000 ) paths for all j 2 [n] in order to conclude the proof that the paths are piecewise-linear. ⇤ The linearity of paths µj (\u0000 ) for j 2 M\u0000 gives conditions on the ranges of \u0000 wherein individuals’ classi￿cation outcomes do not change. As such, for any given tolerance parameter \u0000, we can compute the \u0000\u0000 perturbations that yield no changes to individuals’ welfares. The following Proposition gives the analytical form of these stable regions, where although fairness appears to be “improv- ing” or “worsening,” the adjusted learning process has no material e￿ects on the classi￿catory outcomes that individuals receive. P￿￿￿￿￿￿￿￿￿￿ 3.3. Denote the optimal µ⇤ j values at an \u0000-fair SVM solution as µ\u0000 j for j 2 [n]. Let rj = ✓(K \u0000 )\u00001 ( \u0000n kuk2 v)◆ j with K \u0000 and v as de￿ned in (12) and (13), dj = X i 2M\u0000 ri\u0000i\u0000j h(I \u0000 Pu)xi , (I \u0000 Pu)xj i + r0\u0000j \u0000j = 1 \u0000 ✓ nX i=1 µ\u0000 i \u0000i (I \u0000 Pu)xi\u0000j (I \u0000 Pu)xj + n\u0000\u0000j hxj , ui kuk2 + b\u0000j ◆ (15) All perturbations of \u0000 in the range \u0000\u0000 2 ⇣ maxj mj , minj Mj ⌘ where mj = 8>>>>>>>>>>>< >>>>>>>>>>> : 8>< > : \u0000j dj , j 2 F \u0000 , dj > 0 \u00001, j 2 F \u0000 , dj < 0 min{ C\u0000µ \u0000 j r j , \u0000µ \u0000 j r j }, j 2 M\u0000 8>< > : \u00001, j 2 E\u0000 , dj > 0 \u0000j dj , j 2 E\u0000 , dj < 0 Mj = 8>>>>>>>>>>>< >>>>>>>>>>> : 8>< > : 1, j 2 F \u0000 , dj > 0 \u0000j dj , j 2 F \u0000 , dj < 0 min{ C\u0000µ \u0000 j r j , \u0000µ \u0000 j r j }, j 2 M\u0000 8>< > : \u0000j dj , j 2 E\u0000 , dj > 0 1, j 2 E\u0000 , dj < 0 (16) yield no changes to index memberships in the partition {F , M, E}\u0000 . We defer the interested reader to the Appendix for the full proof of this Proposition, though we provide a sketch here. The result follows from observing that the sensitivities r \u0000 i , 0 for i 2 M\u0000 de￿ned in (13) a￿ect the values @D \u0000 @µ j for all j 2 [n], and additional conditions must hold to ensure that the vectors that are not on the 540 Fair Classification and Social Welfare FAT* ’20, January 27–30, 2020, Barcelona, Spain margin are also unshu￿ed by the fairness perturbation. De￿ne \u0000\u0000 j = 1 \u0000 ✓ nX i=1 µ\u0000 i \u0000i (I \u0000 Pu)xi\u0000j (I \u0000 Pu)xj + n\u0000\u0000j hxj , ui kuk2 + b\u0000j ◆ (17) d\u0000 j = @D\u0000 @µj @\u0000 = X i 2M\u0000 r \u0000 i \u0000i\u0000j h(I \u0000 Pu)xi , (I \u0000 Pu)xj i + r \u0000 0 \u0000j (18) The \u0000\u0000 condition for stability of vectors xj for j < M\u0000 is given by \u0000\u0000 j d\u0000 j (19) Recall the conditions of membership in sets F and E as given in (7) and (9) respectively. The following observations are critical to computing the bounds of the stable region: For j 2 F \u0000 , perturbations \u0000\u0000 that increase \u0000\u0000 j do not threaten j’s exiting the set; if \u0000\u0000 decreases \u0000\u0000 j , then j can enter M\u0000 +\u0000\u0000 . Inversely, for j 2 E\u0000 , perturbations \u0000\u0000 that decrease \u0000\u0000 j ensure that j stays in the same partition, i.e., j 2 E\u0000 +\u0000\u0000 . Perturbations that increase \u0000\u0000 j can cause j to shu￿e into M\u0000 +\u0000\u0000 . For j 2 M\u0000 to stay in the margin, we need µ\u0000 +\u0000\u0000 j 2 [0, C]. Once µ\u0000 j hits either endpoint of the interval, j risks shu￿ing across to F \u0000 +\u0000\u0000 or E\u0000 +\u0000\u0000 . Computing these transition inequalities results in a set of condi- tions that ensure that a partition is stable. Since \u0000\u0000 can be either positive or negative, we take the maximum of the lower bounds (mj ) and the minimum of the upper bounds (Mj ) to arrive at the range of stable perturbations given in (16). We call the bounds of this interval the “breakpoints” of the solution paths. This Proposition reveals a mismatch between the ostensible changes to the fairness level of an \u0000-fair Soft-Margin SVM learning process and the actual felt changes in outcomes by the individuals who are subject to the system. This results from the simple fact that the optimization problem captures changes in the learner’s optimal solution but does not o￿er such ￿ne-grained information on how individuals’ outcomes vary as a result of \u0000\u0000 perturbations. So long as the fairness constraint is binding and its associated dual variable \u0000 > 0, then tightening or loosening a fairness constraint does alter the loss of the optimal learner classi￿er—the actual SVM solution changes—yet analyzed from the perspective of the individual agents xi , so long as the \u0000\u0000 perturbation occurs within the range given by (16), classi￿cations issued under this \u0000 + \u0000\u0000-fair SVM solution are identical to those under the \u0000-fair solution. Thus despite the apparent more “fair” signal that a classi￿er abiding by \u0000 + \u0000\u0000 < \u0000 sends, agents are made no better o￿ in terms of welfare. This result is summarized in the following Corollary. C￿￿￿￿￿￿￿￿ 3.4. Let {p(\u0000 ),W0 (\u0000 ),W1 (\u0000 )} be a triple expressing the welfares of the learner, group z = 0, and group z = 1 under the \u0000-fair SVM solution. Then for any \u0000\u0000 2 (maxj mj , 0) where mj is de￿ned in (16), {p(\u0000 ),W0 (\u0000 ),W1 (\u0000 )} % {p(\u0000 + \u0000\u0000 ),W0 (\u0000 + \u0000\u0000 ),W1 (\u0000 + \u0000\u0000 )}. Once we have demarcated the limits of \u0000\u0000 perturbations that yield no changes to the partition, i.e., {F , M, E}\u0000 = {F , M, E}\u0000 +\u0000\u0000 , we can move on to consider the welfare e￿ects of \u0000\u0000 perturbations that exceed the stable region outlined in Proposition 3.3. At each such breakpoint when \u0000\u0000 reaches maxj mj or minj Mj as de￿ned in (16), the margin set changes: M\u0000 , M\u0000 +\u0000\u0000 . As such, r \u0000 +\u0000\u0000 j for j 2 M\u0000 +\u0000\u0000 must be recomputed via (13). These sensitivities hold until the next breakpoint when the set M updates again. We can associate a group welfare with the classi￿cation scheme at each of the breakpoints. As already illustrated, index partitions are static in the stable regions around each breakpoint, so group welfares will also be unchanged in these regions. As such, we need only compute welfares at breakpoints to characterize the paths for \u0000 2 [0, 1]. This method of analysis allows practitioners to straight- forwardly determine whether the next \u0000 breakpoint actually trans- lates into better or worse outcomes for the group as a whole. Of the four possible events that occur at a breakpoint, index transitions between the partitions M and E correspond to changed classi￿cations that a￿ect group utilities. The following Proposi- tion characterizes those breakpoint transitions that e￿ect welfares triples {p(\u0000 ),W0 (\u0000 ),W1 (\u0000 )} for the learner, group z = 0, and group z = 1, that are strictly Pareto dominated by the welfare triple at a neighboring \u0000 breakpoint. The full proof is left to the Appendix. P￿￿￿￿￿￿￿￿￿￿ 3.5. Consider the welfare triple at the optimal \u0000-fair SVM solution given by {p(\u0000 ),W0 (\u0000 ),W1 (\u0000 )}. Let bL = maxj mj < 0 be the neighboring lower breakpoint where index ` = arg maxj mj ; let bU = minj Mj > 0 be the neighboring upper breakpoint where index u = arg minj Mj , assuming uniqueness in the arg max and arg min. If ` 2 E\u0000 and \u0000` = \u00001, or if ` 2 M\u0000 and \u0000` =+1, then {p(\u0000 + bL ),W0 (\u0000 + bL ),W1 (\u0000 + bL )}} \u0000 {p(\u0000 ),W0 (\u0000 ),W1 (\u0000 )} If u 2 E\u0000 and \u0000u =+1, or if u 2 M\u0000 and \u0000u = \u00001, then {p(\u0000 ),W0 (\u0000 ),W1 (\u0000 )} \u0000 {p(\u0000 + bU ),W0 (\u0000 + bU ),W1 (\u0000 + bU )} Thus minimizing loss in the presence of stricter fairness con- straints need not correspond to monotonic gains or losses in the welfare levels of social groups. Fairness perturbations do not have a straightforward e￿ect on classi￿cations. Further, these results do not only arise as an unfortunate outcome of using the particular proxy fairness constraint suggested by Zafar et al [5]. So long as the \u0000 parameter appears in the linear part of the dual Soft-Margin SVM objective function, the µj (\u0000 ) paths exhibit a piecewise linear form characterized by stable regions and breakpoints. Hence, these results apply to many proxy fairness criteria that have so far been proposed in the literature [5, 17, 28]. Even when the dual variable paths are not piecewise linear, so long as they are non-monotonic, fairer classi￿cation outcomes do not necessarily confer welfare bene￿ts to the disadvantaged group. Monotonicity in welfare space is mathematically distinct from monotonicity in fairness space. The preceding analyses show that although fairness constraints are often intended to improve classi￿cation outcomes for some disadvantaged group, they in general do not abide by the Pareto Principle, a common welfare economic axiom for deciding among social alternatives. That is, asking that an algorithmic procedure abide by a more stringent fairness criteria can lead to enacting classi￿cation schemes that actually make every stakeholder group worse-o￿. Here, the supposed “improved fairness” achieved by decreasing the unfairness tolerance parameter \u0000 fails to translate 541 FAT* ’20, January 27–30, 2020, Barcelona, Spain Lily Hu and Yiling Chen into any meaningful improvements in the number of desirable outcomes issued to members of either group. T￿￿￿￿￿￿ 3.6. Consider two fairness-constrained ERM programs parameterized by \u00001 and \u00002 where \u00001 < \u00002. Then a decision-maker who always prefers the classi￿cation outcomes issued under the “more fair” \u00001-fair solution to those under the “less fair” \u00002-fair solution does not abide by the Pareto Principle. 3.3 Algorithm and Complexity We build upon the previous section of translating fairness con- straints into individual welfare outcomes by considering the opera- tionalization of our analysis and its practicality. The algorithmic procedure presented in this section computes \u0000 breakpoints and tracks the solution paths of the µj (\u0000 ) for all individuals. Hence, the procedure enables the comparison of di￿erent social groups’ welfares—where welfare is determined by the machine’s allocative outcome—by aggregating the classi￿cation outcomes of all individ- uals j in a group z. Algorithm 1 outputs two useful fairness-relevant constructs that have as yet not been explored in the literature: 1) solution paths µj (\u0000 ) for j 2 [n] tracking individuals’ welfares, and 2) full \u0000 parameterized curves tracking groups’ welfares. The analysis of the previous section forms the backbone of the main update rules that construct the µj (\u0000 ) paths in Algorithm 1. In particular the values r \u0000 j , \u0000\u0000 j , and d\u0000 j as de￿ned in (13), (17), and (18) respectively are key to computing the \u0000 breakpoints, which in turn fully determine the piecewise linear form of µj (\u0000 ). There is, however, one corner case that the procedure must check that was not discussed in the preceding section. We had previously required that the matrix K \u0000 be invertible, which is the case whenever our objective function is strictly convex. But if the margin is empty, the standard update procedure, which computes sensitivities r \u0000 j and K \u0000 , will not su￿ce. The KKT optimality condition Pn i=1 µi\u0000i = 0 requires that the multiple indices moving in the margin at once must be positive and negative examples. For this reason we must refer to a di￿erent procedure to compute the \u0000 breakpoint at which this transition occurs. For continuity of the main text of this paper, the full exposition of this analysis is given in the Appendix. The following complexity result highlights the practicality of implementing the fairness-to-welfare mapping in Algorithm 1 to track the full solution paths of an \u0000-fair SVM program. We note that standard SVM algorithms such as LibSVM run in O (n3), and thus once the algorithm has been initialized with the unconstrained SVM solution, the complexity of computing both the full individual solu- tion paths µj (\u0000 ) and the full group welfare curves {W0 (\u0000 ),W1 (\u0000 )} is on the same order as that of computing a single SVM solution. T￿￿￿￿￿￿ 3.7. Each iteration of Algorithm 1 runs in O (n2 + |M|2). For breakpoints on the order of n, the full run time complexity is O (n3 + n|M|2). P￿￿￿￿. Each iteration of the fairness-to-welfare algorithm re- quires the inversion of matrix K \u0000 2 R|M\u0000 |+1 and the computations of r \u0000 j 2 R|M\u0000 | for j 2 M\u0000 , and \u0000\u0000 j and d\u0000 j for j 2 {F , E}\u0000 . The standard Gauss-Jordan matrix inversion technique runs in O (|M|3), but we take advantage of partition update rules to lower the number of computations: Since at each new breakpoint, the ALGORITHM 1: Fairness-to-welfare solution paths as a function of \u0000 Input: set X of n data points {xi, zi, \u0000i } Output: solutions paths µ (\u0000 ) and group welfare curves {W0 (\u0000 ), W1 (\u0000 ) } µ0 = arg minµ D (µ) of (0-fair SVM-D); \u0000 = 0, \u0000\u0000 = 0; |n0 | = Pn i =1 1[zi = 0], |n1 | = Pn i =1 1[zi = 1]; while \u0000 < 1 do W0 = 0, W1 = 0; for each µ \u0000 i do update {F , M, E}\u0000 according to (7), (8), (9); if (µi < C & \u0000i = 1) || (µi = C & \u0000i = 0) then Wzi = Wzi + 1; end end W0 (\u0000 ) = W0 n0 ; W1 (\u0000 ) = W1 n1 ; if |M\u0000 | = 0 then \u0000\u0000 = mini Mi as given in (26); update {F , M, E}\u0000 according to (28) and (29); \u0000 = \u0000 + \u0000\u0000 ; end compute r \u0000 , d \u0000 according to (13), (18); \u0000\u0000 = mini Mi as given in (16); µ \u0000 +\u0000\u0000 i = µ \u0000 i + r \u0000 i \u0000\u0000 for i 2 M\u0000 , µ \u0000 i = µ \u0000 +\u0000\u0000 i for i 2 { F , E}\u0000 ; \u0000 = \u0000 + \u0000\u0000 ; end return (µ (\u0000 ), W0 (\u0000 ), W1 (\u0000 )) partition tends to change because of additions or eliminations of a single index j from the set M, we can use the Cholesky decomposi- tion rank-one update or downdate to ease the need to recompute the full matrix inverse at every iteration, thereby reducing the com- plexity of the operation to O (|M|2). Computing the stability region conditions for j 2 {F , E} requires O ⇣(n \u0000 |M|)|M|⌘ steps. As such, at each breakpoint, the total computational cost is O (|M|2 + n2). The number of breakpoints for each full run of the algorithm depends on the data distribution and how sensitive the solution is to the constraint. As a heuristic, datasets whose fairness constraints bind for smaller \u0000 have fewer breakpoints. Previous empirical results on the full SVM path for L1 and L2 regularization have found that the number of breakpoints tends to be on the order of n [24– 27]. Thus after initialization with 0-fair SVM solution, the ￿nal complexity for the algorithm is O (n3 + n|M|2). ⇤ 3.4 Impact of Fair Learning on Learner’s Welfare Having proven the main welfare-relevant sensitivity result for groups, we return to more standard analysis of the e￿ect of \u0000\u0000 perturbations on the learner’s loss. In this case, we directly solve for the dual variable of the fairness constraint. Recall \u0000 ⇤ from (23): \u0000 ⇤ = \u0000 ⇤ 1 \u0000 \u0000 ⇤ 2 = n(n(\u0000\u0000 \u0000 \u0000+) + Pn i=1 µi\u0000i hxi , ui) kuk2 (20) By complementary slackness, one of \u0000\u0000 and \u0000+ is zero, and the other is \u0000. In particular, if \u0000\u0000 = 0, then \u0000+ = \u0000, then we know that \u0000 > 0. Thus the original fairness constraint that binds is the upper bound on covariance, suggesting that the optimal classi￿er 542 Fair Classification and Social Welfare FAT* ’20, January 27–30, 2020, Barcelona, Spain must be constrained to limit its positive covariance with group z = 1. If \u0000+ = 0, then \u0000\u0000 = \u0000 and \u0000 < 0, and the classi￿er must be constrained to limit its positive covariance with group z = 0. We can interpret the value of the dual variable Lagrange mul- tiplier as the shadow price of the fairness constraint. It gives the additional loss in the objective value that the learner would achieve if the fairness constraint were in￿nitesimally loosened. Whenever a fairness constraint binds, its shadow price is readily computable and is given by \u0000\u0000\u0000 ⇤\u0000\u0000. It bears noting that because (\u0000-fair Soft-SVM) is not a linear program, \u0000\u0000\u0000 ⇤\u0000\u0000 can only be interpreted as a measure of local sensitivity, valid only in a small neighborhood around an optimal solution. But through an alternative lens of sensitivity analysis, we can derive a lower bound on global sensitivity due to changes in the fairness tolerance parameter \u0000. By writing \u0000 as a perturbation vari- able, we can perform sensitivity analysis on the same \u0000-constrained problem. Returning to the perturbation function p(\u0000 ), we have p(\u0000 ) \u0000 sup µ,\u0000 {L(µ⇤, \u0000 ⇤) \u0000 \u0000\u0000\u0000\u0000 ⇤\u0000\u0000} (21) where L(µ⇤, \u0000 ⇤) gives the solution to the 0-fair SVM problem. L(µ⇤, \u0000 ⇤) = max µ 2[0,C]n,\u0000 \u0000 1 2 \u0000\u0000\u0000\u0000\u0000\u0000 nX i=1 µi\u0000i (I \u0000 Pu )xi \u0000\u0000\u0000\u0000\u0000\u0000 2 + X i=1 µi (22) The perturbation formulation given in (21) is identical in form to the original program (\u0000-fair-SVM1-P) but gives a global bound on p(\u0000 ) for all \u0000 2 [0, 1]. Since (21) gives a lower bound, the global sensitivity bound yields an asymmetric interpretation. P￿￿￿￿￿￿￿￿￿￿ 3.8. If \u0000\u0000 < 0 and \u0000\u0000\u0000 ⇤\u0000\u0000 \u0000 0, then p(\u0000 +\u0000\u0000 ) \u0000p(\u0000 ) \u0000 0. If \u0000\u0000 > 0 and \u0000\u0000\u0000 ⇤\u0000\u0000 < \u0000 for small \u0000 , then p(\u0000+\u0000\u0000 )\u0000p(\u0000 ) 2 [\u0000\u0000 \u0000\u0000, 0], and is thus also small in magnitude. Proposition 3.8 shows that tightening the fairness constraint when its shadow price is high leads to a great increase in learner loss, but loosening the fairness constraint when its shadow price is small leads only to a small decrease in loss. 4 EXPERIMENTS To demonstrate the e￿cacy of our approach, we track the im- pact of \u0000-fairness constrained SVM programs on the classi￿cation outcomes of individuals in the Adult dataset. The target variable in the dataset is a binary value indicating whether the individual has an annual income of more or less than $50,000. If such a dataset were used to train a tool to be deployed in consequential resource allocation—say, for the purpose of determining access to credit— then classi￿cation decisions directly impact individuals’ welfares. Individual solution paths and relative group welfare changes are given in Figure 1. As \u0000 increases from left to right, the fairness constraint is loosened, and outcomes become “less fair.” In the case of the \u0000-fair SVM solution to the Adult dataset, the fairness constraint ceases to bind at the optimal solution when \u0000 ⇡ 0.175. The top panel shows example individual piecewise linear paths of dual variables µi (\u0000 ), providing a visual depiction of how individual points can transition across index sets: from µi = 0, i 2 F and being correctly labeled, to µi 2 (0, 1), i 2 M, being correctly labeled but in margin; to µi = 1, i 2 E and being incorrectly labeled. Solid paths indicate individuals coded female; dashed paths indicate those coded males. As the top panel of Figure 1 shows, the actual “journey” of these paths are varied as \u0000 changes. As expected, tightening the fairness constraint in the \u0000-fair pro- gram does tend to lead to improved welfare outcomes for females as a group (more female individuals receive a positive classi￿ca- tion), while males experience a relative decline in group welfare (receiving fewer positive classi￿cations). However, as suggested by our results in Section 3.2, these welfare changes are not monotonic for either group. Tightening the fairness constraint could lead to declines in both groups’ welfares, demonstrating that preferring more fair solutions in this predictive model does not abide by the Pareto Principle. We highlight an instance of this result in the bot- tom panel of Figure 1, where orange dashed lines to the left of black ones mark o￿ solutions where “more fair” outcomes (orange) are Pareto-dominated by “less fair” (black) ones. A practitioner working in a domain in which welfare considerations might override parity- based fairness ones may prefer the outcomes of a fair learning procedure with \u0000 ⇡ 0.045 to one with \u0000 ⇡ 0.015. Additional plots showing absolute changes in group welfare and optimal learner value are given in the Appendix. 5 DISCUSSION The question that leads o￿ this paper—How do leading notions of fairness as de￿ned by computer scientists map onto longer-standing notions of social welfare?—sets an important agenda to come for the ￿eld of algorithmic fairness. It asks that the community look to disciplines that have long considered the problem of allocating goods in accordance with ideals of justice and fairness. For example, the notion of welfare in this paper draws from work in welfare and public economics. The outcomes issued by an optimal classi￿er can, thus, be interpreted using welfare economic tools developed for considerations of social e￿ciency and equity. In an e￿ort to situate computer scientists’ notions of fairness within a broader under- standing of distributive justice, we also show that loss minimization problems can indeed be mapped onto welfare maximization ones and vice versa. For reasons of continuity, analyses of this correspon- dence do not appear in the main text—we defer the interested reader to the Appendix—though we present an abbreviated overview here. We encourage readers to consider the main results of this paper, which construct welfare paths out of fair learning algorithms, as a part of this larger project of bridging the two approaches. 5.1 Bridging Fair Machine Learning and Social Welfare Maximization To highlight the correspondence between the machine learning and welfare economic approaches to allocation, we show that loss minimizing solutions can be understood as welfare maximizing ones under a particular social welfare function. In the Planner’s Problem, a planner maximizes social welfare represented as the weighted sum of utility functions. Inverting the Planner’s Problem gives a question concerning social equity: “Given a particular allocation, what is the presumptive social weight function that would yield it as optimal?” We show that the set of predictions issued by the optimal classi￿er of any loss minimization task can be given as the set of allocations in the Planner’s Problem over the same individuals endowed with a set of welfare weights. Analyzing the distribution of 543 FAT* ’20, January 27–30, 2020, Barcelona, Spain Lily Hu and Yiling Chen Figure 1: Fairness-to-welfare solution paths for individuals (top panel) and groups (bottom panel) on the Adult dataset. implied weights of individuals and groups o￿ers a welfare economic way of considering the “fairness” of classi￿cations. We also derive a converse result: “Given a social welfare maximizing allocation, what model that can achieve an equivalent classi￿cation?” Our solution’s approach records the set of welfares, de￿ned by the number of positively labeled individuals, achievable for each social group. 5.2 Interpreting Welfare Alongside Fairness Welfare economics can lend particular insights into formalizing notions of distributional fairness and general insights into building a “technical” ￿eld and methodology that grapples with normative questions. The ￿eld is concerned with what public policies ought to be, how to improve individuals’ well-beings, and what distribution of outcomes are preferable. Answers to these questions appeal to values and judgments that refer to more than just descriptive or predictive facts about the world. The success of fair machine will largely hang on how well it can adapt to a similar ambitious task. However, welfare economics is not the only—nor should it even serve as the main—academic resource for thinking through how goods ought to be provisioned in a just society. In this moment of broad appeal to the prowess of algorithmic systems, researchers in computing are called on to advise on matters beyond their special- ized expertise and training. Many of these matters require explicit normative, political, and social-scienti￿c reasoning. Insights and methods from across the arts, humanities, social sciences, and nat- ural sciences bear fruit in answering these questions. This paper does not look to contribute a new fair learning algo- rithm or a new fairness de￿nition. We take a popular classi￿cation algorithm, the Soft Margin SVM, append a parity-based fairness constraint, and analyze its implications on welfare. The constraint that we center in the paper is just one concretization of a large menu of fairness notions that have been o￿ered up to now. The method of analysis developed in the paper applies generally to any convex formulations of these constraints, including versions of balance for false positives, balance for false negatives, and equality of opportunity that have circulated in the literature [17, 18, 28]. It is important future work to investigate the welfare implications of state-of-the-art fair classi￿cation algorithms that the community continues to develop, which can deal with a wider range of models and constraints, including non-convex ones. This paper asks that researchers in fair machine learning reeval- uate not only their lodestars of optimality and e￿ciency but also their latest metrics of fairness. By viewing classi￿cation outcomes as allocations of a good, we incorporate considerations of indi- vidual and group utility in our analysis of classi￿cation regimes. The concept of “utility” in evaluations of social policy remains controversial, but in many cases of social distribution, utility con- siderations provide a partial but still important perspective on what is at stake within an allocative task. Utility-based notions of welfare can capture the relative bene￿t that a particular good can have on a particular individual. If machine learning systems are in e￿ect serving as resource distribution mechanisms, then questions about fairness should align with questions of “Who bene￿ts?” Our results show that many parity-based formulations of fairness do not ensure that disadvantaged groups bene￿t. Preferring a classi￿er that better accords with a fairness measure can lead to selecting allocations that lower the welfare for every group. Nevertheless, there remain reasons in favor of limiting levels of inequality not re￿ected in utilitarian calculus. In some cases, the gap between groups is itself objectionable, and minimizing this di￿erence overrides maximiz- ing the absolute utility level of disadvantaged groups. But without acknowledging and accounting for these reasons, well-intentioned optimization tasks that seek to be “fairer” can further disadvantage social groups for no reason but to satisfy a given fairness metric. 544 Fair Classification and Social Welfare FAT* ’20, January 27–30, 2020, Barcelona, Spain REFERENCES [1] Amartya Sen. Equality of What? Cambridge University Press, Cambridge, 1980. Reprinted in John Rawls et al., Liberty, Equality and Law (Cambridge: Cambridge University Press, 1987). [2] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259–268. ACM, 2015. [3] Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems, pages 3315–3323, 2016. [4] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 214–226. ACM, 2012. [5] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classi￿cation without disparate mistreatment. In Proceedings of the 26th Inter- national Conference on World Wide Web, pages 1171–1180. International World Wide Web Conferences Steering Committee, 2017. [6] Yahav Bechavod and Katrina Ligett. Learning fair classi￿ers: A regularization- inspired approach. arXiv preprint arXiv:1707.00044, 2017. [7] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2):153–163, 2017. [8] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-o￿s in the fair determination of risk scores. In Proceedings of the 8th Innovations in Theoretical Computer Science Conference, pages 43:1–43:23. ACM, 2017. [9] Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regularization approach. In Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on, pages 643–650. IEEE, 2011. [10] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325–333, 2013. [11] Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, pages 325–333, 2016. [12] Geo￿ Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and calibration. In Advances in Neural Information Processing Systems, pages 5680–5689, 2017. [13] Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ra- mamurthy, and Kush R Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural Information Processing Systems, pages 3992– 4001, 2017. [14] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances in Neural Information Processing Systems, pages 4066–4076, 2017. [15] Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Schölkopf. Avoiding discrimination through causal reasoning. In Advances in Neural Information Processing Systems, pages 656–666, 2017. [16] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. In Inter- national Conference on Machine Learning, pages 2569–2577, 2018. [17] Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massim- iliano Pontil. Empirical risk minimization under fairness constraints. In Advances in Neural Information Processing Systems, pages 2791–2801, 2018. [18] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classi￿cation. In International Conference on Machine Learning, pages 60–69, 2018. [19] Sendhil Mullainathan. Algorithmic fairness and the social welfare function. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 1–1. ACM, 2018. [20] Hoda Heidari, Claudio Ferrari, Krishna Gummadi, and Andreas Krause. Fairness behind a veil of ignorance: A welfare analysis for automated decision making. In Advances in Neural Information Processing Systems, pages 1265–1276, 2018. [21] Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical review of fair machine learning. arXiv preprint arXiv:1808.00023, 2018. [22] Lydia Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In International Conference on Machine Learning, pages 3156–3164, 2018. [23] Christopher P Diehl and Gert Cauwenberghs. Svm incremental learning, adapta- tion and optimization. In Neural Networks, 2003. Proceedings of the International Joint Conference on, volume 4, pages 2685–2690. IEEE, 2003. [24] Trevor Hastie, Saharon Rosset, Robert Tibshirani, and Ji Zhu. The entire reg- ularization path for the support vector machine. Journal of Machine Learning Research, 5(Oct):1391–1415, 2004. [25] Li Wang, Ji Zhu, and Hui Zou. The doubly regularized support vector machine. Statistica Sinica, 16(2):589, 2006. [26] Gang Wang, Dit-Yan Yeung, and Frederick H Lochovsky. A kernel path algorithm for support vector machines. In Proceedings of the 24th international conference on Machine learning, pages 951–958. ACM, 2007. [27] Masayuki Karasuyama, Naoyuki Harada, Masashi Sugiyama, and Ichiro Takeuchi. Multi-parametric solution-path algorithm for instance-weighted support vector machines. Machine learning, 88(3):297–330, 2012. [28] Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-discriminatory predictors. In Conference on Learning Theory, pages 1920–1953, 2017. [29] Marc Fleurbaey and François Maniquet. A theory of fairness and social welfare, volume 48. Cambridge University Press, 2011. [30] Emmanuel Saez and Stefanie Stantcheva. Generalized social marginal welfare weights for optimal tax theory. American Economic Review, 106(1):24–45, 2016. [31] Lucy F Ackert, Jorge Martinez-Vazquez, and Mark Rider. Social preferences and tax policy design: some experimental evidence. Economic Inquiry, 45(3):487–501, 2007. [32] Floris T Zoutman, Bas Jacobs, and Egbert LW Jongen. Optimal redistributive taxes and redistributive preferences in the netherlands. Erasmus University Rotterdam, 2013. [33] Vidar Christiansen and Eilev S Jansen. Implicit social preferences in the nor- wegian system of indirect taxation. Journal of Public Economics, 10(2):217–245, 1978. [34] Matthew Adler. Well-being and fair distribution: beyond cost-bene￿t analysis. Oxford University Press, 2012. [35] Marc Fleurbaey, François Maniquet, et al. Optimal taxation theory and princi- ples of fairness. Technical report, Université catholique de Louvain, Center for Operations Research and ?, 2015. [36] Ilyana Kuziemko, Michael I Norton, Emmanuel Saez, and Stefanie Stantcheva. How elastic are preferences for redistribution? evidence from randomized survey experiments. American Economic Review, 105(4):1478–1508, 2015. [37] Herbert Edelsbrunner and Ernst Peter Mücke. Simulation of simplicity: a tech- nique to cope with degenerate cases in geometric algorithms. ACM Transactions on Graphics (tog), 9(1):66–104, 1990. 545","libVersion":"0.0.0","langs":"","hash":"","size":0}