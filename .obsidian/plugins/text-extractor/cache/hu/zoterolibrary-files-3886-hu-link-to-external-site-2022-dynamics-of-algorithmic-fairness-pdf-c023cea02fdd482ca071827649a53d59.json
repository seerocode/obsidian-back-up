{"path":".obsidian/plugins/text-extractor/cache/hu/zoterolibrary-files-3886-hu-link-to-external-site-2022-dynamics-of-algorithmic-fairness-pdf-c023cea02fdd482ca071827649a53d59.json","text":"HARVARD UNIVERSITY Graduate School of Arts and Sciences DISSERTATION ACCEPTANCE CERTIFICATE The undersigned, appointed by the Harvard John A. Paulson School of Engineering and Applied Sciences and Philosophy have examined a dissertation entitled: “Dynamics of Algorithmic Fairness” presented by: Lily Hu Signature __________________________________________ Typed name: Professor Y. Chen Signature __________________________________________ Typed name: Professor B. Grosz Signature __________________________________________ Typed name: Professor C. Dwork April 12, 2022 Dynamics of Algorithmic Fairness a dissertation presented by Lily Hu to The Department of The School of Engineering and Applied Sciences in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the subject of Applied Mathematics Harvard University Cambridge, Massachusetts April 2022 ©2022 – Lily Hu all rights reserved. Thesis advisor: Professor Yiling Chen Lily Hu Dynamics of Algorithmic Fairness Abstract The rise of machine learning-based predictive models in making decisions of profound social im- pact has spurred study of those technical properties that may bear on the moral and political char- acter of their deployment. One area of normative concern that has received particularly heightened scrutiny is the fairness of the outcomes that data-based classification tools issue. Much computer sci- ence and mathematics-based research in the fields of algorithmic fairness and fair machine learning looks to diagnose when classifiers may be engaging in discrimination or otherwise generating unfair outputs and to prevent such outcomes using a variety of methods that seeks to alter the classifier’s behavior and thus the outcomes it produces. This dissertation comprises contributions to the burgeoning field of algorithmic fairness that, rather than focusing on the internal workings of an algorithmic system itself, centers instead the in- teraction between machine classifications and the broader societal contexts within which data-based predictive tools are embedded. Each of these works thus conceive of algorithmic tools as only one component of a larger sociotechnical system that distributes key social benefits and burdens. Over the span of the three projects contained within—“Disparate Effects of Strategic Manipulation,” “A Short-term Intervention for Long-term Fairness,” and “Fair Classification and Social Welfare”—it considers changes to institutional incentive structures that data-based classification introduces, the strategic responses of agents who interact with such systems, and the welfare impacts of various fair- ness constraints that have been proffered in the field. Approaching the fairness problem with this wider lens of analysis builds in a broader and longer-term perspective from the start and necessar- ily draws on methods and insights beyond that of applied mathematics and computer science. In so doing, this research makes distinctive contributions to matters that are central in the scholarly discourse in algorithmic fairness, such as debate about fairness-accuracy trade-offs in algorithmic decision-making and the strategic interplay between machine classifications and agent behaviors. This dissertation therefore both advocates for and itself exemplifies a reorientation to questions of fairness by shifting focus from the machine as the central object of interest in favor of a broader vantage that addresses the broader social dynamics of algorithmic fairness. This approach not only challenges the standard methodological tacks taken in the field of algorithmic fairness but also gen- erates insights that track more closely to how these tools actually operate in the world to effect key social outcomes. It thus is better suited to guiding work in algorithmic fairness towards the kinds of interventions we will need to construct a more equitable society. iii Contents Title Page i Copyright ii Abstract iii Acknowledgments vi 1 Introduction 1 2 Disparate Effects of Strategic Manipulation 10 2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2 Model Formalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.3 Equilibrium Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.4 Learner Subsidy Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3 A Short-term Intervention for Long-term Fairness 41 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 3.2 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 4 Fair Classification and Social Welfare 64 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.2 Problem Formalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 4.3 Welfare Impacts of Fairness Constraints . . . . . . . . . . . . . . . . . . . . . . 70 4.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 4.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 5 Conclusion 96 iv Appendix A Appendices 101 A.1 Appendix for Chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 A.2 Appendix for Chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 A.3 Appendix for Chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 References 154 v Acknowledgments Thank you to my collaborators on these projects: Yiling Chen, Nicole Immorlica, and Jenn Wortman Vaughan. To my committee: To Cynthia and Barbara, who taught me through their examples what it is to embody research and personal excellence, how to pursue research with great technical skill but also grounded in a deep scholarly sensibility that is all too rare. Your own work and the way you approach research is excellent by the standards of computer science and observing you both, I think, part of how you manage that excellence is by being so open-minded and receptive to the contributions of other dis- ciplines. Seeing you both engage deeply beyond computer science assured me that my choice to pursue philosophy would not be taken by either of you as a failure. You are each absolute paragons of trailblazing and most remarkably, you supported me even when the trail that I wanted to blaze was different from your own. To Yiling, who took in a student who did not know an ounce of real “computer science,” whose experience coding was zilch, and through patience and a delicate balance of directed guidance along- side an attitude that encouraged exploration, transformed her into someone who found passion in a growing area of computer science and, surprisingly!, had something to contribute to it. Then when she make the crazy decision to take on a Philosophy degree, you supported the choice nonetheless— also in part on your dime. It took immense faith in me and a selflessness that few advisers have. Even fewer would have gone down all these rabbit holes with me, jumped through so many administra- tive hoops, into this great unknown, uncharted territory. I would be in a completely different place without your guidance. Your support in the past six years has quite literally transformed and contin- ues to transform my academic career and my life. Thank you to all my interlocutors, friends, and family all these years who have carried me through so many triumphs and even more defeats. I cannot express my gratitude with sufficient eloquence, so I’ll just leave it at that: immense thank you truly from the bottom of my heart. vi 1 Introduction Before there was anything nearing a fully-fledged discipline or an established research community dedicated to studying the so-called fairness properties of algorithmic systems, a small group of schol- ars identified a risk of using machine classification tools in decision procedures that distribute key social benefits and burdens. Works such as Dwork et al.’s “Fairness Through Awareness” 30 and Kamishima et al.’s “Fairness-aware Learning through Regularization Approach” 53 drew out a po- tential problem inherent to the task of machine-based classification. The aim of these classification 1 systems is to draw distinctions among individuals, sorting them into types vis-à-vis some outcome of interest, and labeling them accordingly. But not all ways of differentiating individuals are per- missible; some are morally wrongful and even legally prohibited. This raises a challenge: How can we make sure that data-based classifiers, tools that are optimized to be highly discriminating in- struments of classification, do not discriminate in the wrongful sense on the basis of salient social groups like sex and race? How can those who design machine learning systems ensure that when they are deployed in systems that distribute important social goods such as access to loans, employ- ment opportunities, and second-chances at public life, that they do not systematically exclude indi- viduals on account of their race, sex, religion, and so on? How would we even know whether and when the highly complex patterns and distinctions that data-based algorithms trace out constitute unfair or discriminatory classification? And most importantly, how can we prevent classifiers from engaging in such discrimination? These are the 10,000 foot questions of algorithmic fairness. Early works in computer science formalized this problem in distinctive ways by defining mathematical or computational notions of “fairness” or “unfairness” or “discrimination,” and then providing approaches to resolving fair- ness problems or describing conditions under which solutions could or could not be found. The framework set by these early papers for how to approach technical questions of algorithmic fairness has remained highly influential to this day. These works also set the foundation upon which, in the following several years, what I will call the first wave of research on algorithmic fairness took place. This wave of works investigated formal properties of the classification system as the primary site of fairness or discrimination concerns. Seminal works such as Hardt et al.’s “Equality of Opportunity in Supervised Learning” and Zafar et al.’s “Fairness Constraints: Mechanisms for Fair Classification” probe the internal dynamics of classification and ask questions such as: How does the classifier treat the data inputs it receives? How well does a classifier’s output meet various mathematical criteria of fairness? Research in this vein typically centers on a machine’s optimization problem or a model of 2 classifier behavior and then devises methods to ensure some formal fairness definition is met given a set of assumptions about the classifier setup. This overall approach has contributed greatly to our understanding of the many ways that algorithmic systems can raise concerns of discrimination. Such work has offered up many mathematical formalizations of fairness that have intuitive appeal and can be implemented in practice, illuminated the different sources of bias in models constructed from data, and shown the prospects and limitations of various de-biasing methods, among other key in- sights. These formalizations of the problem, however, abstract from an important feature of algorithmic decision-making in practice. Socially impactful algorithms do not do anything on their own. They are embedded within existing institutions, plug into other pipelines of decision-making and into larger systems of rules and procedures. This integration with other features of social life is crucial to explaining how numbers and code in a machine can reach out into the world to bestow benefits or inflict harms on real people. Work that approaches fairness by looking to explicitly account for these factors outside of the machine make for what I will call a second wave of research in algorithmic fairness. And I take the works contained in this dissertation to be a small part of this overall effort. I have titled this dissertation “Dynamics of Algorithmic Fairness” because the approach that I take to questions of fairness is centrally concerned with developing a perspective on fair machine learning that centers the interplay between machines, humans, institutions, and social structures. Importantly, however, my research project is still rooted in a mathematical perspective despite its emphasis on the so-called “sociotechnical” nature of algorithmic systems. I take sociotechnical fea- tures as a starting point of my analyses but proceed with a primarily mathematical and computa- tional orientation, using tools from, as examples, learning theory and game theory. Still, while the works in this dissertation are in conversation with computer scientists working in the first wave of research in algorithmic fairness, rather than taking their setups and questions as given, I develop a distinctive formulation of longer-standing problems by highlighting the definite forms that algorith- 3 mic decision-making takes when such systems are embedded in a dynamic social world. This shifted orientation gives rise to two themes that run through my research projects in algo- rithmic fairness, and I will now discuss each in turn. The first conveys a distinctive methodological tack taken in the chapters that follow. Since I am centrally interested in what arises out of the in- terplay between our algorithmic systems and the social systems within which they are embedded, the lens I take to the problem of fairness has a notably wider scope. I analyze classifier behavior and outputs by considering how agents interact with machine classification against a background social structure and set of institutions. This wider lens of analysis comprises of three key features: 1. Agents who interact with machines are strategic and heterogeneous. 2. Ours is a world characterized by persistent social inequalities. 3. Classification outcomes plug into and affect other features of social systems. The first point reminds us that machine classifiers draw on data that are produced by individuals who are not just random draws from a probability distribution. They are reactive; they are strategic. Their behaviors are a function of their interests and their environment. Insofar as agents have differ- ent environments and often different interests, these data are heterogeneous in a deep sense. Models of machine behavior must explicitly account for the fundamentally social nature of data. Second, if the world is indeed characterized by social inequality, then that fact must have some origin, some story that explains how it is so, which must be accounted for explicitly in one’s model of it. One cannot address social inequality, in my view, without having a theory for how it arises and why it continues to be such a seemingly permanent fixture in our world. The third point ties all this together: the social system is a system that links together agents, in- stitutions, actions, beliefs, across time. Presently observed data are the product of a long lineage of previous social choices and conditions, and by the same token, a classification system’s issued out- comes will serve to have ripple effects into the future. A dynamic model incorporates these features 4 of temporality to string together a continuous picture of the world to explain why data look the way they do now, why people and in turn, why data-based machines behave the way they do, and what inertial forces exist to lock us into various steady state equilibria. My claim is that we can only make sense of these crucial facts if we pursue a dynamic and historical understanding of our social systems. Two projects within this dissertation are illustrative of this wider lens of analysis. In Chapter 2, ti- tled “The Disparate Effects of Strategic Manipulation,” Nicole Immorlica, Jenn Wortman Vaughan, and I probe the fairness properties of data-based classifiers that are optimized for a setting of strate- gic classification. We develop a model of strategic interaction between, on the one hand, those can- didates who are being classified, and on the other, the learner who is doing the classifying. Candi- dates in our model are heterogeneous in their strategic behaviors; their ability to respond strategi- cally to a classifier is a function of the costs they face when looking to “trick” a learner by manipu- lating their features. In cases of real world classification, an agent’s costs are not simply a function of their personal interest in receiving a positive classification but is bound up in a complex web of so- cial factors that affect her ability to pursue certain action responses. In a setting of social inequality, those in disadvantaged groups face systematically higher costs than those in advantaged groups. Our results show that whenever one group’s costs are higher than the other’s in the strategic classification game, the learner’s equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some candidates who are members of the advantaged group, while erroneously excluding some candidates who are members of the disadvantaged group. The fact that interplay between a classifier and the broader strategic environment within which it is embedded may result in inequality-reinforcing feedback loops is a theme that runs also through the following chapter, titled “A Short-term Intervention for Long-term Fairness in the Labor Mar- ket.” In this chapter, Yiling Chen and I build a dynamic reputational model of firms and agents contracting in a labor market, which shows how unequal group outcomes may be immovable even 5 when employers’ hiring decisions are bound by an input-output notion of “individual fairness.” This is precisely because the interaction between heterogeneous agents belonging to different social groups and firms generates feedback effects resulting from groups’ divergent accesses to resources and as a result, investment choices, which then serve to reinforce asymmetric outcomes over time. To counter this outcome, we construct a dual labor market composed of a Temporary Labor Mar- ket (TLM), in which firms’ hiring strategies are constrained to ensure statistical parity of workers granted entry into the pipeline, and a Permanent Labor Market (PLM), in which firms hire top performers as desired. Individual worker reputations produce externalities for their group; the cor- responding feedback loop raises the collective reputation of the initially disadvantaged group via a TLM fairness intervention that need not be permanent. The feedback mechanism of the dynamic system is thus co-opted to bring about a regime of group-equitable outcomes. The second theme that is characteristic of my works in this dissertation concerns a set of ques- tions about trade-offs that have been central to the field of algorithmic fairness since its inception and on which my approach sheds, I think, distinctive and illuminating light. Questions of trade-offs concern how different desiderata of predictive algorithms should be weighed against each other. How should an interest in accuracy be traded-off for a concern for fairness? Some researchers have taken the existence of such trade-offs—the fact that one cannot simultaneously make progress on a system’s fairness properties without losing progress on the accuracy front—to be ”inevitable” features of machine classification 24,36,61. Others have taken trade-offs to be an artifact of label bias or some other contingent assumption about the data that are not true in many cases 91,29. More broadly, the debate centers the question of whether in the realm of algorithmic decision-making, we are essentially in a game of compromises, or whether, under certain circumstances, a win-win is possible. This matter has been taken to be one of the most fundamental disputes in the field. An analysis that takes a dynamic approach and embeds algorithms in particular social and eco- 6 nomic contexts shows the limitations of the standard framings of the trade-offs question, guides towards more interesting dimensions of the question, and also lends new insight into the problem more generally. My works enter the debate by bringing to light two facts and from there, analyz- ing questions of trade-offs from quite a different angle. First, once we take the problem of fairness to be a problem that emerges out of the interaction between algorithms and the network of social institutions within which they are embedded, the field’s standard notions of fairness and accu- racy no longer appear as the only central values at stake. Other values—values such as welfare and efficiency—emerge as normatively significant as well and how these values trade-off against fairness and accuracy is not well understood. For example, as I show in an extension of the strategic classi- fication model in Chapter 2, even when members of a disadvantaged group have their higher costs subsidized by the learner, they are not necessarily made better-off by their greater ability to manipu- late. Here we prove a rather paradoxical result about trade-offs: there exist cases in which providing a subsidy improves only the learner’s utility while actually making both candidate groups worse- off—even the disadvantaged group that receives the subsidy. Second, conceiving of algorithms as a part of a dynamic social system that inherits data from the past and produces results that play a hand in influencing outcomes well into the future reveals the standard trade-offs conversation to be besot with a troubling ambiguity. When precisely are we to evaluate an algorithm’s ”fairness” and ”accuracy”? What does it mean to claim that an algorithm is fair? At what time horizon are we even evaluating an algorithm’s fairness or accuracy? If, as dy- namic models show, the accuracy and fairness features of some system change over time, then there is no sense in which an algorithm just is or is not fair, or is or is not accurate. Fairness and accuracy claims that may hold at a given point in time do not hold necessarily for any later point in time. This is exemplified in our model of the labor market in Chapter 3 within which the fairness constraint that we propose does not immediately realize fair outcomes; it only generates group-equitable out- comes at steady-state in the long-term. Still, we show that the move to the group-fair regime may be 7 desirable even for those who care only about the efficiency of the labor market or the welfare of em- ployers. We prove that there exist market conditions under which the group-equitable equilibrium Pareto-dominates the group-inequitable ones arising from strategies that statistically discriminate or employ a “group-blind” criterion. Hence, the trade-off story in this setting is certainly more compli- cated but also more optimistic. I tackle the relationship between fairness and welfare head on in Chapter 4’s “Fair Classification and Social Welfare,” a project in which Yiling Chen and I present a welfare-based analysis of fair clas- sification regimes. We ask about the broad set of works that adopt formal parity-based definitions of fairness the following question: How do leading notions of fairness as defined by computer sci- entists map onto longer-standing notions of social welfare? Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. Our method of analysis, which maps changes in “fairness” space into changes in “welfare” space, assesses whether and which fair learning procedures result in classifi- cation outcomes that make groups better-off welfare-wise. Do gains in fairness always result in gains in the welfare of disadvantaged groups and losses in the welfare of advantaged group? In a surpris- ing result, we show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring “more fair” classifiers does not abide by the Pareto Principle—a fundamental axiom of social choice theory and welfare economics. This makes for another complication in the standard ”trade-offs” narrative: improving along the axis of “fairness” may not necessarily translate into actually improved outcomes for any individuals or groups. This raises an important question: what exactly are gains in fairness for? The works in this dissertation build upon while also challenging paradigms in the algorithmic fairness literature. Even in the short time that has passed since the field’s first significant wave of work, many scholars in the field have recognized the need to consider classifier behaviors as only one 8 aspect of sociotechnical systems, which must as a whole be the target of inquiry into matters of fair- ness and discrimination. I take this to be a healthy sign of a growing consensus within the commu- nity about the limitations of an approach to fairness that considers only the computational features of some classifier or algorithm and an indication of the field’s continued development. I thus close this dissertation with some reflections on the field’s progression in the time I have been fortunate to contribute to it, and some suggestions for fruitful next steps that the community of scholars work- ing on algorithmic fairness can take on in the coming years to both build on past strands of research as well as grow in new directions. 9 2 Disparate Effects of Strategic Manipulation 2.1 Introduction The expanding realm of algorithmic decision-making has not only altered the ways that institutions conduct their day-to-day operations, but has also had a profound impact on how individuals in- terface with these institutions. It has changed the ways we communicate with each other, receive crucial resources, and are granted important social and economic opportunities. In theory, algo- 10 rithms have great potential to reform existing systems to become both more efficient and equitable, but as exposed by various high-profile investigations 87,77,6,34, prediction-based models that make or assist with consequential decisions are, in practice, highly prone to reproducing past and current patterns of social inequality. While few algorithmic systems are explicitly designed to be discriminatory, there are many un- derlying forces that drive socially biased outcomes. For one, since most of the features used in these models are based on proxy, rather than causal, variables, outputs often reflect the various structural factors that bear on a person’s life opportunities rather than the individualized characteristics that decision-makers often seek. Much of the previous work in algorithmic fairness has examined a par- ticular undesirable proxy effect in which a classifier’s features may be linked to socially significant and legally protected attributes like race and gender, interpreting correlations that have arisen due to centuries of accumulated disadvantage as genuine attributes of a group of people marked as mem- bers of some social category. 51,80,60,43 But algorithmic models do not only generate outcomes that passively correlate with social ad- vantages or disadvantages. These tools also provoke a certain type of reactivity, in which agents see a classifier as a guide to action and actively change their behavior to accord with the algorithm’s pref- erences. On this view, classifiers both evaluate and animate their subjects, transforming static data into strategic responses. Just as an algorithm’s use of certain features differentially advantages some populations over others, the room for strategic response that is inherent in many automated sys- tems also naturally favors social groups of privilege. Admissions procedures that heavily weight SAT scores motivate students who have the means to take advantage of test prep courses and even take the exam multiple times. Loan approval systems that rely on existing lines of credit as an indication of creditworthiness encourage those who can to apply for more credit in their name. Thus an algorithm that scores applicants to determine how a resource should be allocated sets a standard for what an ideal candidate’s features ought to be. A responsive subject would look to alter 11 how she appears to a classifier in order to increase her likelihood of gaining the system’s approval. But since reactivity typically requires informational and material resources that are not equally ac- cessible to all. Thus, even when an algorithm draws on features that seem to arise out of individual effort, these metrics can be skewed to favor those who are more readily able to alter their features. In the machine learning literature, agent reactivity to a classifier is termed “strategic manipula- tion.” Since previous work in strategic classification has typically depicted interactions between the agent who selects the classifier, the so-called learner, and those candidates who are being classified as antagonistic, such actions are usually viewed as distortions that aim to undermine the published classifier. 15,44 As shown in Hardt et al., 44 a learner who anticipates these responses can, under cer- tain formulations of agent costs, adapt to protect against the misclassification errors that would have resulted from manipulation, recovering an accuracy level that is arbitrarily close to the theo- retical maximum. These results are welcome news for a learner who correctly assesses agents’ best- responses. Indeed in most strategic manipulation models, agents are depicted as equally able to pursue manipulation, allowing the learner who knows their costs to accurately preempt strategic responses. While there are occasions in which agents do largely face homogeneous costs—an even playing field, as it were—in many other social uses of machine learning tools, agents encounter dif- fering costs of altering the attributes that are ultimately observed and assessed by the classifier. As such, in this chapter we ask, “What are the effects of strategic classification and manipulation in a world of social stratification?” As in previous work in strategic classification, we cast the problem as a Stackelberg game in which the learner moves first and publishes her classifier before candidates best-respond and manipulate their features. 15,44,4,25 But in contrast with the models in previous work by Brückner and Scheffer 15 and Hardt et al., 44 we formalize the setting of a society comprised of social groups that not only may differ in terms of distributions over unmanipulated features and true labeling functions but also face different costs to manipulation. This extra set of differences brings to light questions that favor 12 an analysis that focuses on the welfares of the candidates who must contend with these classifiers: Do classifiers formulated with strategic behavior in mind impose disparate burdens on different groups? If so, how can a learner mitigate these adverse effects? The altered gameplay and outcomes of strategic classification raise questions of fairness that are intertwined with those of optimality. Though our model is quite general, we obtain technical results that reveal important social ram- ifications of using classification in systems marked by inequality and a potential for manipulation. Our analysis shows that, under our model, even when the learner knows the costs faced by differ- ent groups, her equilibrium classifier will always act to reinforce existing inequalities by mistakenly excluding qualified candidates who are less able to manipulate their features while also mistakenly admitting those candidates for whom manipulation is less costly, perpetuating the relative advantage of group already advantaged in the social structure. We delve into the cost disparities that generate such inevitable classification errors. Next, we consider the impact of providing subsidies to lighten the burden of manipulation for individuals who are in the disadvantaged group that faces higher costs. We find that such an inter- vention can improve the learner’s classification performance as well as mitigate the extent to which her errors are inequality-reinforcing. However, we show that there exist cases in which providing subsidies enforces an equilibrium learner strategy that actually makes some individual candidates worse-off without making any better-off. Paradoxically, in these cases, paying a subsidy to the disad- vantaged group actually benefits only the learner while both candidate groups experience a welfare decline! Further analysis of these scenarios reveals that, in many cases, all parties would have pre- ferred a world in which manipulation of features was not possible for any candidates. Our chapter’s agent-centric analysis views data points as representing individuals and classifi- cations as impacting those individuals’ welfares. This orientation departs from the dominant per- spective in learning theory, which privileges a vendor’s predictive accuracy, and instead evaluates classification regimes in light of the social consequences of the outcomes they issue. By incorpo- 13 rating insights and techniques from game theory and economics, domains that consider deeply the effects of various policies on agents’ behaviors and outcomes, we hope to broaden the perspective that machine learning takes on socially-oriented tools. Presenting more democratically-inclined analysis has been central to the field of algorithmic fairness, and we hope our work sheds new light on this generic setting of classification with strategic agents. 2.1.1 Related Work While many earlier approaches to strategic classification in the machine learning literature have tended to view learner-agent interactions as adversarial, 55,9 our work does not assume inherently antagonistic relationships, and instead, shares the Stackelberg game-theoretic perspective akin to that presented in Brückner and Scheffer 15 and built upon by Hardt et al. 44 Departing from these models’ focus on static prediction and homogeneous manipulation costs, Dong et al. 27 propose an online setting of strategic classification in which agents appear sequentially and have individual costs for manipulation that are unknown to the learner. Unlike our work, they take a traditional learner-centric view, whereas our concerns are with the welfare of the candidates. Agent features and potential manipulations in the face of a learner classifier can also be inter- preted as serving informational purposes. In the economics literature on signaling theory, agents interact with a principal—the counterpart to our learner—via signals that convey important infor- mation relevant to a particular task at hand. Classic works, such as Spence’s paper on job-market signaling, focus their analysis on the varying quality of information that signals provide at equilib- rium. 85 The emphasis in our analysis on different group costs shares features with a recent update to the signaling literature by Frankel and Kartik, 39 who also distinguish between natural actions, corresponding to unmanipulated features in our model, and “gaming” ability, which operate sim- ilarly to our cost functions. The connection between gaming capacity and social advantage is also explicitly discussed in work by Esteban and Ray 33 who consider the effects of wealth and lobby- 14 ing on governmental resource allocation. While most works in the economics signaling literature center on the decay of the informativeness of signals as gaming and natural actions become indistin- guishable, some recent work in computer science has also considered the effect of costly signaling on mechanism design. 58,59 In contrast to both of these perspectives, our work highlights the effect of manipulation on a learner’s action and as a consequence, on the agents’ welfares. In independent, concurrent work by Milli et al. 73 also consider the social impacts of strategic classification. Whereas our model highlights the interplay between a learner’s Stackelberg equilib- rium classifier and agents’ best-response manipulations at the feature level, their work traces the relationship between the learner’s utility and the social burden, a measure of agents’ manipulation costs. They show that an institution must select a point on the outcome curve that trades off its pre- dictive accuracy with the social burden it imposes. In their model, an agent with an unmanipulated feature vector x has a likelihood ℓpxq of having a positive label and can manipulate to change her original feature vector to any vector y with ℓpyq ď ℓpxq at zero cost, or to y with ℓpyq ą ℓpxq for a positive cost. This assumption, which the authors call “outcome monotonicity,” allows them to reason about manipulations in (one-dimensional) likelihood space rather than feature space, since the optimal learner strategies amount to thresholds on likelihoods. In contrast, we allow features to be differently manipulable (perhaps a student can boost her SAT score via test prep courses, but can do nothing to change her grades from the previous year, and cannot freely obtain a higher SAT score in exchange for a worse record of extracurricular activities), which affects the forms of both the learner’s equilibrium classifier and agents’ best-response manipulations. Despite these differences in model and focus, their analysis yields results that are qualitatively similar to ours. Highlighting the differential impact of classifiers on social groups, they also find that overcoming stringent thresholds is more burdensome on the disadvantaged group. 15 2.2 Model Formalization As in Brückner and Scheffer 15 and Hardt et al., 44 we formalize the Strategic Classification Game as a Stackelberg competition in which the learner moves first by committing to and publishing a binary classifier f. Candidates, who are endowed with “innate” features, best respond by manipulating their feature inputs into the classifier. Formally, a candidate is defined by her d-dimensional feature vector x P X “ r0, 1sd and group membership A or B, with A signifying the advantaged group and B the disadvantaged. Group membership bears on manipulation costs such that a candidate from group m who wishes to move from a feature vector x to a feature vector y must pay a cost of cmpyq ´ cmpxq. We note that these cost function forms are similar to the class of separable cost functions considered in Hardt et al. 44 We assume that higher feature values indicate higher quality to the learner, and thus restrict our attention to manipulations such that y ě x, where the symbol ě signifies a component- wise comparison such that y ě x if and only if @i P rds, yi ě xi. Throughout this chapter, we study non-negative monotone cost functions such that the cost of manipulating from a feature vector x to a feature vector y increases as x and y get further apart. To motivate this distinction between features and costs, consider the use of SAT scores as a signal of academic preparedness in the U.S. college admissions process. The high-stakes nature of the SAT has encouraged the growth of a test prep industry dedicated to helping students perform better on the exam. Test preparation books and courses, while also exposing students to content knowledge and skills that are covered on the SAT, promise to “hack” the exam by training students to internal- ize test-taking strategies based on the format, structure, and style of its questions. One can view SAT scores as a feature used by a learner building a classifier to select candidates with sufficient academic success according to some chosen standard. The existence of test prep resources then presents an opportunity for some applicants to inflate their scores, which might “trick” the tool into classifying the candidates as more highly qualified than they are in fact. In this example, a candidate’s strategic 16 manipulation move refers to her investment in these resources, which despite improving her exam score, do not confer any genuine benefits to her level of academic preparation for college. Just as access to test prep resources tends to fall along income and race lines, we view candidates’ different abilities to manipulate as tied to their group membership. We model these group differ- ences with respect to availability of resources and opportunity by enforcing a cost condition that orders the two groups. We suppose that for all x P r0, 1sd and y ě x, cApyq ´ cApxq ď cBpyq ´ cBpxq. (2.1) Manipulating from a feature vector x to y is always at least as costly for a member of group B as it is for a member of group A. We believe our model’s inclusion of this cost condition reflects an authentic aspect of our social world wherein one group is systematically disadvantaged with respect to a task in comparison to another. In our setup, we also allow groups to have distinct probability distributions DA and DB over unmanipulated features and to be subject to different true labeling functions hA and hB defined as hApxq “ $ ’’& ’’% 1, @x such that řd i“1 wA,ixi ě τA, 0, @x such that řd i“1 wA,ixi ă τA, (2.2) hBpxq “ $ ’’& ’’% 1, @x such that řd i“1 wB,ixi ě τB, 0, @x such that řd i“1 wB,ixi ă τB. (2.3) We assume that hApxq “ 1 ùñ hBpxq “ 1 for all x P r0, 1s. Returning to the SAT example, research has shown that scores are skewed by race even before factoring in additional considerations such as access to manipulation. 18 In such cases, the true threshold for the disadvantaged group is 17 lower than that for the advantaged group. We leave this generality in our model to acknowledge and account for the influence that various social and historical factors have on candidates’ unmanipu- lated features and not, we emphasize, as an endorsement of a view that groups are fundamentally different in ability. A formal description of the Strategic Classification Game with Groups is given in the following definition. Definition 1 (Strategic Classification Game with Groups). In the Strategic Classification Game with Groups, candidates with features x P r0, 1sd and group memberships A or B are drawn from distributions DA and DB. The population proportion of each group is given by pA and pB where pA ` pB “ 1. A candidate from group m pays cost cmpyq ´ cmpxq to move from her original features x to y ě x. There exist true binary classifiers hA and hB, for candidates of each group. Probability distributions, cost functions, and true binary classifiers are all common knowledge. Gameplay proceeds in the following manner: 1. The learner issues a classifier f generating outcomes t0, 1u. 2. Each candidate observes f and manipulates her features x to y ě x. A group m candidate with features x who moves to y earns a payoff fpyq ´ pcmpyq ´ cmpxqq. The learner incurs a penalty of CFP ÿ mPtA,Bu pmPx„Dmrhmpxq “ 0, fpyq “ 1s ` CFN ÿ mPtA,Bu pmPx„Dmrhmpxq “ 1, fpyq “ 0s, where CFP and CFN denote the cost of a false positive and a false negative respectively. The learner looks to correctly classify candidates with respect to their original features x, whereas 18 each candidate hopes to manipulate her features to attain a positive classification, expending as little cost as possible in the process. Under this setup, candidates are only willing to manipulate their features if it flips their classification from 0 to 1 and if the cost of the manipulation is less than 1. We note that defining the utility of a positive classification to be 1 can be considered a scaling and thus is without loss of generality. This learner-candidate interaction is very similar to that studied in Hardt et al. 44 However, our inclusion of groups with distinct manipulation costs leads to an ambiguity regarding a candidate’s initial features that does not exist when all candidates have an equal opportunity to manipulate. In very few cases can a vendor distinguish among candidates based on their group membership for the explicit purpose of issuing distinct classification policies, especially if that group category is a protected class attribute. As such, in our setup, we require that a learner publish a classifier that is not adaptive to different agents based on their group memberships. It is important to note that the positive results in Hardt et al.’s 44 formulation of the Strategic Classification Game, wherein for separable cost functions, the learner can attain a classification er- ror at test-time that is arbitrarily close to the optimal payoff attainable, do not carry over into this setting of heterogeneous groups and costs. Even when hA “ hB, the existence of different costs of agent manipulation, even when separable as in our model, introduces a base uncertainty to the learning problem that generates errors that cannot be extricated so long as the learner must publish a classifier that does not distinguish candidates based on their group memberships. Second, an analy- sis of the learner’s strategy and performance, the perspective typically taken in most learning theory papers, contributes only a partial view of the total welfare effect of using classification in strategic settings. The main objective of this chapter is to offer a more thorough and holistic inspection of all agents’ outcomes, paying special heed to the different outcomes experienced by candidates of the two groups. Insofar as all social behaviors are impelled by goals, interests, and purposes, we should view data that is strategically generated to be the rule rather than the exception in social machine 19 learning settings. Remark on the assumption that hA and hB are known. Our assumption that the learner has knowledge of groups’ true labeling functions is not central to our analysis. We make such an assumption to highlight the pure effect of groups’ differential costs of manipulation on equilibrium gameplay and consequent welfares rather than the potential side effects due to a learner’s noisy estimation of the true classifiers. Our general findings do not substantially rely on this feature of the model, and the overall results carry through into a setting in which the learner optimizes from samples. Remark on unequal group costs The differences in costs cA and cB encoded by the cost condition is not restricted to referring only to differences in the monetary cost of manipulation. Instead, as is common in information economics and especially signaling theory, “cost” reflects the multiplicity of factors that bear on the effort exer- tion required by feature manipulation. 85,86,66,10 To demonstrate the generality of our formulation of distinct group costs, we show that the cost condition given in (2.1) is equivalent to a more explicit derivation of the choice that an agent faces when deciding whether to manipulate her feature. A rational agent with feature x will only pursue manipulation if her value for a positive classifica- tion minus her cost of manipulation exceeds her value for a negative classification: vpfpxq “ 0q ď vpfpyq “ 1q ´ upcpyq ´ cpxqq. (2.4) The monotone function u translates the costs borne by a candidate to manipulate from x to y into her “utility space,” i.e., it reflects the value that she places on that expenditure. We can rewrite the 20 previous inequality to be cpyq ´ cpxq ď u´1`vpfpyq “ 1q ´ vpfpxq “ 0q ˘. (2.5) Substituting in k “ u´1`vpfpyq “ 1q ´ vpfpxq “ 0q ˘, we have cpyq ´ cpxq ď k. Since the same cost expenditure is valued more highly by the disadvantaged group than by the advantaged group, the function u is more convex for group B than for group A. Thus all else equal, we have cApyq ´ cApxq ď cBpyq ´ cBpxq as desired. More generally, the functions v, c, and u may each be different for the groups. As such, the disadvantage encoded in the cost condition can arise due to differences in valuations of classifications (v), differences in costs (c), or differences in valuations of those costs (u). 2.3 Equilibrium Analysis We begin by studying agents’ best-response strategies in the basic Strategic Manipulation Game with Groups in which candidates belong to one of two groups A and B, and the cost condition holds so that group B members face greater costs to manipulation than group A members. To build intuition, we first consider best-response strategies in the one-dimensional case in which candidates have features x P r0, 1s and group cost functions are of any non-negative monotone form. We then move on to consider the d-dimensional case in which candidate features are given as vectors x P r0, 1sd and manipulation costs are assumed to be linear. 2.3.1 One-dimensional Features In the d “ 1 case, the cost condition given in (2.1) may be written as c1 Apxq ď c1 Bpxq for all x P r0, 1s. Since the true decision boundaries are linear, in the one-dimensional case, they may be written as threshold functions where thresholds τA and τB are constants in r0, 1s and for agents in group m, 21 hmpxq “ 1 if and only if x ě τm. A university admissions decision based on a single score is an example of such a classifier. Although the SAT does not act as the sole determinant of admissions in the U.S., in countries such as Australia, Brazil, and China, a single exam score is often the only factor of applicant quality that is considered for admissions. When the learner has access to τA and τB, and group costs cA and cB satisfy the cost condition, the following proposition characterizes the space of undominated strategies for the learner who seeks to minimize any error-penalizing cost function. Proposition 1 (One-D Undominated Learner Strategies). Given group cost functions cA and cB and true label thresholds τA and τB where τB ď τA, there exists a space of undominated learner threshold strategies rσB, σAs Ă r0, 1s where σA “ c´1 A pcApτAq ` 1q and σB “ c ´1 B pcBpτBq ` 1q. That is, for any error penalties CFP and CFN, the learner’s equilibrium classifier f is based on a threshold σ P rσB, σAs such that for all manipulated features y, fpyq “ $ ’’& ’’% 1, @y ě σ, 0, @y ă σ. (2.6) To understand this result, first notice that if the learner were to face only those candidates from group A, she would achieve perfect classification by labeling as 1 only those candidates with unma- nipulated feature x ě τA. This strategy is enacted by considering candidates’ best-response manipu- lations. A rational candidate would only be willing to manipulate her feature if the gain she receives in her classification exceeds her costs of manipulation. The learner would like to guard against ma- nipulations by candidates with x ă τA but still admit candidates with x ě τA, so she considers the maximum manipulated feature y that is attainable by a rational candidate with x “ τA who is will- ing to spend up to a cost of one in order to secure a better classification, as illustrated in Figure 2.1. The maximum such y value is σA, and thus, the learner sets a threshold at σA, admitting all those 22 with y ě σA and rejecting all those with y ă σA. The same reasoning applies to a learner facing only group B candidates, and the learner sets a threshold at σB, admitting all those candidates with y ě σB and rejecting all those with y ă σB. It can be shown that for all valid values of τA, τB, cA, and cB, necessarily σB ď σA. Then all clas- sifiers with threshold σ ă σB are dominated by σB, in the sense that for any arbitrary error penalties CFP and CFN, the learner would suffer higher costs by setting her threshold to be σ rather than σB. In the same way, all thresholds σ ą σA are dominated by σA, thus leaving rσB, σAs to be the space of undominated thresholds. For an account of the full proof of this result (and all omitted proofs), see the appendix. Even without committing to a particular learner cost function, the space of optimal strategies characterized in Proposition 1 leads to an important consequence. A rational learner in the Strategic Classification Game always selects a classifier that exhibits the following phenomenon: it mistakenly admits unqualified candidates from the group with lower costs and mistakenly excludes qualified candidates from the group with higher costs. This result is formalized in Proposition 2. To state the proposition, the following definition is instructive. Whereas the true thresholds τA and τB are a function of unmanipulated features, the learner only faces candidate features that may have been manipulated. In order to make these observed features commensurable with τA and τB, it is helpful for the learner to “translate” a candidate’s possibly manipulated feature y to its minimum corresponding original unmanipulated value. Definition 2 (Correspondence with unmanipulated features). For any observed candidate feature y P r0, 1s, the minimum corresponding unmanipulated feature is defined as ℓApyq “ maxt0, c´1 A pcApyq ´ 1qu, ℓBpyq “ maxt0, c ´1 B pcBpyq ´ 1qu (2.7) 23 Figure 2.1: Group cost functions for a one‐dimensional feature x. τA and τB signify true thresholds on unmanipulated features for group A and B, but a learner must issue a classifier on manipulated features. The threshold σA perfectly classifies group A candidates; σB perfectly classifies group B candidates. A learner selects an equilibrium threshold σ˚ P rσB, σAs, committing false positives on group A (red bracket) and false negatives on group B (blue bracket). for a candidate belonging to group A and group B respectively. The corresponding values ℓApyq and ℓBpyq are defined such that a candidate who presents feature y must have as her true unmanipulated feature x ě ℓApyq if she is a group A member and x ě ℓBpyq if she is a group B member. Proposition 2 (Learner’s Cost in 1 Dimension). A learner who employs a classifier f based on a threshold strategy σ P rσB, σAs only commits false positives errors on group A and false negatives errors on group B. The cost Cpσq of such a classifier is CFNpBPx„DB“x P rτB, ℓBpσqq ‰ ` CFPpAPx„DA“ x P rℓApσq, τAq‰ , where false negative errors entail penalty CFN, and false positive errors entail penalty CFP. 24 A learner who commits to classifying only one of the groups correctly bears costs given by the following corollaries. Corollary 1. A classifier based on σA perfectly classifies group A candidates and bears cost CpσAq “ CFNpBPx„DB“x P rτB, ℓBpσqq‰. Corollary 2. A classifier based on σB perfectly classifies group B candidates and bears cost CpσBq “ CFPpAPx„DA“x P rℓApσq, τAq ‰. Notice that the learner’s errors always cut in the same direction—by unduly benefiting group A candidates and unduly rejecting group B candidates, these errors act to reinforce the existing social inequality that had generated the unequal group cost conditions in the first place. Since these errors arise out of the asymmetric group costs of manipulation, the Strategic Classification Game can be viewed as an interactive model that itself perpetuates the relative advantage of group A over group B candidates. Within the undominated region rσB, σAs, the equilibrium learner threshold σ˚ is attained as the solution to the optimization problem σ˚ “ arg min σPrσB,σAs Cpσq. (2.8) In the game’s greatest generality where candidates are drawn from arbitrary probability distribu- tions, groups bear any costs that abide by the cost condition, and the learner has arbitrary error penalties, the equilibrium learner threshold σ˚ cannot be specified any further. However, under some special cases of candidate cost functions and probability distributions, the equilibrium thresh- old can be characterized more precisely. Specifically, when candidates from both groups are as- sumed to be drawn from a uniform distribution over unmanipulated features in r0, 1s, an error- minimizing learner seeks a threshold value σ˚ that minimizes the length of the interval of errors, 25 given by the following quantity: σ˚ “ arg min σPrσB,σAs ℓBpσq ´ ℓApσq. From here, one natural assumption of candidate cost functions would have that groups A and B bear costs that are proportional to each other. In this case, the curvature of the cost functions is determinative of a learner’s equilibrium threshold. Proposition 3. Suppose group cost functions are proportional such that cApxq “ qcBpxq for q P p0, 1q, that DA and DB are uniform on r0, 1s, and that CFN “ CFP and pA “ pB “ 1 2 . Let σ˚ be the learner’s equilibrium threshold. When cost functions are strictly concave, σ˚ “ σB. When cost functions are strictly convex, σ˚ “ σA. When cost functions are affine, the learner is indifferent between all σ˚ P rσB, σAs. 2.3.2 General d-Dimensional Feature Vectors In the general d-dimensional case of the Strategic Classification Game, candidates are endowed with features that are given by a vector x P r0, 1sd and can choose to manipulate and present any feature y ě x to the learner. In this section, we consider optimal learner and candidate strategies when group costs are linear such that they may be written as cApxq “ dÿ i“1 cA,ixi; cBpxq “ dÿ i“1 cB,ixi (2.9) for groups A and B respectively. Now, the cost condition cApyq ´ cApxq ď cBpyq ´ cBpxq for all y ě x—defined component-wise as before—implies that @i P rds, cA,i ď cB,i. In d dimensions, the true classifiers hA and hB have linear decision boundaries such that for a group A candidate with 26 feature x, hApxq “ $ ’’& ’’% 1 řd i“1 wA,ixi ě τA, 0 řd i“1 wA,ixi ă τA, (2.10) and for a group B candidate with feature x, hBpxq “ $ ’’& ’’% 1 řd i“1 wB,ixi ě τB, 0 řd i“1 wB,ixi ă τB. (2.11) We assume that all components xi contribute positively to an agent’s likelihood of being classified as 1 so that wA,i, wB,i ě 0 for all i. To ensure that the cost of manipulation is always non-negative, all cost coefficients are positive: cB,i, cA,i ě 0 for all i P rds. A candidate may now manipulate any combination of the d components of her initial feature x to reach the final feature y that she presents to the learner. Despite this increased flexibility on the part of the candidate, we are still able to characterize the performance of undominated learner classifiers, generalizing the result in Proposition 2. All potentially optimal classifiers exhibit the same inequality-reinforcing property inherent within the one-dimensional interval of undominated threshold strategies, trading off false positives on group A candidates with false negatives on group B candidates. Before we formally present this result, we first describe candidates’ best-response strate- gies. Here, a geometric view of the space of potential manipulations is informative. Suppose a candidate endowed with a feature vector x faces costs řd i“1 cixi and is willing to ex- pend a total cost of 1 for manipulation. Then she can move to any y ě x contained within the d-simplex with orthogonal corner at x and remaining vertices at x ` 1 ci ei where ei is the ith standard basis vector. This region is given by Δpxq “ ! x ` dÿ i“1 ti ci ei P r0, 1s dˇ ˇ ˇ dÿ i“1 ti ď 1 ; ti ě 0 @i ) . (2.12) 27 Figure 2.2: The forward simplex. A candidate in group A with unmanipulated feature vector x can manipulate to reach any feature vector y P ΔApxq at a cost of at most 1. Δpxq, depicted in Figure 2.2, gives the space of potential movement for a candidate with unmanip- ulated feature x who is willing to expend a total cost of 1. Notice that ti can be interpreted as the cost that a candidate expends on movement in the ith direction. Thus řd i“1 ti gives the total cost of manipulation. Moving beyond the range of possible moves, in order to describe how a rational candidate will best-respond to a learner, we must consider the published classifier. Suppose a learner publishes a classifier f based on a hyperplane řd i“1 giyi “ g0, so that fpyq “ 1 if and only if řd i“1 giyi ě g0. A best-response manipulation occurs along the direction that generates the greatest increase in the value řd i“1 gipyi ´ xiq for the least cost. As such, a candidate will move in any directions i P arg maxiPrds gi ci . This result is formalized in the following lemma. Lemma 1 (d-D Candidate Best Response). Suppose a learner publishes the classifier fpyq “ 1 if and only if řd i“1 giyi ě g0. Consider a candidate with unmanipulated feature vector x and linear costs řd i“1 cixi. If fpxq “ 1 or if for all i P rds, fpx ` 1 ci eiq “ 0, the candidate’s best response is to set y “ x. Otherwise, letting K “ arg maxiPrds gi ci , her manipulation takes the form y “ x ` dÿ i“1 ti ci ei for any t such that ti ě 0 for all i P rds, ti “ 0 for all i R K, and řd i“1 gipxi ` ti ci q “ g0. 28 Figure 2.3: A perfect classifier for group A. Every candidate with unmanipulated feature vector x on or above the true decision boundary for group A is able to manipulate to a point y P ΔApxq on or above the blue decision boundary depicted here. No candidate with an unmanipulated feature vector below the true decision boundary is able to do so. The kink in the blue decision boundary arises due to the restriction of features to r0, 1s d. A perfect classifier for group A does not need to have this kink; for example, a more lenient perfect classifier can be formed by “straightening” it out. While in the d-dimensional case, a candidate has many more choices of manipulation directions to pursue, a best response strategy will always lead her to increase her feature in those components that are most valued by the learner and least costly for manipulation. That is, she behaves according to a “bang for your buck” principle, in which the optimal manipulations are in the direction or directions where the ratio gi ci is highest. Despite the fact that the optimal manipulation may not be unique, as in the cases where there are multiple equivalently good directions for a candidate to move in, a learner who knows candidates’ costs can still anticipate best-response manipulations and avoid errors on that group. As such, we are once again able to construct a perfect classifier for candidates of group A and a perfect classifier for candidates of group B. Theorem 1 (d-D Space of Dominant Learner Strategies). In the general d-dimensional Strategic Classification Game with linear costs, there exists a classifier that perfectly classifies group A and a classifier that perfectly classifies group B. All undominated classifiers commit no false positive errors on 29 group A and no false negative errors on group B. A full exposition of the proof appears in the appendix, but here we present an abbreviated expla- nation of the result. For each group m, the learner computes an optimal boundary that perfectly classifies all of its members by considering the set of simplices tΔmpxqu anchored at the vectors ¯x that satisfy w⊺ m¯x “ τm and drawing the strictest hyperplane that intersects each simplex. That is for all hyperplanes gi : řd j“1 gi,jxj “ gi,0 that are constructed to intersect each simplex, then g1 : řd j“1 g1,jxj “ g1,0 is the strictest if for all x P r0, 1sd, dÿ j“1 g1,jxj “ g1,0 ùñ dÿ j“1 gi,jxj “ gi,0 ě gj,0 for all gi. Due to the cost ordering, for any x P r0, 1sd, ΔBpxq Ď ΔApxq, and thus wherever a comparison is possible, the group A boundary is at least as strict as the group B boundary. Figure 2.3 gives a visualization of a boundary formed by connecting the simplices Δp¯xq; the corresponding classifier perfectly classifies the group. As in the one-dimensional general costs case, learner strategies necessarily entail inequality- reinforcing classifiers: a rational learner equipped with any error-penalizing cost function will se- lect an equilibrium strategy that trades off undue optimism with respect to group A for undue pessimism with respect to group B. We note that except in the extreme case in which there exists a perfect classifier for all candidates in the population, this result implies that the classifier for group A issues false negatives on group B, and the classifier for group B issues false positives on group A. In order to formalize this result, we would like to generalize the idea behind the minimum correspon- dence unmanipulated features given by ℓAp¨q and ℓBp¨q in (2.7) for general d-dimensions and linear costs. A learner who observes a possibly manipulated feature vector y must consider the space of unma- 30 nipulated feature vectors that the candidate could have had. Thus we can make use of the simplex idea of potential manipulation; however in this case, the learner seeks to project a simplex “back- ward” to “undo” the potential candidate manipulation. Since groups are subject to different costs, simplices Δ´1 A pyq and Δ´1 B pyq—a depiction is given in Figure 2.4—which represent the region from where a candidate could have manipulated, will differ based on the candidate’s group membership, with Δ´1 A pyq “ ! y ´ dÿ i“1 ti cA,i ei P r0, 1s dˇ ˇ ˇ dÿ i“1 ti ď 1 ; ti ě 0 @i ) , (2.13) Δ´1 B pyq “ ! y ´ dÿ i“1 ti cB,i ei P r0, 1s dˇ ˇ ˇ dÿ i“1 ti ď 1 ; ti ě 0 @i ) . (2.14) We can now use these constructs in order to define d-dimensional generalizations of ℓApyq and ℓBpyq. Definition 3 (Correspondence with Unmanipulated Features in d-D). For any observed candidate feature y P r0, 1sd, the minimum corresponding unmanipulated feature vectors are given by ℓApyq “ ␣x P Δ´1 A pyq X r0, 1s dˇ ˇEˆx P Δ´1 A pyq such that ˆx ă x (, (2.15) ℓBpyq “ ␣x P Δ´1 B pyq X r0, 1s dˇ ˇEˆx P Δ´1 B pyq such that ˆx ă x( (2.16) for a candidate belonging to group A and group B respectively. The corresponding values ℓApyq and ℓBpyq are defined such that a candidate who presents feature y must have had a true unmanipulated feature vector x ě ¯x for some ¯x P ℓApyq if she is a group A member and x ě ¯x for some ¯x P ℓBpyq if she is a group B member. For any hyperplane decision boundary g containing vectors y, the minimum corresponding fea- ture vectors given by ℓApyq and ℓBpyq are helpful for determining the effective thresholds that g 31 Figure 2.4: The backward simplex. A candidate in group A with manipulated feature vector y could have started with any feature vector x P Δ´1 A pyq and paid a cost of at most 1. generates on unmanipulated features for groups A and B. Lemma 2. Suppose a learner classifier f is based on a hyperplane g : řd i“1 gixi “ g0. Construct the set Lmpgq “ # arg min xPℓmpyq dÿ i“1 gixiˇ ˇ ˇ@y s. t. dÿ i“1 giyi “ g0 + (2.17) Then a group m agent with feature x can move to some y with fpyq “ 1 and cmpyq ´ cmpxq ď 1 if and only if x ě ℓ for some ℓ P Lmpgq. By definition, for any two ℓ1, ℓ2 P Lmpgq, ÿ i“1 giℓ1,i “ ÿ i“1 giℓ2,i “ g0 ´ gkm cm,km , where km P arg maxi“rds gi cm,i . Thus a learner who cares only about the true label of presented fea- tures, will construct her decision boundary g such that all ℓ P Lmpgq have the same true label. A cost-minimizing learner who publishes a classifier f based on a hyperplane g on manipulated features will commit errors on those candidates with unmanipulated features x P r0, 1sd con- tained within the boundaries given by LApgq and LBpgq. This space can be understood as the d- dimensional generalization of the rℓApσq, ℓBpσqs error interval in one-dimension. 32 Proposition 4 (Learner’s Cost in d Dimensions). A learner who publishes an undominated classifier f based on a hyperplane g⊺x “ g0 can only commit false positives on group A candidates and false negatives on group B candidates. The cost of such a classifier is CFNPx„DB” x P `g⊺x ă g0 ´ gkB ckB č w ⊺ Bx ě τB ˘ı ` CFPPx„DA” x P `w ⊺ Ax ă τA č g⊺x ě g0 ´ gkA ckA ˘ı , where kB P arg maxiPrds gi cB,i and kA P arg maxiPrds gi cA,i . 2.4 Learner Subsidy Strategies Since in our setting, the learner’s classification errors are directly tied to unequal group costs, we ask whether she would be willing to subsidize group B candidates in order to shrink the manipula- tion gap between the two groups and as a result, reduce the number of errors she commits. In this section, we formalize subsidies as interventions that a learner can undertake to improve her classi- fication performance. Although in many high-stakes classification settings, the barriers that make manipulation differentially accessible are non-monetary—such as time, information, and social access—in this section, we consider subsidies that are monetary in nature to alleviate the financial burdens of manipulation. We introduce these subsidies for the purpose of analyzing their effects on not only the learner’s classification performance but also candidate groups’ outcomes. Since subsidies mitigate the in- herent disparities in groups’ costs and increase access to manipulation, one might expect that their implementation would surely improve group B’s overall welfare. In this section, we show that in some cases, optimal subsidy interventions can surprisingly have the effect of lowering the welfare of candidates from both groups without improving the welfare of even a single candidate. 33 2.4.1 Subsidy Formalization There are different ways in which a learner might choose to subsidize candidates costs. In the main text of this chapter, we focus on subsidies that reduce each group B candidate’s costs such that the agent need only pay a β fraction of her original manipulation cost. Definition 4 (Proportional subsidy). Under a proportional subsidy plan, the learner pays a propor- tion 1 ´ β of each group B candidate’s cost of manipulation for some β P r0, 1s. As such, a group B candidate who manipulates from an initial feature vector x to a final feature vector y bears a cost of β`cBpyq ´ cBpxq˘. In the appendix, we also introduce flat subsidies in which the learner absorbs up to a flat α amount from each group B candidate’s costs, leaving the candidate to pay maxt0, cBpyq ´ cBpxq ´ αu. Similar results to those shown in this section hold for flat subsidies. When considering proportional subsidies, the learner’s strategy now consists of both a choice of β and a choice of classifier f to issue. The learner’s goal is to minimize her penalty CFP ÿ mPtA,Bu pmPx„Dm“hmpxq “ 0, fpyq “ 1 ‰ ` CFN ÿ mPtA,Bu pmPx„Dm“hmpxq “ 1, fpyq “ 0 ‰ ` λcostpf, βq, where costpf, βq is the monetary cost of the subsidy, CFP and CFN denote the cost of a false positive and a false negative respectively as before, and λ ě 0 is some constant that determines the relative weight of misclassification errors and subsidy costs for the learner. For ease of exposition, the remainder of the section is presented in terms of one-dimensional features. In Section A.1.3 of the appendix, we show that in many cases, the d-dimensional linear costs setting can be reduced to this one-dimensional setting. As an analog of (2.7), we define ℓ β Bpyq “ pβcBq´1pβcBpyq´1q, giving the minimum corresponding unmanipulated feature x for any observed feature y. Under the proportional subsidy, for a given y, 34 the group B candidate must have x ě ℓβ Bpyq. From this, we define σβ B such that ℓβ Bpσβ Bq “ τB. In order to compute the cost of a subsidy plan, we must determine the number of group B can- didates who will take advantage of a given subsidy benefit. Since manipulation brings no benefit in itself, candidates will only choose to manipulate and use the subsidy if it will lead to a positive classification. For a published classifier f with threshold σ, we then have costpf, βq “ `1 ´ β˘ ż σ ℓβ Bpσq `cBpσq ´ cBpxq˘Px„DBpxqdx. Although the learner’s optimization problem can be solved analytically for various values of λ, we are primarily interested in taking a welfare-based perspective on the effects of various classification regimes on both the learner and candidate groups. In the following section, we analyze how the im- plementation of a subsidy plan can alter a learner’s classification strategy and consider the potential impacts of such policies on candidate groups. 2.4.2 Group Welfare Under Subsidy Plans While a learner would choose to adopt a subsidy strategy primarily in order to reduce her error rate, offering cost subsidies can also be seen as an intervention that might equalize opportunities in an environment that by default favors those who face lower costs. That is, if costs are keeping group B down, then one might believe that reducing costs will surely allow group B a fairer shot at ma- nipulation, and, as a result, a fairer shot at positive classification. Alas we find that mitigating cost disparities by way of subsidies does not necessarily lead to better outcomes for group B candidates. In fact, an optimal subsidy plan can actually reduce the welfares of both groups. Paradoxically, in some cases, the subsidy plan boosts only the learner’s utility, whereas every individual candidate from both groups would have preferred that she offer no subsidies at all. The following theorem captures the surprising result that subsidies can be harmful to all candi- 35 dates, even those from the group that would appear to benefit. Theorem 2 (Subsidies can harm both groups). There exist cost functions cA and cB satisfying the cost conditions, learner distributions DA and DB, true classifiers with threshold τA and τB, population pro- portions pA and pB, and learner penalty parameters CFN, CFP, and λ, such that no candidate in either group has higher payoff at the equilibrium of the Strategic Classification Game with proportional sub- sidies compared with the equilibrium of the Strategic Classification Game with no subsidies, and some candidates from both group A and group B are strictly worse off. We note that a slightly weaker version of the theorem holds for flat subsidies. In particular, there exist cases in which some individual candidates have higher payoff at the equilibrium of the Strategic Classification Game with flat subsidies compared with the equilibrium with no subsidies, but both group A and group B candidates have lower payoffs on average with the subsidies. To prove the theorem, it suffices to give a single case in which both candidate groups are harmed by the use of subsidies. However, to illustrate that this phenomenon does not arise only as a rare corner case, we provide one such example here plus two in the appendix, and discuss general con- ditions under which this occurs. In each example, we consider a particular instance of the Strategic Classification Game and compare the welfares of candidates at equilibrium when the learner is able to select a proportional subsidy with their welfares at equilibrium when no subsidy is allowed. Example 1. Suppose that a learner is error-minimizing such that CFN “ CFP “ 1 and λ “ 3 4 . Suppose that unmanipulated features for both groups are uniformly distributed with pA “ pB “ 1 2 . Let group cost functions be given by cApxq “ 8? x ` x and cBpxq “ 12? x; note that the cost condition c1 Apxq ă c1 Bpxq holds for x P r0, 1s. Let the true group thresholds be given by τA “ 0.4 and τB “ 0.3. When subsidies are not allowed, the learner chooses a classifier with threshold σ˚ “ σB « 0.398 at equilibrium. This threshold perfectly classifies all candidates from group B, while permitting false positives on candidates from group A with features x P r0.272, 0.4q. 36 If the learner decides to implement a proportional subsidies plan, at equilibrium the learner chooses a classifier with threshold σ˚ prop “ σA « 0.546 and a subsidy parameter β˚ “ 0.558. Her new threshold now correctly classifies all members of group A, while committing false negatives on group B members with features x P r0.3, 0.348q. Some candidates in group B are thus strictly worse-off, while none improve. Without the subsidy offering, group B members had been perfectly classified, but now there exist some candidates who are mistakenly excluded. Further, one can show that candidates who are positively classified must pay more to manipulate to the new threshold in spite of receiving the subsidy benefit. This increased cost is due to the fact that the higher classification threshold imposes greater burdens on manipulation than the β subsidy alleviates. Group A candidates are also strictly worse-off since the threshold increase eliminates false positive benefits that some members had previously been granted in the no-subsidy regime. Moreover, all can- didates who manipulate must expend more to do so, since these candidates do not receive a subsidy payment. Only the learner is strictly better off with the implementation of this subsidy plan. Additional examples in the appendix show cases in which both groups experience diminished welfare when they bear linear costs. Even when the learner has an error function that penalizes false negatives twice as harshly as false positives and thus is explicitly concerned with mistakenly exclud- ing group B candidates, an equilibrium subsidy strategy can still make both groups worse-off. We thus highlight two consequences of subsidy interventions: On the one hand, with reduced cost burdens, more candidates from the disadvantaged group should be able to manipulate to reach a positive classification. However, subsidy payments also allow a learner to select a classifier that is at least as strict as the one issued without offering subsidies. These are opposing forces, and these examples show that without needing to distort underlying group probability distributions or the learner’s penalty function in extreme ways, the effect of mitigating manipulation costs may be out- weighed by the overall impact of a stricter classifier. 37 This result can also be extended to show that a setup in which candidates are unable to manipu- late their features at all can be preferred by all three parties—groups A and B as well as the learner— to both the manipulation and subsidy regimes. We provide an informal statement of this propo- sition below and defer the interested reader to its formal statement and demonstration in the ap- pendix. Proposition 5. There exist general cost functions such that the outcomes issued by a learner’s equilib- rium classifier under a non-manipulation regime is preferred by all parties—the learner, group A, and group B—to outcomes that arise both under her equilibrium manipulation classifier and under her equilibrium subsidy strategy. 2.5 Discussion Social stratification is constituted by forms of privilege that exist along many different axes, weaving and overlapping to create an elaborate mesh of power relations. While our model of strategic ma- nipulation does not attempt to capture this irreducible complexity, we believe this work highlights a likely consequence of the expansion of algorithmic decision-making in a world that is marked by deep social inequalities. We demonstrate that the design of classification systems can grant undue re- wards to those who appear more meritorious under a particular conception of merit while justifying exclusions of those who have failed to meet those standards. These consequences serve to exacerbate existing inequalities. Our work also shows that attempts to resolve these negative social repercussions of classifica- tion, such as implementing policies that help disadvantaged populations manipulate their features more easily, may actually have the opposite effect. A learner who has offered to mitigate the costs facing these candidates may be encouraged to set a higher classification standard, underestimating the deeper disadvantages that a group encounters, and thus serving to further exclude these popu- 38 lations. However, it is important to note that these unintended consequences do not always arise. A conscientious learner who offers subsidies to equalize the playing field can guard against such paradoxes by making sure to classify agents in the same way even when offering to mitigate costs. Other research in signaling and strategic classification has considered models in which manipula- tion is desirable from the learner’s point of view. 39,62 Though this perspective diverges from the one we consider here, we acknowledge that there do exist cases in which manipulation serves to improve a candidate’s quality and thus leads a learner to encourage such behaviors. It is important to note, however, that although this account may accurately represent some social classification scenarios, differential group access to manipulation remains an issue, and in fact, cases in which manipulation genuinely improves candidate quality may present even more problematic scenarios for machine learning systems. As work in algorithmic fairness has shown, feedback effects of classification can lead to deepening inequalities that become “justified” on the basis of features both manipulated and “natural”. 32 The rapid adoption of algorithmic tools in social spheres calls for a range of perspectives and ap- proaches that can address a variety of domain-specific concerns. Expertise from other disciplines ought to be imported into machine learning, informing and infusing our research in motivation, application, and technical content. As such, our work seeks to investigate, from a theoretical learn- ing perspective, some of the potential adverse effects of what sociology has called “quantification,” a world increasingly governed by metrics. In doing so, we bring in techniques from game theory and information economics to model the interaction between a classifier and its subjects. This chapter adopts a framework that tries to capture the genuine unfair aspects of our social reality by model- ing group inequality in a population of agents. Although this perspective deviates from standard idealized settings of learner-agent interaction, we believe that so long as machine learning tools are designed for deployment in the imperfect social world, pursuing algorithmic fairness will require us to explicitly build models and theory to address critical issues such as social stratification and un- 39 equal access. 40 3 A Short-term Intervention for Long-term Fairness 3.1 Introduction As algorithms are increasingly deployed to make social decisions that have previously been under the sole purview of humans, a growing body of work has challenged the reigning primacy of op- 41 timality and efficiency when issues of bias and discrimination are potentially at stake. Research in the growing field of algorithmic fairness has sought to address these concerns about the machine decision-making process by examining and manipulating standard tasks such as ranking or classi- fication under generalized constraints of “fairness.” Such computational notions of fairness have been varied but two broad opposing perspectives have proposed solutions that either defend fair- ness at the individual level (similar individuals are treated similarly) 30 or at the group level (groups are awarded proportional representation). 53,35 While this chapter similarly adopts a constraint- based intervention to achieve fairness, we depart from standard accounts of fairness that consider static domain-general algorithms and instead develop a dynamic model for the specific domain of decision-making in the labor market. Our work considers the role that firms’ hiring practices play in perpetuating economic inequalities between social groups by way of the disparate outcomes that groups experience in their employment opportunities and wage prospects. We address the issue by building upon a dynamic model of worker and firm behavior that has been shown to generate the asymmetric group outcomes that are observed empirically between black and white workers in the United States 16,5,41 and appending a constraint on firms’ hiring practices that successfully induces a group-equitable equilibrium. As we focus on the particular domain of labor market dynamics, our chapter draws upon an ex- tensive literature in economics. The theory of statistical discrimination, originally set forth in two seminal papers by Phelps 78 and Arrow, 8 explains disparate group outcomes as the result of ratio- nal agent behaviors that lock a system into an unfavorable equilibrium. In the basic model, workers compete for a skilled job with wage w. Skill acquisition requires workers to expend an investment cost of c, which is distributed according to a function F. A worker’s investment decision is an as- sessment of her expected wage gain compared with her investment cost. Firms seek information about a worker’s hidden ability level but can only base hiring decisions on observable attributes: her noisy investment signal and group membership. The firm’s response to this missing informa- 42 tion problem is to update its beliefs about a worker’s qualifications by drawing on its prior for her group’s ability levels. Therefore if a firm holds different priors for different groups, it will also set different group-specific hiring thresholds. Further, since these distinct thresholds are observed and internalized by workers, they adjust their own investment strategies accordingly—individuals within the unfavored group will lower their investment levels, and individuals in the favored group will continue to invest at a high level. Notably, even when the distribution of investment costs F is the same for each group*, an asymmetric equilibrium can arise in which groups invest at different levels, further informing firms’ distinct priors and reinforcing disparate employment prospects. In other words, rational workers and firms best respond in ways that exactly confirm the others’ beliefs and strategies, and thus, the discriminatory outcome is “justified.” A proponent of “individual fairness” may diagnose the problem of statistical discrimination as a failure to treat candidates of similar investments similarly†. After all, the mistaken inference of unequal group ability levels indeed appears to be the origin of firms’ inequitable hiring decisions. Moreover, when investment level is positively correlated with likelihood of being qualified, hiring based solely on investments is both rational and individually-fair. However, this group-blind solu- tion fails to take into account a critical aspect of workers’ investments—namely that they are choices rather than givens. Failure to recognize the upstream causes of observed data features brings to light the prickly notion of “ground truth” that has, from the start, plagued work on machine learning bias. Within a system as complex as the labor market, an input-output account of fairness that as- sesses the mapping of workers’ investment levels to their hiring outcomes does not resolve the un- derlying source of inequalities that drives the differences in attributes between groups. Because both *This has been the standard assumption in the economics literature since Arrow. 8 †In the exposition of “individual fairness” proposed by Dwork et al., 30 the built-in flexibility of the generic similarity metric between persons can include group membership and even be used to justify “fair affirmative action.” However, within an economic signaling environment where firms’ hiring standards affect workers’ investments, a more flexible metric approach that compares quality within and across groups still fails to account for the strategy and incentive features of the labor market and thus the group coordination failure that characterizes many statistical discrimination equilibria. 43 statistical discrimination and machine learning rely on data that harbor historical inequalities, local fairness checks are often incapable of addressing the self-perpetuating nature of biases. Even without group biases, the paradox remains: the cyclic equilibrium ensures local procedural fairness—fairness with respect to investment choices—while maintaining global disparate outcomes. The difficulty in pinpointing a particular cause of observed system-wide asymmetric outcomes challenges our mission in designing constraints to ensure fairness within the domain. If the out- comes themselves are trapped in a feedback loop, a successful fairness constraint should first jolt the system out of its current steady-state, and second, launch it on a path towards a preferable equilib- rium. As such, a successful approach must consider fairness in situ. This chapter presents a domain- specific dynamic model with an intervention that effects system-wide impact, guaranteeing a group- equitable equilibrium that is stable and self-sustaining. In our model, workers invest in human capital, enter first a Temporary Labor Market (TLM) and then transition into a Permanent Labor Market (PLM)‡. We use this partition to impose a con- straint on TLM hiring practices that enforces group statistical parity representation. However, the restriction need not apply in the PLM where firms select natural best response hiring strategies. Our employment model is reputational—a worker carries an individual reputation, which is a summary of her past job performances and belongs to a group with a collective reputation, which is a measure of the proportion of its members producing “good” outcomes. Working within this model, we show that by imposing this constraint on firms’ hiring strategies in the TLM, the resulting steady-state in the PLM is symmetric such that an equal proportion of workers in the two groups produce good outcomes and are thus hired. The labor market at equilib- rium, both procedurally and in outcomes, satisfies leading notions of “fairness”–group, individual, meritocratic 35,30,57—discussed in the algorithmic fairness literature. Furthermore, we show that ‡Contracting in a segmented market is common in the labor economics literature. Of these, our work is most similar to Kim and Loury, 70 but notably they model the effects of statistical discrimination, while ours explicitly requires group-equitable outcomes. 44 under particular labor market conditions, it Pareto-dominates the asymmetric outcomes that arise under two unconstrained rational hiring strategies: group-blind hiring and statistical discriminatory hiring. Our fairness intervention exploits the complementary nature of individual and collective reputations such that the system produces its own feedback loop that incrementally addresses initial inequalities in group social standing. As such, the TLM intervention need not be permanent— statistical parity of hired workers becomes the natural result of firms’ optimal hiring strategies once group equality is restored and the fairness constraint becomes obsolete. This chapter’s constraint-based approach to achieving equitable group outcomes in a reputa- tional model of labor market interactions melds the perspectives and techniques of labor economics with the motivations of algorithmic fairness. However, our system-wide view also challenges a thread of work in the literature that characterizes notions of fairness as input-output-based prop- erties of a decision-making function. By casting workers and firms as strategic agents in a dynamic game, we incorporate complexities of the labor market dynamic such as agents’ expectations, incen- tives, and externalities that are otherwise difficult to encapsulate in a static classification setting. We advocate for an intervention that addresses the root of disparities between black and white workers’ positions in the labor market and society—not only positions of unequal prospects and outcomes but as important, positions of unequal opportunities and, as a result, qualifications. Ensuring pro- cedural fairness in the hiring decision alone is insufficient for this greater task. Our proposed con- straint is designed to perturb a labor market at asymmetric equilibrium by co-opting the system’s own cyclic effects to install group-equality that is self-sustaining in the long-term. In Section 2, we present a standard model of labor market dynamics and introduce our fairness intervention. Section 3 contains an overview of the equilibria results of the constrained-hiring model along with a comparison against equilibria arising from two rational hiring strategies free from such a constraint. The chapter ends with a reflection on the equilibrium tendencies of discrim- ination and their implications on the design of fairness constraints. We also offer some comments 45 on the dynamic feedback effects that are inherent features of persistent inequalities and the chal- lenges they issue upon future work in algorithmic fairness. 3.1.1 Related Work Within the algorithmic fairness literature, Zemel et al. 97 address group and individual notions of fairness by constructing a mapping of agent data to an intermediate layer of clusters that each pre- serve statistical parity while obfuscating protected attributes. A second map taking cluster assign- ments to their final classifications then allows “similar” agents to be treated similarly. This dual-map approach roughly corresponds to the roles of the TLM and PLM in our model. Related work has sought distance metrics to guide the initial mapping, 30 but since criteria for similarity vary by do- main, general approaches often face obstacles of application. Our chapter’s concentrated treatment of labor market dynamics aims to addresses this concern. We answer a call by Friedler et al. 40 to specify a particular world view of fairness within a domain and classification task. Our model starts with an assumption of inherent equality between groups. As such, differences in observable invest- ment decisions or job outcomes are due to unequal societal standing, producing secondary effects of inequality, rather than fundamental differences in the nature of the individuals. Labor market discrimination has been of long-standing interest in economics due to the per- sistent inequalities in employment prospects among groups of different race, gender, and other socially-salient attributes. 16,5,41 Since most explicit forms of wage discrimination are now illegal in the U.S. and genetic accounts of group differences have been largely discredited, 76 modern theories of labor market discrimination have updated the classical works—Becker’s “taste-based” discrimina- tion 12 and Phelps’ model of exogenous group productivity differences 78—by examining the social sources of asymmetric outcomes. Research in the field has produced models that consider temporal dynamics, utilize distinct group cost functions, and develop wages endogenously. 19,7 We follow in this line of work by incorporating a dynamic group reputation parameter into an individual’s cost 46 function, a modeling choice informed by the vast empirical literature showing the differential ex- ternalities produced by groups of differential social standing. Our model is not the first that makes explicit this linkage. In research examining the impact of neighborhood segregation on agents’ ac- cesses to resources for skill acquisition, Bowles, Loury, and Sethi 14 include a group “skill share” metric that functions similarly to our notion of group reputation in its effect on individuals’ costs. This chapter also frames the hiring process as reputational in nature, following a distinct liter- ature on collective reputation. 88,92 Of these, our work shares most in common with the model proposed by Levin, 67 in which workers carry an individual reputation that contributes to their group’s reputation. Levin shows that even when cost conditions evolve stochastically, reputations can produce a persistent feedback effect that leads to convergence to an asymmetric equilibrium in which groups occupy distinct social standings. Unlike in Levin, the notion of collective reputa- tion in our model bears not only on workers’ forward-looking expectations and incentives but also explicitly impacts future generations’ investment costs. Additionally, since our work has in mind the information-processing capabilities of artificial intelligence agents, we formalize the concept of “individual reputation” as composed of a total history of previous outcomes. These additional “data,” while potentially overwhelming for human decision-makers, can be handled by an algorith- mic decision-maker. Since the functionality of machine learning in the hiring process is ultimately based in a form of “rational” statistical discrimination of worker data and job histories, this strand of economics literature is particularly relevant for considerations of algorithmic fairness in the labor market. 3.2 Model We highlight the role of the fairness constraint within the rest of the standard labor market dynam- ics of the model by utilizing a dual labor market setup composed of a Temporary Labor Market 47 (TLM) and a Permanent Labor Market (PLM). In the former, a hiring constraint is established to ensure statistical parity, and in the latter, firms hire according to their best response hiring practices in a reputational model applied to the particular setting of employment. This partition does little to impinge upon the standard dynamics of the labor market—workers flow from the TLM to the PLM, wages are labor-market-wide, and individual worker reputations in the PLM produce externalities for the collective group reputations that play a key role in individuals’ pre-TLM investment decisions. 3.2.1 General Setup Consider a society of n workers who pass through the labor market sequentially at times t “ 0, 1, .... The labor markets maintain a constant relative size: m proportion of the workers reside in the TLM, and 1 ´ m reside in the PLM. Movement is governed by Poisson processes—workers immediately re- place departing ones in the TLM, transition from the TLM to the PLM according to the parameter κ, and leave the PLM at rate λ. Each worker belongs to one of two groups μ P tB, Wu with population share σB and 1 ´ σB re- spectively. We assume that these subpopulation proportions of workers are stable such that a worker of group μ who leaves the labor market is replaced via the birth of a new worker of the same group. The distribution of individual abilities, described by the CDF Fpθq, is stable over time and identical across groups. In contrast, societal reputation varies with time and by group. A group’s time t rep- utation πμ t gives the proportion of all individuals in group μ who are producing “good” outcomes in the labor market, over the interval timespan rt ´ τ, ts, where the parameter τ ě 0 controls the time-lag effect of a group’s previous generations’ performance on its present reputation. Prior to entering the labor market, workers select education investment levels η, weighing the cost of investment with its expected reward. Firms hire and pay workers based on expected performance, awarding wage wpgtq for a “good” worker, where gt gives the proportion of “good” workers in the 48 PLM at time t. To prevent constant fluctuation at each time step, the wage wt “ wpgt1q updates in a Poisson manner such that t1 ă t gives the time of the last wage change. The hiring process is for- malized by assigning workers to either skilled or unskilled tasks with distinct wages. For simplicity, workers who do not pass particular hiring thresholds may still be considered “hired,” but they are assigned to an unskilled task and paid a wage normalized to 0. As a function, the wage premium wt is decreasing in gt, since as the relative supply of “good” workers increases, imperfect worker substitutability lowers their marginal productivity, thus de- creasing wage. We impose a minimum wage w such that limgtÑ8 wpgtq “ w and a maximum wage w such that limgtÑ0 wpgtq “ w. In the context of the model, minimum and maximum wages should not be considered as only products of labor laws, rather they also act to track the supply of “good” workers relative to firms’ demand. 3.2.2 Temporary Labor Market A worker i of group μ chooses to invest in human capital ηi ě 0 according to her expected wage gain of being in the skilled labor market wt§ and her personal cost function for investment, cπμ t pθi, ηiq, which is a function decreasing in her individual ability θi and increasing in her selected level of investment ηi. The incorporation of group reputation πμ into an individual’s cost function re- flects the differential externalities produced by groups of differential social standing. 14 We posit that a worker belonging to a group with a superior societal reputation has improved cost con- ditions relative to her counterparts with equal ability in the lower reputation group. Formally, @πμ t ă πν t , cπμ t pθi, ηiq is a positive monotonic transformation of cπν t pθi, ηiq. Investment in human capital operates as an imperfect signal, and workers have a hidden true type: qualified or unqualified, ρ P tQ, Uu. Let γ : Rě0 Ñ r0, 1s be a monotonically increas- ing function that maps a worker’s investment level to her probability of being qualified. Unlike in §Workers are boundedly rational and unable to anticipate future wage dynamics. 49 worker enters TLM; TLM ﬁrm makes hiring decision worker exerts effort on the job in TLM, produces outcome worker exerts effort on the job in PLM, produces outcome PLM ﬁrm makes hiring decision 5 6 worker is born into 1 of 2 groups worker makes investment decision 0 1 2 3 4 worker enters PLM; PLM ﬁrm makes hiring decision Figure 3.1: Timeline of worker and firm interactions throughout the labor market pipeline. Spence’s original work on education signaling 84 in which investment confers no productivity ben- efits and thus operates purely as a signal to employers, in our model, a worker’s chosen investment level η has intrinsic value insofar as it is positively correlated with her likelihood of being qualified γpηq. Given this setup, a firm’s TLM hiring strategy is a mapping HT : Rě0 Ś μ Ñ t0, 1u such that the hiring decision for worker i is based only her observable investment level ηi P Rě0 and group membership μ. A worker who is hired into the TLM enters the pipeline and is eligible to compete for a PLM skilled job; a worker who does not pass the TLM hiring stage remains in the market but is permanently excluded from candidacy for the skilled wage. In this chapter, we mainly consider only those workers who successfully enter the skilled hiring pipeline, considering all others as “not hired.” As such, we use the terms “skilled” and “hired” interchangeably. 3.2.3 Permanent Labor Market Labor market dynamics follow in the style of repeated principal-agent interactions with hidden ac- tions (effort exertion) but observable histories (reputation of outcomes). Once hired into the TLM, a worker i exerts on-the-job effort—choosing either high (H) or low (L) effort—which stochasti- cally produces an observable good (G) or bad (B) outcome that affects her individual reputation and thus future reward. Exerting L is free, but exerting H bears cost eρpθiq, which is a function of qualification ρ P tQ, Uu and ability level θi. Effort is more costly for unqualified individuals: @θi, eUpθiq ą eQpθiq. We emphasize here that the notions of ability level θ and qualification status 50 ρ are distinct worker qualities. A high ability worker is one who has the general attributes that bear on success in the realms of education and work, whereas a qualified worker is one who has the ap- propriate training and skills for a given job. We may say, very crudely, that a worker is “born” with an ability level and “earns” a qualification status. In our model, a worker’s ability level precedes her investment decision, which begets a qualification status. High effort increases the probability of a good outcome G. If pρ,k gives the probability of achiev- ing outcome G with qualifications ρ and effort level k, then the following inequalities hold. pQ,H ą pQ,L; pU,H ą pU,L; pQ,L ą pU,L Since the effect of qualifications on exerting high effort is already incorporated in its cost, pQ,H “ pU,H, we write both quantities as pH. We then simplify pQ,L and pU,L to pQ and pU respectively. We emphasize the distinction between the effort exertion cost functions ep¨q here and the previ- ous investment cost functions cp¨q—the former are pertinent to workers already in the labor market and differ by qualification status, whereas the latter relate to pre-labor-market decisions and differ by group membership. Separate cost functions allow for a finer analysis of the salient factors that influence agent behavior at distinct points of the labor market pipeline. The inclusion of group membership into human-capital investment costs reflects the genuine differences in resources avail- able to workers of different groups in their paths to education attainment¶. A worker keeps the same TLM job until the Poisson process with parameter κ selects her to move into the PLM, where at each time step, she cycles through jobs, exerting a chosen effort level, pro- ducing an observable outcome, and accumulating a history of past performances that includes her TLM outcome. At each time step, firms in the PLM want to hire all and only those workers who ¶We do not claim that group membership ceases to be a relevant factor impacting agent behavior once workers are in the labor market, but we note that a worker’s qualifications, or the extent to which her skill investment proved to be successful, becomes an overriding determinant. Insofar as education investment bears on qualification status, a worker’s group membership continues to impact her labor market outcomes. 51 consistently exert effort. To do so, firms distill a worker’s history of observable outcomes into her “individual reputation” Πt i, which gives the proportion of outcomes G in her recent length-t his- tory. In a labor market system of repeated worker-firm contracting, firms have the power to use these observable individual reputations to set self-enforcing relational contracts. A firm’s PLM hir- ing strategy is a mapping HP : r0, 1s Ñ t0, 1u such that the decision is solely a function of Πt i. Figure 3.1 depicts a timeline of how workers move through the labor market pipeline and interact with firms. While “fairness” is a notoriously thorny ethical concept to define, the goal here of achieving long- term fairness is equivalent to attaining group equality in labor market outcomes. Since groups do not differ in fundamental or intrinsic ways, their job and wage prospects should also not systemati- cally diverge at a fair steady-state. Table 3.1: Table of notation Notation Significance Fpθq CDF of ability levels θ πμ group μ reputation σμ group μ population share wt wage at time t gμ t proportion of group μ workers producing good outcomes at time t η investment level pH, pQ, pU probability of producing G given effort level cπμ t pθ, ηq cost of investment γpηq probability of being qualified ρ P tQ, Uu hidden qualification status eρpθq cost of effort exertion Πt i individual reputation at time t 52 3.3 Results Reputation-based labor market models, such as the one described in this chapter, can generate asym- metric group outcomes when firms utilize rational strategies such as statistical discrimination or group-blind hiring. 8,22,7,19 Since this chapter examines the effect of our proposed intervention on system-wide dynamics and outcomes, in the following section, we consider only those strategies and equilibria outcomes that arise in this fairness-constrained setting. 3.3.1 Equilibrium Strategies and Steady-States We start by describing TLM strategies resulting from the fairness constraint, then move onto the PLM and analyze firms’ and workers’ best response strategies together. Gameplay in the PLM mir- rors repeated principal-agent interactions wherein firms have the power to enforce contracts by monitoring individual reputations, and thus we consider strategies that constitute a sequential equi- librium. Since a firm in the TLM prefers candidates who are more likely to be qualified, optimal hiring follows a threshold strategy: Given a hiring threshold ˆη, @i such that ηi ě ˆη, HT piq “ 1, and inversely, @i such that ηi ă ˆη, HT piq “ 0. However, since firms must abide by the statistical parity hiring rule, their optimal threshold strategy is uniquely determined: if a firm aims to hire a fraction ℓ of all workers, its investment thresholds will be implicitly defined and group-specific, so that in the TLM, skilled employees from groups μ and ν will constitute σμℓ and p1 ´ σμqℓ proportions of the full worker population respectively. A worker of group μ, observing her group-specific TLM investment threshold pημ, will weigh her cost of investment with her expected wage gain wt. All workers i with cπμ t pθi, pημq ď wt will choose to invest exactly at the level ηi “ pημ and be hired for the skilled position in the TLM; all other workers will invest at level ηi “ 0 and fail to enter the pipeline to compete for the skilled job. Workers who 53 pass the first hiring stage know that their future PLM opportunities will depend on their observable outcome in the TLM, and as such they exert effort in a one-shot game. A worker i with qualification status ρ exerts high effort on the job if and only if eρpθiq ď wtppH ´ pρq. As previously shown, while the statistical parity constraint preserves the fundamental equality of ability distributions Fpθq between groups, the group-specific investment thresholds pημ generate group-specific investment strategies. As consequence, since investment has positive returns on qual- ification status, groups may have differing proportions of qualified candidates in the PLM pool. We denote by γμ t the proportion of candidates in group μ who are qualified at time t, leaving 1 ´ γμ t who are unqualified. Then the proportion of group μ workers in the TLM who produce good outcomes follows the recursive model g μ t “pHr1 ´ Fp pθQqγμ t ´ Fp pθUqp1 ´ γμ t qs ` pQFp pθQqγμ t ` pUFp pθUqp1 ´ γμ t q (3.1) where pθρ “ e ´1 ρ pwtppH ´ pρqq and gt1 “ σμℓg μ t1 ` p1 ´ σμqℓgν t1 with wt “ wpgt1q where t1 gives the time of the last wage update. It is important to note that gμ t gives the proportion of workers in the skilled labor market who at time t are producing good outcomes in their jobs. This quantity does not exactly coincide with group reputation, πμ t , which gives a (time-interval average) normalized metric that scales with the proportion of all members in group μ–including those who are not granted entry into the skilled job pipeline–who are producing good outcomes. A PLM worker’s future-anticipatory strategy is a selection of time, reputation, wage, and hiring threshold-dependent probabilities of effort exertion εpΠt1 i q with Πt1 i P tΠt1u where the index i of Πt1 i denotes a particular individual reputation level in the set of all possible reputation levels tΠu and t1 tracks the length of time that has passed since the last wage update. Supposing that workers engage in N-depth reasoning where N \" t1, this quantity may be computed via backward induction 54 on the continuation value for a given individual reputation, VpΠt1 i q. With this setup, the continua- tion value VpΠNq “ 0, and the agent with ability θ and qualification ρ solves the following dynamic programming problem VpΠt1 i , ˆΠt1, wtq “ sup εpΠt1 i qPr0,1s ! p1 ´ λqrVpΠt1`1 i , GqrεpΠt1 i qppH ´ pρq ` pρs ` VpΠt1`1 i , Bqrp´εpΠt1 i qqppH ´ pρq ` 1 ´ pρss ` 1Πt1 i ě ˆΠt1 wt) where VpΠt1 i , Gq “ Vp Πt1 i t1 ` 1 t1 ` 1 , ˆΠt1, wtq and VpΠt1 i , Bq “ Vp Πt1 i t1 t1 ` 1 , ˆΠt1, wtq and @t, wt “ wT when the agent looks forward from time T where the worker solves for optimal effort exertion probabilities εpΠt1 i q for each possible reputation Πt1 i P tΠt1u, and high effort is only optimal at time t if VpΠt1 i , GqppH ´ pρq ě eρpθq. If firms seek those workers who appear willing and able to exert high effort upon being hired, their equilibrium strategy is to select a reputation threshold ˆΠt1 “ pH ´ Δt1 when facing a worker with history length t1 since the last wage update. Δt1 ą 0 acts as the firm’s optimistic forgiveness buffer, permitting a worker’s recent time t1 reputation to be slightly under the pH threshold, to en- sure that it does not penalize workers who exert high effort but are unlucky and receive B outcomes. An optimal choice of Δt1 monotonically decreases in t1 toward 0 as the reputation of a worker con- sistently exerting high effort converges to pH as t1 Ñ 8. Note that the firm must also take care not to decrease Δt1 too slowly, lest workers are able to exert low effort and continue to be hired. Thus the firm optimizes its hiring threshold ˆΠt1 “ pH ´ Δt1 by decreasing Δ just enough at each time step to motivate consistent high effort from workers who can afford it. All other workers exert low effort in each round. Thus given a firm’s reputation threshold ˆΠt1, its equilibrium PLM hir- ing strategy HP is a mapping such that if and only if the worker’s accumulated reputation since the last wage update Πt1 i exceeds the threshold ˆΠt1, HP pΠt1 i q “ 1, and the worker is hired. Otherwise 55 HP pΠt1 i q “ 0, and the worker does not earn the wage premium. This strategy is summarized in the following Proposition, and we defer the interested reader to the Appendix for its proof. Proposition 6. There exists a pair of PLM equilibrium strategies pH, Eq of firm-hiring and worker- effort respectively such that (i) A firm’s hiring strategy H is a selection of a reputation threshold function of the form ˆΠt1 “ pH ´ Δt1, where Δt1 is a monotonically decreasing function in t1, such that HpΠt1q “ 1 if and only if Πt1 i ě ˆΠt1, otherwise Hpiq “ 0. (ii) A worker’s effort strategy E is a selection of effort levels that considers only the wage wt and cost of effort such that Epwtq “ H if and only if eρpθq ď wtppH ´ pρq, else Epwtq “ L. Interestingly, the strategies employed in the repeated worker-firm interactions in the PLM gen- erate a recursive relationship of the proportion of “good” workers for each group that mirrors the structure of (3.1). PLM firms’ stringent threshold reputation hiring strategy imposes the same type of “pressure” on workers at each round of employment as does the single-shot game in the TLM. In both labor markets, every outcome “counts.” Having elaborated upon the dynamics of both the TLM and PLM, we incorporate worker move- ment and combine the results to obtain a recursive relationship that governs the sequence of work- ers’ performance results from an initial wage w0. Note that the multiplicity of possible firm hiring strategies produces a multiplicity of dynamic paths of outcomes tpg μ t , gν t qu8 0 to steady-state, but given that in our model, firms are willing to hire only and all workers who consistently exert high effort, firm and worker equilibrium strategies are as described in Proposition 1, there is a unique se- quence of group outcome pairs pg μ t , gν t q such that there exists a time t “ T with the property that @t ě T, pg μ T, gν Tq “ pg μ t , gν t q. Theorem 3. Under the described labor market conditions in which ℓ proportion of workers gain entry into the TLM and firms abide by the statistical parity hiring constraint, the proportion of all workers 56 in group μ producing good outcomes at time t, gμ t in the full labor market follows the recursive system g μ t`1 “ pHr1 ´ FpθQqγμ t ´FpθUqp1 ´ γμ t qs ` pQFpθQqγμ t ` pUFpθUqp1 ´ γμ t q (3.2) where πμ t “ σμℓ τ tÿ j“t´τ g μ j , (3.3) γμ t “ φp pημpπμ t qq, (3.4) θρ “ e ´1 ρ pwtppH ´ pρqq, (3.5) gt “ σμℓgμ t ` p1 ´ σμqℓgν t , (3.6) where φ and pημ in Eq. 3.4 are monotonically increasing functions whose composition combines the labor market’s reputational feedback effect with firms’ TLM constrained group-investment thresholds. Then there exists a unique stable symmetric steady-state equilibrium and convergence time T, wherein ˜πμ t “ ˜πν t “ ˜π, @t ą T, satisfying system-wide fairness, with a corresponding unique stable wage ˜w. To understand why the existence of this unique stable symmetric equilibrium is guaranteed when TLM firms are bound to the statistical parity requirement, consider the two variables that affect a group μ worker i’s likelihood of producing a good outcome: her ability level θi and her probability of being qualified PpQ|ˆημq “ γμ. Since there are positive returns to investment, γμ is increasing in πμ: As her group μ social standing rises, cost conditions improve, and as a result, workers in future generations are more likely to be qualified. With the imposition of the TLM hiring constraint, firms recognize the groups’ different costs of investment and hire in a manner that retains equality be- tween the two groups’ underlying ability distributions Fpθq within the labor market, which assures that the proportions of workers producing good outcomes in each group gμ do not diverge within the skilled labor market pipeline. Moreover, the statistical parity hiring constraint requires that firms hire in a manner such that workers from a disadvantaged group μ are not inequitably blocked from entering the skilled labor market and always constitute σμℓ of the TLM. As a result of maintaining 57 both identical ability distributions Fpθq and proportional representation σμ in the TLM, statisti- cal parity hiring ensures that as group outcomes in the skilled labor market converge, so do group reputations. Thus, the γt-generated positive feedback loop that pushes towards diverging group out- comes is always constrained, allowing the natural reputational feedback on group investment cost functions cπμ to drive the convergence of group outcomes and thus group reputations to a single steady-state value. Importantly, throughout the path of tpgμ t , gν t qu outcomes toward this symmet- ric steady-state, the “severity” of the TLM fairness constraint on firms’ hiring strategies continually slackens until it recedes into disuse. For a full exposition of the proof, see the Appendix. Under statistical parity hiring in the TLM, groups with unequal initial social standing will grad- ually approach the same reputation level according to time-lag τ. The constraint has the effect of co-opting the “self-confirming” loop for group reputation improvement—collective reputation pro- duces a positive externality, lowering individual group members’ cost functions, thus improving in- vestment conditions for future workers, further raising individual and group reputation. We point out that the empirically-validated link between group reputations and members’ investment costs makes a TLM statistical parity constraint a more efficient means of addressing group inequalities than a similar intervention in the PLM. Since the TLM represents the entry point into the market, enforcing statistical parity at the onset ensures that lower reputation workers are not disproportion- ately excluded from the pipeline as a whole. We next compare this steady-state under the TLM constraint with long-term outcomes of other rational hiring strategies that are not bound by any fairness constraints and show that under particu- lar market conditions, the fair steady-state is Pareto-dominant. 3.3.2 Comparative Statics with Unconstrained Hiring Strategies In the absence of any constraint, firms are free to select any strategy that will maximize their prob- ability of employing high-ability, qualified workers. Two such common strategies are group-blind, 58 sometimes called “meritocratic,” and statistical discriminatory hiring. We provide an overview of each practice and then continue on to comparing their long-term equilibria outcomes with the sym- metric steady-state that arises under our TLM hiring constraint. Consider a group-blind TLM hiring strategy that is individual-based, operating under an equal- treatment philosophy. Without considering agent group membership— suppose again μ P tB, Wu— the firm hires a proportion ℓ of workers by selecting a single investment level threshold ˜η for all workers, implicitly defined as ℓ “ p1 ´ σBq ´1 ´ Fpc´1 πWp˜ηppH ´ pρqq ¯ ` σB´ 1 ´ Fpc´1 πB p˜ηppH ´ pρqq¯ where σB and 1 ´ σB give the proportion of individuals in groups B and W respectively, and the function cπμp¨q determines the group μ investment level. Pragmatically under this strategy, the firm will examine the broad distribution of all investment levels and select a threshold above which it is willing to employ workers. This strategy is also rationalized by the fact that the threshold ˜η maxi- mizes the expected number of hired workers who are qualified. An alternative class of firm hiring strategies employ statistical discrimination, in which priors regarding a worker’s observable attributes, such as group membership, are used to infer a particular individual’s hidden attributes. In particular, if TLM firms hold priors ξB and ξW about the two groups’ capabilities, upon observing an applicant’s group μ and investment level η, they will update their beliefs of the prospective employee’s qualifications according to: PpQ|μ, ηq “ pQpηqξμ pQpηqξμ ` p1 ´ ξμqpUpηq where pQpηq and pUpηq give the probability of a qualified and unqualified worker having investment level η respectively. 59 Theorem 4. In a PLM with unsaturated demand (w “ ¯w) for skilled workers, the TLM constraint leads to a symmetric steady-state equilibrium that Pareto-dominates the asymmetric equilibria that arise under group-blind and statistical discriminatory hiring. We present an abbreviated exposition of the underlying factors that drive unconstrained hiring strategies to Pareto-dominated outcomes. For the full account of the proof, see the Appendix. Group-blind hiring satisfies neither of the two key constrained hiring guarantees described in the proof explanation for Theorem 3—namely, groups no longer share equal ability distributions Fpθq nor are they proportionally represented in the market according to their demographic shares σμ. The violation of both of these criteria contribute to group reputation divergence and thus the existence of persistent asymmetric outcomes between groups. At the asymmetric steady-state, groups retain distinct investment costs that, under a group-blind investment threshold, generate group-specific ability level thresholds rθB and ĂθW. If group reputation πB ă πW, then these ability thresholds may be ranked with respect to the threshold ¯θ that arises un- der the fairness constraint: ĂθW ă ¯θ ă rθB. These hiring strategies inequitably bound the proportion of able and qualified workers in group B who are eligible to compete for skilled jobs, leaving behind an untapped source of group B individuals who would have otherwise been hired. Under PLM con- ditions in which demand for skilled workers is unsaturated and the wage wpgtq “ ¯w, workers in group W who are barred from entering the labor market in the proposed fair regime are not hired at equilibrium under group-blind hiring anyway. With strictly better-off employment outcomes for group B workers and no worse outcomes for group W workers, the constrained-hiring equilibrium Pareto-dominates the group-blind hiring equilibrium. Similarly, statistical discriminatory hiring leads to group-specific ability thresholds and does not guarantee statistical parity. As Coate and Loury 22 show, self-confirming asymmetric equilibria also exist under this regime, wherein lower investment levels within the group with lower social standing are justified by firms’ more stringent hiring standards. These effects have consequences that mirror 60 the Pareto-dominated results under group-blind hiring. 3.4 Discussion Describing disparate outcomes in employment as caused by rational agent best response strategies suggests that the field of algorithmic fairness should consider the labor market’s inherent dynamic setting in its approach to potential interventions. Fairness constraints that are conceived as isolated procedural checks have a limited capacity to install system-wide fairness that is self-sustaining and long-lasting. The problem of fairness in the labor market is fundamentally tied to historical factors. Within nearly all societal domains in which fairness is an issue, past and current social relations dif- ferentially impact subjects, producing distinct sets of resources, options, and opportunities that continue to mark agents’ choices and outcomes today. Empirical evidence points to what economist and social theorist Glenn Loury has called “development bias,” in which black members of society have reduced chances of realizing their potential, as the greater source of racial inequality in welfare outcomes than discriminatory hiring. 71 This perspective challenges the notion that assuring “indi- vidual fairness” of the actual procedure of hiring should be the primary concern in assuring a labor market that is unbiased as a whole. Not only is the standard learning theory formulation of the problem, in which agent attributes are treated as a priori givens, inadequate to attend to development bias, it also neglects the (ar- guably) meritocratic goals of the labor market. In economic settings, rewarding merit primarily serves an instrumental purpose—to incentivize investment and effort—rather than existing simply to pass along desert-based awards to candidates. Framing the problem as one of clustering or clas- sification fails to understand the labor market as an incentive-oriented system. Fairness criteria that solely assess an algorithm’s treatment of workers’ qualifications similarly fall into the trap of view- ing hiring decisions only as rewards to meritorious individuals without considering the incentive 61 purposes of the reward system at-large. In contrast, a dynamic model recognizes the ripple effect of development bias in the past and calls for a fairness intervention with incentive features that carries momentum into the future. The labor market as a source of economic opportunity is an ideal setting for a notion of fairness that is oriented toward a future beyond the short timeline of firm hiring cycles. It is precisely our focus on steady-state outcomes that allows for this long-term conception of fairness. However, it should be noted that the employment outcomes along the path to the symmetric equilibrium are by no means guaranteed to satisfy any notions of fairness, neither individual nor group. But we claim that conceiving of fairness in this way—as a project that aims to achieve permanent societal group- egalitarianism—is an ambition that is not only a worthy goal in itself but also one that we show may be economically socially optimal. Our model of individual reputations as a sequence of previous outcomes in the PLM fits within the hiring regime today, in which employers have increased access to worker data. Since algorithms will be largely responsible for making sense of this historical data, future work should consider how systems that sift through a worker’s history should be designed to determine when group membership-related considerations, such as the ones embedded in the TLM constraint proposed here, should be taken into account. As machine decision-makers are deployed increasingly through- out hiring processes, we must grapple with a long tradition of explicit and implicit human biases that have rendered the labor market prone to discriminatory practices. We hope that this work can suggest ways that algorithmic fairness interventions can shift these hiring strategies towards con- tributing to a better, fairer future. While this chapter has shown that imposing the TLM hiring constraint ultimately leads to a group-symmetric outcome, we do not claim that ours is the only intervention able to produce such an equilibrium. The labor market pipeline in reality is an elaborate sequence of agent choices and social stages that is much more complex and heterogeneous than our model’s pre-TLM, TLM, and 62 PLM periods. The true space of possible policy interventions dwarfs those considered in this work. Interventions aimed at reducing the economic inequalities that exist between black and white com- munities have been implemented at a variety of junctures in the standard social pipeline, ranging from direct governmental subsidy programs for childhood education costs in high-poverty areas to private companies’ attempts at diversifying hiring by partnering with historically black colleges. As such, there may exist a multiplicity of intervention-types that all ultimately lead to group-egalitarian outcomes. Further analysis of the costs and efficiencies associated with each of these regimes will produce a richer understanding of potential fairness interventions and their concomitant welfare effects. Insofar as work in labor market fairness ought to inspire action and policy in the real world, these open questions will require both theoretical and empirical attention. 63 4 Fair Classification and Social Welfare 4.1 Introduction In his 1979 Tanner Lectures, Amartya Sen noted that since nearly all egalitarian theories are founded on an equality of some sort, the heart of the issue rests on clarifying the “equality of what?” prob- lem. 83 The field of fair machine learning has not escaped this essential question. Does machine learning have an obligation to assure probabilistic equality of outcomes across various social groups? 35,45 64 Or does it simply owe an equality of treatment? 30 Does fairness demand that individuals (or groups) be subject to equal mistreatment rates? 95,11 Or does being fair refer only to avoiding some intolera- ble level of algorithmic error? Currently, the task of accounting for fair machine learning cashes out in the comparison of myr- iad metrics—probability distributions, error likelihoods, classification rates—sliced up every way possible to reveal the range of inequalities that may arise before, during, and after the learning pro- cess. But as shown in work by Chouldechova 20 and Kleinberg et al., 61 fundamental statistical in- compatibilities rule out any solution that can satisfy all parity metrics. Fairness-constrained loss minimization offers little guidance on its own for choosing among the fairness desiderata, which appear incommensurable and result in different impacts on different individuals and groups. We are thus left with the harsh but unavoidable task of adjudicating between these measures and methods. How ought we decide? For a given application, who actually benefits from the operationalization of a certain fairness constraint? This is a basic but critical question that must be answered if we are to understand the impact that fairness constraints have on classification outcomes. Much research in fairness has been motivated by the well-documented negative impacts that these systems can have on already structurally disadvantaged groups. But do fairness constraints as currently formulated in fact earn their reputation as serving to improve the welfares of marginalized social groups? When algorithms are adopted in social environments—consider, for example, the use of predic- tive systems in the financial services industry—classification outcomes directly bear on individuals’ material well-beings. We, thus, view predictions as resource allocations awarded to individuals and by extension, to various social groups. In this chapter, we build out a method of analysis that takes in generic fair learning regimes and analyzes them from a welfare perspective. Our main contributions, presented in Section 3, are methodological as well as substantive in the field of algorithmic fairness. We show that how “fair” a classifier is—how well it accords with a group parity constraint such as “equality of opportunity” or “balance for false positives”—does not 65 neatly translate into statements about different groups’ welfares are affected. Drawing on techniques from parametric programming and finding a SVM’s regularization path, our method of analysis finds the optimal ε-fair Soft-Margin SVM solution for all values of a fairness tolerance parameter ε P r0, 1s. We track the welfares of individuals and groups as a function of ε and identify those ranges of ε values that support solutions that are Pareto-dominated by neighboring ε values. Further, the algorithmic implementation of our analyses is computationally efficient, with a complexity on the same order as current standard SVM solvers that fit a single SVM model, and is thus practical as a procedure that translates fairness constraints into welfare effects for all ε. Our substantive results show that a classifier that abides by a stricter fairness standard does not necessarily issue improved outcomes for the disadvantaged group. In particular, we prove two re- sults: first, starting at any nonzero ε-fair optimal SVM solution, we express the range of Δε ă 0 perturbations that tighten the fairness constraint and lead to classifier-output allocations that are weakly Pareto dominated by those issued by the “less fair” original classifier. Second, there are nonzero ε-fair optimal SVM solutions, such that there exist Δε ă 0 perturbations that yield clas- sifications that are strongly Pareto dominated by those issued by the “less fair” original classifier. We demonstrate these findings on the Adult dataset. In general, our results show that when notions of fairness rest entirely on leading parity-based notions, always preferring more fair machine learning classifiers does not accord with the Pareto Principle, an axiom typically seen as fundamental in social choice theory and welfare economics. The purposes of our work are twofold. The first is simply to encourage a welfare-centric un- derstanding of algorithmic fairness. Whenever machine learning is deployed within important so- cial and economic processes, concerns for fairness arise when societal ideals are in tension with a decision-maker’s interests. Most leading methodologies have focused on optimization of utility or welfare to the vendor but have rarely awarded those individuals and groups who are subject to these systems the same kind of attention to welfare effects. Our work explicitly focuses its analysis on the 66 latter. We also seek to highlight the limits of conceptualizing fairness only in terms of group-based par- ity measures. Our results show that at current, making a system “more fair” as defined by popular metrics can harm the vulnerable social populations that were ostensibly meant to be served by the imposition of such constraints. Though the Pareto Principle is not without faults, the frequency with which “more fair” classification outcomes are welfare-wise dominated by “less fair” ones occurs is troublesome and should lead scholars to reevaluate popular methodologies by which we under- stand the impact of machine learning on different social populations. 4.1.1 Related Work Research in fair machine learning has largely centered on computationally defining “fairness” as a property of a classifier and then showing that techniques can be invented to satisfy such a no- tion. 53,30,97,35,96,52,45,79,17,64,60,95,11,56,28,3 Since most methods are meant to apply to learning prob- lems generally, many such notions of fairness center on parity-based metrics about a classifier’s be- havior on various legally protected social groups rather than on matters of welfare. Most of the works that do look toward a welfare-based framework for interpreting appeals to fairness sit at the intersection of computing and economics. Mullainathan 75 also makes a compar- ison between policies as set by machine learning systems and policies as set by a social planner. He argues that algorithmic systems that make explicit their description of a global welfare function are less likely to perpetrate biased outcomes and are more successful at ameliorating social inequalities. Heidari et al. 48 propose using social welfare functions as fairness constraints on loss minimization programs. They suggest that a learner ought to optimize her classifier while in Rawls’ original posi- tion. As a result, their approach to social welfare is closely tied with considerations of risk. Rather than integrate social welfare functions into the supervised learning pipeline, we claim that the result of an algorithmic classification system can itself be considered a welfare-impacting allocation. Thus, 67 our work simply takes a generic ε-fair learning problem as-is, and then considers the welfare impli- cations of its full path of outcomes for all ε P r0, 1s on individuals as well as groups. Attention to the potential harms of machine learning systems, is not new, of course. Within the field of algorith- mic fairness, Corbett-Davies and Goel 23 and Liu et al. 68 both devote most of their analyses to the person-impacting effects of classificatory systems. The techniques that we use to translate fair learning outcomes into welfare paths are related to a number of existing works. The proxy fairness constraint in our instantiation of the ε-fair SVM prob- lem original appeared in Zafar et al.’s work on restricting the disparate impact of machine classi- fiers. 96 Their research introduces this particular proxy fairness constrained program and shows that it can be efficiently solved and well approximates target fairness constraints. We use the constraint to demonstrate our overall findings about the effect of fairness criteria on individual and group welfares. We share some of the preliminary formulations of the fair SVM problem with Donini et al. 28 though they focus on the statistical and fairness guarantees of the generalized ERM program. Though this area seems far afield from questions of fairness and welfare, our analysis on the effect of Δε fairness perturbations on welfare makes use of these general methods. 26,46,90,89,54 4.2 Problem Formalization Our framework and results are motivated by those algorithmic use cases in which considerations of fairness and welfare stand alongside those of efficiency. Because our chapter connects machine classification and notions of algorithmic fairness with conceptions of social welfare, we first provide an overview of the notation and assumptions that feature throughout our work. In the empirical loss minimization problem, a learner seeks a classifier h that issues the most accu- rate predictions when trained on set of n data points txi, zi, yiun i“1. Each triple gives an individual’s 68 feature vector xi P X , protected class attribute zi P t0, 1u, and true label yi P t´1, `1u.* A classifier that assigns an incorrect label hpxiq ‰ yi incurs a penalty. The empirical risk minimizing predictor is given by h ˚ :“ arg min hPH nÿ i“1 ℓphpxiq, yiq where hypothesis h : X Ñ R gives a learner’s model, the loss function ℓ : R ˆ t´1, `1u Ñ R gives the penalty incurred by a prediction, and H is the hypothesis class under the learner’s consideration. Binary classification systems issue predictions hpxq P t´1, `1u. Notions of fairness have been formalized in a variety of ways in the machine learning literature. Though Dwork et al.’s 30 initial conceptualization remains prominent and influential, much work has since defined fairness as a parity notion applied across different protected class groups. 45,20,61,95,28,3 The following definition gives the general form of these types of fairness criteria. Definition 5. A classifier h satisfies a general group-based notion of ε-fairness if |Ergpℓ, h, xi, yiq|Ezi“1s ´ Ergpℓ, h, xi, yiq|Ezi“0s| ď ε (4.1) where g is some function of classifier h performance, and Ezi“0 and Ezi“1 are events that occur with respect to groups z “ 0 and z “ 1 respectively. Further specifications of the function g and the events E instantiate particular group-based fair- ness notions. For example, when gpℓ, h, xi, yiq “ hpxiq and Ezi refers to the events in which yi “ `1 for each group zi P t0, 1u, Definition 5 gives an ε-approximation of equality of opportunity. 45 When gpℓ, h, xi, yiq “ ℓphpxiq, yiq and Ezi refers to all classification events for each group zi, Definition *Though individuals in a dataset will typically be coded with many protected class attributes, in this chapter we will consider only a single sensitive attribute of focus. 69 5 gives the notion of ε-approximation of overall error rate balance. 20 Notice that as ε increases, the constraint loosens, and the solution is considered “less fair.” As ε decreases, the fairness constraint becomes more strict, and the solution is considered “more fair.” Mapping classification outcomes to changes in individuals’ welfares gives a useful method of analysis for many data-based algorithmic systems that are involved in resource distribution pipelines. In particular, we consider tools that issue outcomes uniformly ranked, or preferred, by those indi- viduals who are the subjects of the system. That is, individuals agree on which outcome is preferred. Examples of such systems abound: applicants for credit generally want to be found eligible; can- didates for jobs generally want to be hired, or at least ranked highly in their pool. These realms are precisely those in which fairness considerations are urgent and where fairness-adjusted learning methods are most likely to be adopted. 4.3 Welfare Impacts of Fairness Constraints The central inquiry of our work asks how fairness constraints as popularized in the algorithmic fairness community relate to welfare-based analyses that are dominant in economics and policy- making circles. Do fairness-adjusted optimization problems actually make marginalized groups better-off in terms of welfare? In this section, we work from an empirical risk minimization (ERM) program with generic fairness constraints parametrized by a tolerance parameter ε ą 0 and trace individuals’ and groups’ welfares as a function of ε. We assume that an individual benefits from receiving a positive classification, and thus we define group welfare as Wk “ 1 nk ÿ i|zi“k hpxiq ` 1 2 , k P t0, 1u (4.2) where nk give the number of individuals in group z “ k. We note that Wk can be defined in ways other than (4.2), which assumes that positive classification are always and only welfare-enhancing. 70 Other work has considered the possibility that positive classifications may in fact make individuals worse-off if they are false positives. 68 The definition of Wk can be generalized to account for these cases. First, in Section 4.3.1, we present an instantiation of the ε-fair ERM problem with a fairness constraint proposed in prior work in algorithmic fairness. We work from the Soft-Margin SVM pro- gram and derive the various dual formulations that will be of use in the following analyses. In Sec- tion 4.3.2, we move on to show how Δε perturbations to the fairness constraint in the ε-fair ERM problem yield changes in classification outcomes for individuals and by extension, how they impact a group’s overall welfare. Our approach, which draws a connection between fairness perturbations and searches for an optimal SVM regularization parameter, tracks changes in an individual’s classifi- cation by taking advantage of the codependence of variables in the dual of the SVM. By perturbing the fairness constraint, we observe changes in not its own corresponding dual variable but in the corresponding dual of the margin constraints, which relay the classification fates of data points. Leveraging this technique, we plot the “solution paths” of the dual variable as a function of ε, which in turn allows us to compute group welfares as a function of ε and draw out substantive re- sults on the dynamics of how classification outcomes change in response to ε-fair learning. We prove that stricter fairness standards do not necessarily support welfare-enhancing outcomes for the disad- vantaged group. In many such cases, the learning goal of ensuring group-based fairness is incompat- ible with the Pareto Principle. Definition 6 (Pareto Principle). Let x, y be two social alternatives. Let ľi be the preference ordering of individuals i P rns, and ľP be the preference ordering of a social planner. The planner abides by the Pareto Principle if x ľP y whenever x ľi y for all i. In welfare economics, the Pareto Principle is a standard requirement of social welfare functionals— it would appear that the selection of an allocation that is Pareto dominated by an available alterna- 71 tive would be undesirable and even irresponsible! Nevertheless, we show that applying fairness crite- ria to loss minimization tasks in some cases do just that. We perform our analysis on the Soft-Margin SVM optimization problem and, for concreteness, work with a well-known fairness formulation in the literature. However, we note that our methods and results apply to fairness-constrained convex loss minimization programs more generally. We also show that this method of analysis can form practical tools. In Section 4.3.3, we present a computationally efficient algorithmic implementation of our analyses, fitting full welfare solution paths for all ε P r0, 1s values in a time complexity that is on the same order as that of a single SVM fit. We close this section by working from the shadow price of the fairness constraint to derive local and global sensitivities of the optimal solution to Δε perturbations. 4.3.1 Setting up the ε-fair ERM program The general fairness-constrained empirical loss minimization program can be written as minimize h P H ℓphpxq, yq subject to fhpx, yq ď ε (4.3) where ℓphpxq, yq gives the empirical loss of a classifier h P H on the dataset X . To maximize accu- racy, the learner ought to minimize 0-1 loss; however because the loss function ℓ0´1 is non-convex, a convex surrogate loss such as hinge loss (ℓh) or log loss (ℓlog) is frequently substituted in its place to ensure that globally optimal solutions may be efficiently found. fhpx, yq ď ε gives a group- based fairness constraint of the type given in Definition 5, where ε ą 0 is the unfairness “tolerance parameter”—a greater ε permits a greater group disparity on a metric of interest; a smaller ε more tightly restricts the level of permissible disparity. We examine the behavior of fairness-constrained linear SVM classifiers, though we note that our 72 techniques generalize to nonlinear kernels SVMs, since interpretations of the dual of the SVM and the full SVM regularization path are the same with kernels. 46 Our learner minimizes hinge loss with L1 regularization; equivalently, she seeks a Soft-Margin SVM that is “ε-fair.” Both SVM models and “fair training” approaches are in broad circulation. The fair empirical risk minimization program is thus given as minimize θθθ, b 1 2 ∥θθθ∥2 ` C nÿ i“1 ξi subject to yipθθθ ⊺xi ` bq ´ 1 ` ξi ě 0, (ε-fair Soft-SVM) ξi ě 0, fθθθ,bpx, yq ď ε where the learner seeks SVM parameters θθθ, b; ξi are non-negative slack variables that violate the mar- gin constraint in the Hard-Margin SVM problem yipθθθ⊺xi ` bq ´ 1 ě 0, and C ą 0 is a hyperparam- eter tunable by the learner to express the trade-off between preferring a larger margin and penalizing violations of the margin. fθθθ,bpx, yq is the group parity-based fairness constraint. The abundant literature on algorithmic fairness presents a long menu of options for the various forms that fθθθ,b could take, but generally speaking, the constraints are non-convex. As such, much work has enlisted methods that depart from directly pursuing efficient constraint-based convex pro- gramming techniques in order to solve them. 53,11,3,95 Researchers have also devised convex proxy alternatives, which have been shown to approximate the intended outcomes of original fairness constraints well. 96,28,93 In particular, in this chapter, we work with the proxy constraint proposed by Zafar et al., 96 which constrains disparities in covariance between group membership and the 73 (signed) distance between individuals’ feature vectors and the hyperplane decision boundary: fθθθ,bpx, yq “ | 1 n nÿ i“1pzi ´ ¯zqpθθθ⊺xi ` bq| ď ε (4.4) ¯z reflects the bias in the demographic makeup of X : ¯z “ 1 n řn i“1 zi. Let (ε-fair-SVM1-P) be the Soft-Margin SVM program with this covariance constraint. The corresponding Lagrangian is LPpθθθ, b, ξξξ, λλλ, μμμ, γ1, γ2q “ 1 2 ∥θθθ∥2 ` C nÿ i“1 ξi ´ nÿ i“1 λi ´ nÿ i“1 μipyipθθθ ⊺xi ` bq ´ 1 ` ξiq ´ γ1`ε ´ 1 n nÿ i“1pzi ´ ¯zqpθθθ⊺xi ` bq ˘ (ε-fair-SVM1-L) ´ γ2` ε ´ 1 n nÿ i“1p¯z ´ ziqpθθθ ⊺xi ` bq˘ where θθθ P Rd, b P R, ξξξ P Rn are primal variables. The (non-negative) Lagrange multipliers λλλ, μμμ P Rn correspond to the n non-negativity constraints ξi ě 0 and the margin-slack constraints yipθθθ⊺xi ` bq ´ 1 ` ξi ě 0 respectively. The multipliers γ1, γ2 P R correspond to the two linearized forms of the absolute value fairness constraint. By complementary slackness, dual variables reveal information about the satisfaction or violation of their corresponding constraints. The analyses in the subsequent two subsections will focus on these interpretations. By the Karush-Kuhn-Tucker (KKT) conditions, at the solution of the convex program, the gra- dients of L with respect to θθθ, b, and ξi are zero. Plugging in these conditions, the dual Lagrangian is LDpμμμ, γq “ ´ 1 2 ∥ nÿ i“1 μiyixi ´ γ n nÿ i“1pzi ´ ¯zqxi∥2 ` nÿ i“1 μi ´ |γ|ε (4.5) where γ “ γ1 ´ γ2. The dual maximizes this objective subject to the constraints μi P r0, Cs for all 74 i P rns and ř i“1 μiyi “ 0. We thus arrive at the Wolfe dual problem maximize μμμ, γ, V ´ 1 2 ∥ nÿ i“1 μiyixi ´ γ n nÿ i“1pzi ´ ¯zqxi∥2 ` nÿ i“1 μi ´ Vε subject to μi P r0, Cs, i “ 1, . . . , n, (ε-fair-SVM1-D) nÿ i“1 μiyi “ 0, γ P r´V, Vs where we have introduced the variable V to eliminate the absolute value function |γ| in the objec- tive. Notice that when γ “ 0 and neither of the constraints bind, we recover the standard dual SVM program. Since we are concerned with fair learning that does alter an optimal solution, we consider cases where V is strictly positive. We introduce additional dual variables β´ and β`, corresponding to the γ P r´V, Vs constraint and derive the Lagrangian Lpμμμ, γ, V, β´, β`q “ ´ 1 2 ∥ nÿ i“1 μiyixi ´ γ n nÿ i“1pzi ´ ¯zqxi∥2 ` nÿ i“1 μi ´ Vε ` γpβ´ ´ β`q ` Vpβ´ ` β`q Under KKT conditions, β´ ` β` “ ε and γ˚ “ npnpβ´ ´ β`q ` řn i“1 μiyixxi, uyq ∥u∥2 (4.6) where u “ řn i“1pzi ´ ¯zqxi geometrically gives some group-sensitive “average” of x P X . We can now rewrite (ε-fair-SVM1-D) as 75 maximize μμμ,β´,β` ´ 1 2 ∥ nÿ i“1 μiyipI ´ Puqxi∥ 2 ` nÿ i“1 μi ` 2n ři μiyixxi, uy ` n2pβ´ ´ β`q 2∥u∥2 pβ´ ´ β`q subject to μi P r0, Cs, i “ 1, . . . , n, nÿ i“1 μiyi “ 0, (ε-fair SVM2-D) β´, β` ě 0, β´ ` β` “ ε where I, Pu P Rdˆd. The former is the identity matrix, and the latter is the projection matrix onto the vector u. As was also observed by Donini et al., the ε “ 0 version of (ε-fair SVM2-D) is equivalent to the standard formulation of the dual SVM program with Kernel Kpxi, xjq “ xpI ´ Puqxi, pI ´ Puqxjy. 28 Since we are interested in the welfare impacts of fair learning when fairness constraints do have an impact on optimal solutions, we will assume that the fairness constraint binds. For clarity of exposition, we assume that the positive covariance constraint binds, and thus that β´ “ 0 and β` “ ε in (ε-fair SVM2-D). This is without loss of generalization—the same analyses apply when the negative covariance constraint binds. The dual ε-fair SVM program becomes minimize μμμ 1 2 ∥ nÿ i“1 μiyipI ´ Puqxi∥2 ´ nÿ i“1 μi ` nεp2 ři μiyixxi, uy ´ nεq 2∥u∥2 subject to μi P r0, Cs, i “ 1, . . . , n, (ε-fair SVM-D) nÿ i“1 μiyi “ 0 76 We will work from this formulation of the constrained optimization problem for the remainder of the chapter. 4.3.2 Impact of Fair Learning on Individuals’ Welfares We now move on to investigate the effects of perturbing a fixed ε-fair SVM by some Δε on the clas- sification outcomes that are issued. We ask, “How are individuals’ and groups’ classifications, and thus their welfares, impacted when a learner tightens or loosens a fairness constraint?” The key insight that drives our methods and results is that rather than perform sensitivity analysis directly on the dual variable corresponding to the fairness constraint—which, as we will see in Section 4.3.4, only gives information about the change in the learner’s objective value—we track changes in the classi- fier’s behavior by analyzing the effect of Δε perturbations on another set of dual variables: μi that correspond to the primal margin constraints. Each of these n dual variables indicate whether its cor- responding vector xi is correctly classified, lies in the margin, or incorrectly classified. Leveraging how these μi change as a function of ε thereby allows us to track the solution paths of individual points and by extension, compute group welfare paths. Define a function ppεq : R Ñ R that gives the optimal value of the ε-fair loss minimizing pro- gram in (ε-fair SVM1-P), which by duality is also the optimal value of (ε-fair SVM-D). We begin at a solution ppεq and consider changes in classifications at the solution ppε ` Δεq, where Δε are pertur- bations can be positive or negative, so long as ε ` Δε ą 0. At an optimal solution, the classification fate of each data point xi is encoded in the dual variable μ˚ i , which is a function of ε. μipεq is the ε- parameterized solution path of μi such that at any particular solution ppεq, the optimal value of the dual variable μ˚ i “ μipεq. As a slight abuse of notation, we reserve notation μipεq for the functional form of the solution path and write με i; to refer to the value of the dual variable at a given ε. 77 Lemma 3. The dual variable paths μipεq for all i P rns are piecewise linear in ε. Though this lemma seems merely of technical interest, it is a workhorse result for both our methodological contributions—our analytical results and our computationally efficient algorithm, which converts fairness constraints to welfare paths—as well as our substantive fairness results about how fairness perturbations impact individual and learner welfares. The algorithm we present in Sec- tion 4.3.3, performs full welfare analysis for all values of ε in a computationally efficient manner by taking advantage of the piecewise linear form of individual and group welfares. Piecewise linearity also sets the stage for the later substantive results about the tension between fairness improvements and the Pareto Principle. We thus walk through the longer proof of this key result in the main text of the chapter as it provides important exposition, definitions, and derivations for subsequent re- sults. Proof. Let Dε be the value of the objective function in (ε-fair SVM-D). By the dual formulation of the Soft-Margin SVM, we can use the value of BDε Bμj to partition the set of indices j P rns in a way that corresponds to the classification fates of individual vectors xj at the optimal solution: BDε Bμj ą 0 ÝÑ μ ε j “ 0, and j P F ε (4.7) BDε Bμj “ 0 ÝÑ μ ε j P r0, Cs, and j P Mε (4.8) BDε Bμj ă 0 ÝÑ μ ε j “ C, and j P E ε (4.9) Hence, xj are either correctly classified free vectors (4.7), vectors in the margin (4.8), or error vectors (4.9). We track membership in these sets by letting tF, M, Euε be the index set partition at the ε- fair solution. To analyze the impact that applying a fairness constraint has on individuals’ or groups’ welfares, we track the behavior of BDε Bμj and observe how vector index membership in sets F ε, Mε, 78 and E ε change under a perturbation to ε. This information will in turn reveal how classifications change or remain stable upon tightening or loosening the fairness constraint. Fairness perturbations do not always shuffle data points across the different membership sets F ε, Mε, and E ε. It is clear that for j P tF, Euε, so long as a perturbation of Δε does not cause BDε Bμj to flip signs or to vanish to 0, j will belong to the same set and hεpxjq “ hε`Δεpxjq where hεpxjq gives the ε-fair classification outcome for xj. In these cases, an individual’s welfare is unaffected by the change in the fairness tolerance level from ε to ε ` Δε. In contrast, vectors xj with j P Mε are subject to a different condition to ensure that they stay in the margin: BDε Bμj “ BDε`Δε Bμj “ 0, i.e., perturbing by Δε does not lead to any changes in BDε Bμj : BDε Bμj “ nÿ i“1 μiyipI ´ PuqxiyjpI ´ Puqxj ` nεyjxxj, uy ∥u∥2 ` byj ´ 1 “ 0 (4.10) for all j P Mε. Let rε,Δε j be the change in με j upon perturbing ε by Δε, then we have μ ε`Δε j “ μ ε j ` rε,Δε j (4.11) recalling that με j is the value of μj at the optimal solution ppεq. Let rrrε,Δε P Rn`1 be the vector of με i sensitivities to perturbations Δε with r ε,Δε 0 as the change in the offset b. For all unshuffled j P Mε, we can compute rε,Δε j by taking the finite difference of (4.10) with respect to a Δε perturbation, nÿ i“1 rε,Δε i yiyjxpI ´ Puqxi, pI ´ Puqxjy ` rε,Δε 0 yj “ ´nyjΔε ∥u∥2 xu, xjy It is clear that rε,Δε i “ 0 for all i that are left unshuffled in the partition tF, Euε. For these “stable ranges” where no i changes its index set membership, we can simplify the previous expression by 79 summing over only those r ε,Δε i where i P Mε: ÿ iPMε rε,Δε i yiyjxpI ´ Puqxi, pI ´ Puqxjy ` rε,Δε 0 yj “ ´nyjΔε ∥u∥2 xu, xjy Thus we can compute r ε,Δε i by inverting the matrix Kε “ ¨ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˚ ˝ 0 y1 y2 . . . y|Mε| y1 ... yiyjxpI ´ Puqxi, pI ´ Puqxjy y2 y|Mε| ˛ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‹ ‚ P Rp|Mε|`1qˆp|Mε|`1q (4.12) where indices are renumbered to only reflect i, j P Mε. This matrix is invertible so long as the margin is not empty and the Kernel Kpxi, xjq “ xpI ´ Puqxi, pI ´ Puqxjy forms a positive definite matrix. Since the objective function in (ε-fair SVM-D) is quadratic, a sufficient condition for Kε to be invertible is that the objective is strictly convex—we assume this as a technical condition.† The †We mention the case in which the margin is empty in Section 3.3, though we refer the interested reader to the Appendix for a full exposition of how με j are updated when the margin is empty and as a result, we cannot compute how i move across index sets via the sensitivities rrr. 80 sensitivities of με j for j P Mε to Δε perturbations are given by rrrε,Δε “ pK εq ´1´ ´n ∥u∥2 v¯ loooooooomoooooooon rrrε Δε, where v “ » — — — — — — — – 0 ... yjxu, xjy ... ﬁ ﬃ ﬃ ﬃ ﬃ ﬃ ﬃ ﬃ ﬂ P R|Mε|`1 (4.13) Plugging this back into (4.11), we have μ ε`Δε j “ μ ε j ` ´`K ε˘´1` ´n ∥u∥2 v ˘¯ jloooooooooomoooooooooon rε j Δε (4.14) Hence, for all j P Mε that stay in the margin, the solution path function μjpεq is linear in ε. For j P tF, Euε that stay in their partition sets, μjpε ` Δεq “ μjpεq, so the function is constant. When Δε perturbations do result in changes in the partition, there are four ways that indices could be shuffled across sets: 1. j P E ε moves into Mε`Δε 2. j P F moves into Mε`Δε 3. j P Mε moves into F ε`Δε 4. j P Mε moves into E ε`Δε Since index transitions only occur by way of changes to the margin, we need now only confirm that each of these transitions maintains continuous μjpεq paths for all j P rns in order to conclude the proof that the paths are piecewise-linear. The linearity of paths μjpεq for j P Mε gives conditions on the ranges of ε wherein individuals’ 81 classification outcomes do not change. As such, for any given tolerance parameter ε, we can com- pute the Δε perturbations that yield no changes to individuals’ welfares. The following Proposition gives the analytical form of these stable regions, where although fairness appears to be “improving” or “worsening,” the adjusted learning process has no material effects on the classificatory outcomes that individuals receive. Proposition 7. Denote the optimal μ˚ j values at an ε-fair SVM solution as με j for j P rns. Let rj “ ´ pK εq´1p ´n ∥u∥2 vq ¯ j with Kε and v as defined in (4.12) and (4.13), dj “ ÿ iPMε riyiyjxpI ´ Puqxi, pI ´ Puqxjy ` r0yj gj “ 1 ´ ´ nÿ i“1 μ ε iyipI ´ PuqxiyjpI ´ Puqxj ` nεyjxxj, uy ∥u∥2 ` byj¯ (4.15) All perturbations of ε in the range Δε P ` maxj mj, minj Mj˘ where mj “ $ ’’’’’’’’’’’’’’& ’’’’’’’’’’’’’’% $ ’’& ’’% gj dj , j P F ε, dj ą 0 ´8, j P F ε, dj ă 0 mint C´με j rj , ´με j rj u, j P Mε $ ’’& ’’% ´8, j P E ε, dj ą 0 gj dj , j P E ε, dj ă 0 Mj “ $ ’’’’’’’’’’’’’’& ’’’’’’’’’’’’’’% $ ’’& ’’% 8, j P F ε, dj ą 0 gj dj , j P F ε, dj ă 0 mint C´με j rj , ´με j rj u, j P Mε $ ’’& ’’% gj dj , j P E ε, dj ą 0 8, j P E ε, dj ă 0 (4.16) yield no changes to index memberships in the partition tF, M, Euε. We defer the interested reader to the Appendix for the full proof of this Proposition, though we provide a sketch here. The result follows from observing that the sensitivities rε i ‰ 0 for i P Mε defined in (4.13) affect the values BDε Bμj for all j P rns, and additional conditions must hold to ensure 82 that the vectors that are not on the margin are also unshuffled by the fairness perturbation. Define g ε j “ 1 ´ ´ nÿ i“1 μ ε iyipI ´ PuqxiyjpI ´ Puqxj ` nεyjxxj, uy ∥u∥2 ` byj¯ (4.17) d ε j “ BDε BμjBε “ ÿ iPMε rε iyiyjxpI ´ Puqxi, pI ´ Puqxjy ` rε 0yj (4.18) The Δε condition for stability of vectors xj for j R Mε is given by gε j dε j (4.19) Recall the conditions of membership in sets F and E as given in (4.7) and (4.9) respectively. The following observations are critical to computing the bounds of the stable region: For j P F ε, perturbations Δε that increase gε j do not threaten j’s exiting the set; if Δε decreases gε j , then j can enter Mε`Δε. Inversely, for j P E ε, perturbations Δε that decrease gε j ensure that j stays in the same partition, i.e., j P E ε`Δε. Perturbations that increase gε j can cause j to shuffle into Mε`Δε. For j P Mε to stay in the margin, we need με`Δε j P r0, Cs. Once με j hits either endpoint of the interval, j risks shuffling across to F ε`Δε or E ε`Δε. Computing these transition inequalities results in a set of conditions that ensure that a partition is stable. Since Δε can be either positive or negative, we take the maximum of the lower bounds (mj) and the minimum of the upper bounds pMjq to arrive at the range of stable perturbations given in (4.16). We call the bounds of this interval the “breakpoints” of the solution paths. This Proposition reveals a mismatch between the ostensible changes to the fairness level of an ε-fair Soft-Margin SVM learning process and the actual felt changes in outcomes by the individuals who are subject to the system. This results from the simple fact that the optimization problem cap- tures changes in the learner’s optimal solution but does not offer such fine-grained information on 83 how individuals’ outcomes vary as a result of Δε perturbations. So long as the fairness constraint is binding and its associated dual variable γ ą 0, then tightening or loosening a fairness constraint does alter the loss of the optimal learner classifier—the actual SVM solution changes—yet analyzed from the perspective of the individual agents xi, so long as the Δε perturbation occurs within the range given by (4.16), classifications issued under this ε ` Δε-fair SVM solution are identical to those under the ε-fair solution. Thus despite the apparent more “fair” signal that a classifier abiding by ε ` Δε ă ε sends, agents are made no better off in terms of welfare. This result is summarized in the following Corollary. Corollary 3. Let tppεq, W0pεq, W1pεqu be a triple expressing the welfares of the learner, group z “ 0, and group z “ 1 under the ε-fair SVM solution. Then for any Δε P pmaxj mj, 0q where mj is defined in (4.16), tppεq, W0pεq, W1pεqu Á tppε ` Δεq, W0pε ` Δεq, W1pε ` Δεqu. Once we have demarcated the limits of Δε perturbations that yield no changes to the partition, i.e., tF, M, Euε “ tF, M, Euε`Δε, we can move on to consider the welfare effects of Δε pertur- bations that exceed the stable region outlined in Proposition 7. At each such breakpoint when Δε reaches maxj mj or minj Mj as defined in (4.16), the margin set changes: Mε ‰ Mε`Δε. As such, rε`Δε j for j P Mε`Δε must be recomputed via (4.13). These sensitivities hold until the next break- point when the set M updates again. We can associate a group welfare with the classification scheme at each of the breakpoints. As al- ready illustrated, index partitions are static in the stable regions around each breakpoint, so group welfares will also be unchanged in these regions. As such, we need only compute welfares at break- points to characterize the paths for ε P r0, 1s. This method of analysis allows practitioners to straightforwardly determine whether the next ε breakpoint actually translates into better or worse outcomes for the group as a whole. Of the four possible events that occur at a breakpoint, index transitions between the partitions 84 M and E correspond to changed classifications that affect group utilities. The following Proposi- tion characterizes those breakpoint transitions that effect welfares triples tppεq, W0pεq, W1pεqu for the learner, group z “ 0, and group z “ 1, that are strictly Pareto dominated by the welfare triple at a neighboring ε breakpoint. The full proof is left to the Appendix. Proposition 8. Consider the welfare triple at the optimal ε-fair SVM solution given by tppεq, W0pεq, W1pεqu. Let bL “ maxj mj ă 0 be the neighboring lower breakpoint where index ℓ “ arg maxj mj; let bU “ minj Mj ą 0 be the neighboring upper breakpoint where index u “ arg minj Mj, assuming uniqueness in the arg max and arg min. If ℓ P E ε and yℓ “ ´1, or if ℓ P Mε and yℓ “ `1, then tppε ` bLq, W0pε ` bLq, W1pε ` bLquu ă tppεq, W0pεq, W1pεqu If u P E ε and yu “ `1, or if u P Mε and yu “ ´1, then tppεq, W0pεq, W1pεqu ă tppε ` bUq, W0pε ` bUq, W1pε ` bUqu Thus minimizing loss in the presence of stricter fairness constraints need not correspond to monotonic gains or losses in the welfare levels of social groups. Fairness perturbations do not have a straightforward effect on classifications. Further, these results do not only arise as an unfortunate outcome of using the particular proxy fairness constraint suggested by Zafar et al. 96 So long as the ε parameter appears in the linear part of the dual Soft-Margin SVM objective function, the μjpεq paths exhibit a piecewise linear form characterized by stable regions and breakpoints. Hence, these results apply to many proxy fairness criteria that have so far been proposed in the literature. 28,93,96 Even when the dual variable paths are not piecewise linear, so long as they are non-monotonic, fairer clas- sification outcomes do not necessarily confer welfare benefits to the disadvantaged group. Mono- tonicity in welfare space is mathematically distinct from monotonicity in fairness space. 85 The preceding analyses show that although fairness constraints are often intended to improve classification outcomes for some disadvantaged group, they in general do not abide by the Pareto Principle, a common welfare economic axiom for deciding among social alternatives. That is, ask- ing that an algorithmic procedure abide by a more stringent fairness criteria can lead to enacting classification schemes that actually make every stakeholder group worse-off. Here, the supposed “improved fairness” achieved by decreasing the unfairness tolerance parameter ε fails to translate into any meaningful improvements in the number of desirable outcomes issued to members of either group. Theorem 5. Consider two fairness-constrained ERM programs parameterized by ε1 and ε2 where ε1 ă ε2. Then a decision-maker who always prefers the classification outcomes issued under the “more fair” ε1-fair solution to those under the “less fair” ε2-fair solution does not abide by the Pareto Principle. 4.3.3 Algorithm and Complexity We build upon the previous section of translating fairness constraints into individual welfare out- comes by considering the operationalization of our analysis and its practicality. The algorithmic procedure presented in this section computes ε breakpoints and tracks the solution paths of the μjpεq for all individuals. Hence, the procedure enables the comparison of different social groups’ welfares—where welfare is determined by the machine’s allocative outcome—by aggregating the classification outcomes of all individuals j in a group z. Algorithm 1 outputs two useful fairness- relevant constructs that have as yet not been explored in the literature: 1) solution paths μjpεq for j P rns tracking individuals’ welfares, and 2) full ε parameterized curves tracking groups’ welfares. The analysis of the previous section forms the backbone of the main update rules that construct the μjpεq paths in Algorithm 1. In particular the values rε j , gε j , and dε j as defined in (4.13), (4.17), and (4.18) respectively are key to computing the ε breakpoints, which in turn fully determine the 86 piecewise linear form of μjpεq. There is, however, one corner case that the procedure must check that was not discussed in the preceding section. We had previously required that the matrix Kε be invertible, which is the case whenever our objective function is strictly convex. But if the margin is empty, the standard update procedure, which computes sensitivities rε j and Kε, will not suffice. The KKT optimality condition řn i“1 μiyi “ 0 requires that the multiple indices moving in the margin at once must be positive and negative examples. For this reason we must refer to a different procedure to compute the ε breakpoint at which this transition occurs. For continuity of the main text of this chapter, the full exposition of this analysis is given in the Appendix. The following complexity result highlights the practicality of implementing the fairness-to- welfare mapping in Algorithm 1 to track the full solution paths of an ε-fair SVM program. We note that standard SVM algorithms such as LibSVM run in Opn3q, and thus once the algorithm has been initialized with the unconstrained SVM solution, the complexity of computing both the full indi- vidual solution paths μjpεq and the full group welfare curves tW0pεq, W1pεqu is on the same order as that of computing a single SVM solution. Theorem 6. Each iteration of Algorithm 1 runs in Opn2 ` |M|2q. For breakpoints on the order of n, the full run time complexity is Opn3 ` n|M|2q. Proof. Each iteration of the fairness-to-welfare algorithm requires the inversion of matrix Kε P R|Mε|`1 and the computations of rε j P R|Mε| for j P Mε, and gε j and dε j for j P tF, Euε. The standard Gauss-Jordan matrix inversion technique runs in Op|M|3q, but we take advantage of partition update rules to lower the number of computations: Since at each new breakpoint, the partition tends to change because of additions or eliminations of a single index j from the set M, we can use the Cholesky decomposition rank-one update or downdate to ease the need to recom- pute the full matrix inverse at every iteration, thereby reducing the complexity of the operation to Op|M|2q. Computing the stability region conditions for j P tF, Eu requires O`pn ´ |M|q|M| ˘ 87 steps. As such, at each breakpoint, the total computational cost is Op|M|2 ` n2q. The number of breakpoints for each full run of the algorithm depends on the data distribution and how sensitive the solution is to the constraint. As a heuristic, datasets whose fairness constraints bind for smaller ε have fewer breakpoints. Previous empirical results on the full SVM path for L1 and L2 regularization have found that the number of breakpoints tends to be on the order of n. 46,90,89,54 Thus after initialization with 0-fair SVM solution, the final complexity for the algorithm is Opn3 ` n|M|2q. 4.3.4 Impact of Fair Learning on Learner’s Welfare Having proven the main welfare-relevant sensitivity result for groups, we return to more standard analysis of the effect of Δε perturbations on the learner’s loss. In this case, we directly solve for the dual variable of the fairness constraint. Recall γ˚ from (A.13): γ˚ “ γ˚ 1 ´ γ˚ 2 “ npnpβ´ ´ β`q ` řn i“1 μiyixxi, uyq ∥u∥2 (4.20) By complementary slackness, one of β´ and β` is zero, and the other is ε. In particular, if β´ “ 0, then β` “ ε, then we know that γ ą 0. Thus the original fairness constraint that binds is the upper bound on covariance, suggesting that the optimal classifier must be constrained to limit its positive covariance with group z “ 1. If β` “ 0, then β´ “ ε and γ ă 0, and the classifier must be constrained to limit its positive covariance with group z “ 0. We can interpret the value of the dual variable Lagrange multiplier as the shadow price of the fairness constraint. It gives the additional loss in the objective value that the learner would achieve if the fairness constraint were infinitesimally loosened. Whenever a fairness constraint binds, its shadow price is readily computable and is given by |γ˚|. It bears noting that because (ε-fair Soft- 88 SVM) is not a linear program, |γ˚| can only be interpreted as a measure of local sensitivity, valid only in a small neighborhood around an optimal solution. But through an alternative lens of sensitivity analysis, we can derive a lower bound on global sensitivity due to changes in the fairness tolerance parameter ε. By writing ε as a perturbation variable, we can perform sensitivity analysis on the same ε-constrained problem. Returning to the perturbation function ppεq, we have ppεq ě sup μμμ,γ tLpμμμ ˚, γ˚q ´ ε|γ˚|u (4.21) where Lpμμμ˚, γ˚q gives the solution to the 0-fair SVM problem. Lpμμμ ˚, γ˚q “ max μμμPr0,Csn,γ ´ 1 2 ∥ nÿ i“1 μiyipI ´ Puqxi∥2 ` ÿ i“1 μi (4.22) The perturbation formulation given in (4.21) is identical in form to the original program (ε-fair- SVM1-P) but gives a global bound on ppεq for all ε P r0, 1s. Since (4.21) gives a lower bound, the global sensitivity bound yields an asymmetric interpretation. Proposition 9. If Δε ă 0 and |γ˚| \" 0, then ppε ` Δεq ´ ppεq \" 0. If Δε ą 0 and |γ˚| ă δ for small δ, then ppε ` Δεq ´ ppεq P r´δΔε, 0s, and is thus also small in magnitude. Proposition 9 shows that tightening the fairness constraint when its shadow price is high leads to a great increase in learner loss, but loosening the fairness constraint when its shadow price is small leads only to a small decrease in loss. 4.4 Experiments To demonstrate the efficacy of our approach, we track the impact of ε-fairness constrained SVM programs on the classification outcomes of individuals in the Adult dataset. The target variable in the dataset is a binary value indicating whether the individual has an annual income of more or less 89 than $50,000. If such a dataset were used to train a tool to be deployed in consequential resource allocation—say, for the purpose of determining access to credit—then classification decisions di- rectly impact individuals’ welfares. Individual solution paths and relative group welfare changes are given in Figure 1. As ε increases from left to right, the fairness constraint is loosened, and outcomes become “less fair.” In the case of the ε-fair SVM solution to the Adult dataset, the fairness constraint ceases to bind at the optimal solution when ε « 0.175. The top panel shows example individual piecewise linear paths of dual variables μipεq, providing a visual depiction of how individual points can transition across index sets: from μi “ 0, i P F and being correctly labeled, to μi P p0, 1q, i P M, being correctly labeled but in margin; to μi “ 1, i P E and being incorrectly labeled. Solid paths indicate individuals coded female; dashed paths indicate those coded males. As the top panel of Figure 1 shows, the actual “journey” of these paths are varied as ε changes. As expected, tightening the fairness constraint in the ε-fair program does tend to lead to im- proved welfare outcomes for females as a group (more female individuals receive a positive classifi- cation), while males experience a relative decline in group welfare (receiving fewer positive classifica- tions). However, as suggested by our results in Section 3.2, these welfare changes are not monotonic for either group. Tightening the fairness constraint could lead to declines in both groups’ welfares, demonstrating that preferring more fair solutions in this predictive model does not abide by the Pareto Principle. We highlight an instance of this result in the bottom panel of Figure 1, where or- ange dashed lines to the left of black ones mark off solutions where “more fair” outcomes (orange) are Pareto-dominated by “less fair” (black) ones. A practitioner working in a domain in which wel- fare considerations might override parity-based fairness ones may prefer the outcomes of a fair learn- ing procedure with ε « 0.045 to one with ε « 0.015. Additional plots showing absolute changes in group welfare and optimal learner value are given in the Appendix. 90 Figure 4.1: Fairness‐to‐welfare solution paths for individuals (top panel) and groups (bottom panel) on the Adult dataset. 4.5 Discussion The question that leads off this chapter—How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare?—sets an important agenda to come for the field of algorithmic fairness. It asks that the community look to disciplines that have long considered the problem of allocating goods in accordance with ideals of justice and fairness. For example, the notion of welfare in this chapter draws from work in welfare and public economics. The outcomes issued by an optimal classifier can, thus, be interpreted using welfare economic tools developed for considerations of social efficiency and equity. In an effort to situate computer scien- tists’ notions of fairness within a broader understanding of distributive justice, we also show that loss minimization problems can indeed be mapped onto welfare maximization ones and vice versa. For reasons of continuity, analyses of this correspondence do not appear in the main text—we defer the interested reader to the Appendix—though we present an abbreviated overview here. We en- 91 courage readers to consider the main results of this chapter, which construct welfare paths out of fair learning algorithms, as a part of this larger project of bridging the two approaches. 4.5.1 Bridging Fair Machine Learning and Social Welfare Maximization To highlight the correspondence between the machine learning and welfare economic approaches to allocation, we show that loss minimizing solutions can be understood as welfare maximizing ones under a particular social welfare function. In the Planner’s Problem, a planner maximizes social welfare represented as the weighted sum of utility functions, where each individual’s weight repre- sents the value placed by society on her welfare. Inverting the Planner’s Problem of social welfare maximization generates a question concerning social equity: “Given a particular allocation, what is the presumptive social weight function that would yield it as optimal?” We show that the set of pre- dictions issued by the optimal classifier of any loss minimization task can be given as the set of allo- cations in the Planner’s Problem over the same individuals endowed with a set of welfare weights. These weights lie at the heart of debates over fairness of distribution in economics. Analyzing the distribution of implied weights of individuals and groups offers a welfare economic way of consider- ing the “fairness” of classifications. 4.5.2 Interpreting Welfare Alongside Fairness Welfare economics can lend particular insights into formalizing notions of distributional fairness and general insights into building a technical field and methodology that grapples with normative questions. The field is concerned with what public policies ought to be, how to improve individ- uals’ well-beings, and what distribution of outcomes are preferable. Answers to these questions appeal to values and judgments that refer to more than just descriptive or predictive facts about the 92 world. The success of fair machine will largely hang on how well it can adapt to a similar ambitious task. However, welfare economics is not the only—nor should it even serve as the main—academic resource for thinking through how goods ought to be provisioned in a just society. In this moment of broad appeal to the prowess of algorithmic systems, researchers in computing are called on to advise on matters beyond their specialized expertise and training. Many of these matters require explicit normative, political, and social-scientific reasoning. Insights and methods from across the arts, humanities, social sciences, and natural sciences bear fruit in answering these questions. This chapter does not look to contribute a new fair learning algorithm or a new fairness defini- tion. We take a popular classification algorithm, the Soft Margin SVM, append a parity-based fair- ness constraint, and analyze its implications on welfare. The constraint that we center in the chapter is just one concretization of a large menu of fairness notions that have been offered up to now. The method of analysis developed in the chapter applies generally to any convex formulations of these constraints, including versions of balance for false positives, balance for false negatives, and equality of opportunity that have circulated in the literature. 93,28,3 It is important future work to investigate the welfare implications of state-of-the-art fair classification algorithms that the community contin- ues to develop, which can deal with a wider range of models and constraints, including non-convex ones. This chapter asks that researchers in fair machine learning reevaluate not only their lodestars of optimality and efficiency but also their latest metrics of fairness. By viewing classification outcomes as allocations of a good, we incorporate considerations of individual and group utility in our anal- ysis of classification regimes. The concept of “utility” in evaluations of social policy remains con- troversial, but in many cases of social distribution, utility considerations provide a partial but still important perspective on what is at stake within an allocative task. Utility-based notions of welfare can capture the relative benefit that a particular good can have on a particular individual. If machine 93 learning systems are in effect serving as resource distribution mechanisms, then questions about fairness should align with questions of “Who benefits?” Our results show that many parity-based formulations of fairness in machine learning do not ensure that disadvantaged groups benefit. Pre- ferring a classifier that better accords with a fairness measure can lead to selecting allocations that lower the welfare for every group. We note that nevertheless, there are several reasons in favor of lim- iting levels of inequality not reflected in utilitarian calculus. In some cases, the gap between groups is itself objectionable, and considerations of relational equality between groups overrides gains to the absolute utility level of disadvantaged groups. But without acknowledging and accounting for these reasons, well-intentioned optimization tasks that seek to be “fairer” can further disadvantage social groups for no reason but to satisfy a given fairness metric. 94 ALGORITHM 1: Fairness-to-welfare solution paths as a function of ε Input: set X of n data points txi, zi, yiu Output: solutions paths μμμpεq and group welfare curves tW0pεq, W1pεqu μμμ0 = arg minμμμ Dpμμμq of (0-fair SVM-D); ε “ 0, Δε “ 0; |n0| “ řn i“1 1rzi “ 0s, |n1| “ řn i“1 1rzi “ 1s; while ε ă 1 do W0 “ 0, W1 “ 0; for each με i do update tF, M, Euε according to (4.7), (4.8), (4.9); if (μi ă C & yi “ 1q || pμi “ C & yi “ 0q then Wzi “ Wzi ` 1; end end W0pεq “ W0 n0 ; W1pεq “ W1 n1 ; if |M ε| “ 0 then Δε “ mini Mi as given in (A.16); update tF, M, Euε according to (A.18) and (A.19); ε “ ε ` Δε; end compute rrr ε, dddε according to (4.13), (4.18); Δε “ mini Mi as given in (4.16); με`Δε i “ με i ` r ε iΔε for i P Mε, με i “ με`Δε i for i P tF, Euε; ε “ ε ` Δε; end return pμμμpεq, W0pεq, W1pεqq 95 5 Conclusion The orientation that I adopt towards algorithmic fairness in this dissertation is one that focuses on the interaction between machine classifications and the broader societal contexts within which they are embedded. I consider changes to the incentive structures that data-based classification intro- duces, the strategic responses of agents who interact with such systems, and the welfare impacts of various fairness constraints that have been proffered in the field. Hence, the common theme that knits these works together is that of a perspective that foregrounds the dynamics of algorithmic fair- 96 ness. Through the works contained in this dissertation, I hope to have shown the virtues of taking this approach. Models that re-embed algorithms in their social and economic environments provide insight into fairness that: • inform how these tools actually operate in the real world to impact social outcomes, • challenge standard wisdom in algorithmic fairness, • better equip us to design systems that work against social inequality. In the past several years, researchers in the field have increasingly come to adopt a broader so- ciotechnical framing of the problem of algorithmic fairness. One upshot of this gradual shift in perspective is that the dynamics-focused, wider lens analysis that I take in this dissertation is less distinctive than it was six years ago, when I first embarked on work in this area. I therefore want to conclude this dissertation by pivoting to propose new paths forward for theorizing about fair classi- fication and thus, fair distribution, at a higher level of abstraction that also makes connections with some of my philosophical work on topics adjacent to algorithmic fairness. My first suggestion is one that may be surprising given the field’s shift in recent years away from putting forth new technical definitions of fairness and my preceding comments about these pro- posals. While I agree that all such formal accounts suffer from serious defects and fail as “defini- tions” of fairness, their introduction into the discourse has nevertheless been fruitful. They have forced us to articulate precisely what might be wrong with notions such as, say, meritocratic fair- ness 57 or equalized odds 45 or counterfactual fairness 64—not just from a technical perspective about their effects on predictive accuracy or their difficulty in being translated into convex constrained optimization problems but from a distinctively normative perspective: why such notions are not adequate conceptions of fairness on moral and political grounds. Works such as Mitchell et al.’s “Algorithmic Fairness: Choices, Assumptions, and Definitions” 74 and Reubin Binns’ “Fairness in Machine Learning: Lessons from Political Philosophy” 13 uncover and elucidate what value-laden 97 modeling choices and assumptions are embedded in technical accounts of fairness and thus what their substantive ethical content is. Laying out formal definitions of fairness to be scrutinized in this manner thus yields generative cross-disciplinary dialogue, which is beneficial to the field as a whole. In the present world within which we live, machine learning algorithms play a crucial function in many important resource allocation pipelines. Computer scientists have thus been unwittingly cast as partial social planners. Given this state of affairs, it is paramount that the values that structure algorithm design are made explicit to broader society as well as to the engineers who build the sys- tems themselves. Furthermore, technical formalization of fairness contributes also to our normative thinking about what constitutes fairness. For example, the field’s so-called fairness “impossibility results” have spurred significant discussion about which if any of the three fairness criteria of cal- ibration, balance for false positives, or balance for false negatives is necessary and/or sufficient for fairness 47,69. The results have also prompted scholars to renovate accounts of discrimination in the law, consider which remedies towards fairness might be compatible with or at odds with what equal protection in the law requires, and evaluate which approaches to fairness are more likely to in fact make headway in addressing social inequalities 72,49,94. Much of this progress was spurred by com- puter scientists’ proposals of fairness definitions. Though they might have themselves been shown to be deficient, these formalizations generated fruitful critical discussion and exchange that both deepens our understanding of the normative matters at stake as well as strengthens our ability to build tools that may better meet our aims towards “fairness”. More work that follows in this vein can thus be constructive for the field’s development. Second, the rapid rise of artificial intelligence and machine learning tools and the sheen of an ex- citing “newness” of these technologies can often make it seem as though the questions at the heart of the field of algorithmic fairness are also new and newly urgent. In truth, a field that is method- ologically not so far off from our own, that of welfare economics, has for decades been concerned with the development of technical tools to probe social and fundamentally value-laden ethical ques- 98 tions of resource distribution. And so welfare economics, in my view, stands out as a natural point of connection for technical research in algorithmic fairness. The discipline can both provide specific insights into formalizing substantive notions of fairness in distribution and also general insights into how to build a technical field and methodology that more effectively grapples with normative ques- tions. Welfare economics is carved out as the branch of economics that is explicitly concerned with what public policies ought to be, how to maximize individuals’ well-beings, and what types of dis- tributive outcomes are preferable. Answers to these questions appeal to values and judgments that do not refer only to descriptive or predictive facts about a state of affairs. It would appear that the success of fair machine will largely hang on how well it can adapt to a similar ambitious task. Since notions of fairness are invariably context-dependent and always informed by background normative views, it is unsurprising that there has been such wide disagreement within the community about which of the many fairness definitions is the “right” one. Insofar as developing a unified framework of analysis of these competing formal notions, their compatibilities with each other, and their im- pacts on other social values such as efficiency are key disciplinary aims for both algorithmic fairness and welfare economics, each community has great potential to grow from engagement with the other. Finally, work on causal inference that studies the causal effects of social categories such as race and sex shares significant overlap with concerns about the discriminatory potential of machine learning systems. Much research in causal inference in the social sciences looks to identify race or sex causal estimands from observational data and thereby claims to quantify the extent of discrimi- nation on the basis of race or sex. It is thus no surprise that causal methods have been imported into approaches towards fairness in data-based predictive systems. Causal inference about race and sex has notoriously been the subject of decades-long method- ological and conceptual disputes, which continues to this day. In recent works, prominent causal inference practitioners have debated whether one can even quantify the effect that race has on a 99 decision that takes place downstream of other decisions that were themselves causally affected by race. 63,42,98 Cases of such multi-stage race-inflected decision-making processes abound. As an exam- ple, if race influences who police decide to stop such that administrative records of police encounters embody a selection effect, then certain race-causal estimands on outcomes downstream from stops will be biased absent strong untestable assumptions. This problem clearly bears directly on the use of any such data in machine learning-based classification systems. Work on the methodological chal- lenges to producing unbiased estimates of such race-causal estimands and bounding such effect estimates will greatly inform the extent to which machine learning systems can take biased input data at “face-value”. But furthermore, the problem of upstream racial bias presents also a concep- tual problems for the prospect of quantifying the amount of ”taint” that data have and thus the extent to which one can claim that an algorithm’s outputs are not causally influenced by race. I ar- gue elsewhere with legal scholar Issa Kohler-Hausmann that if causal inference practitioners start with the premise that there exists a race selection effect (i.e., racial discrimination or bias) on obser- vational data from which their methods draw, then race-causal estimands quantified using such data are misdefined, because of a violation of the consistency axiom of causal inference. 50 Those who work in causal inference frequently debate which assumptions about the data generation process are necessary and/or sufficient in order for causal identification to be sound. Research in algorith- mic fairness stands to greatly benefit from further contact with such work, as the methodological and conceptual problems at issue there are central too in our field. This is a line of research that I am pursuing in my own philosophical work, where I hope that bringing my technical and analytical tools to tackle foundational questions in causal reasoning in the (social) sciences can cross over to make an impact on how we think about what constitutes fairness and discrimination in algorithms. 100 A 101 Appendices A.1 Appendix for Chapter 2 A.1.1 Proofs from Section 2.3.1 Proof of Proposition 1 We first construct the optimal learner classifier when facing only candidates of a single group. Sup- pose the learner encounters only group A candidates. Then using her knowledge that the true classi- fier hA is based on a threshold τA P r0, 1s, she can construct a classifier that admits those candidates with scores x ě τA and rejects candidates x ă τA. Since the maximal manipulation cost that any candidate would be willing to undertake is 1, for all x P r0, 1s, cApyq ´ cApxq ď 1 and therefore y ď c´1 A pcApxq ` 1q Thus a candidate with feature x “ τA would be able to move to any feature y ď σA where σA “ c´1 A pcApτAq ` 1q. Repeating the same reasoning for group B, a candidate with feature x “ τB would be willing to move to any feature y ď σB where σB “ c ´1 B pcBpτBq ` 1q. Now we want to show that rσB, σAs marks an interval of undominated strategies. First we prove the ordering that σB ď σA for all cost functions cB and cA and all thresholds τB ď τA. Recall that since hApxq “ 1 ùñ hBpxq “ 1, we have τB ď τA. Although we cannot order cBpτBq and cApτAq, 102 we have, by monotonicity of cB cBpτBq ď cBpτAq. Let Δ “ cBpτAq ´ cBpτBq. Notice that if Δ ě 1, cBpτBq ` 1 ď cBpτAq, and so σB “ c´1 B pcBpτBq ` 1q ď τA ă σA, where the last inequality is due to monotonicity of cA. ✓ Let us consider the Δ P p0, 1q case. By the cost condition, we can write c1 BpτAq ě c1 ApτAq. This implies that c ´1 B pcBpτAq ` 1q ď c´1 A pcApτAq ` 1q Substituting in cBpτAq “ cBpτBq ` Δ, we have c ´1 B pcBpτBq ` Δ ` 1q ď c´1 A pcApτAq ` 1q “ σA. By monotonicity of cB, the left hand side is ě σB, and we have that σB ď σA as desired. ✓ Notice that for all σ ă σB, the learner commits false positive errors on candidates from group B, since σB is optimal for group B classification. She commits more false positives on group A candi- dates as well and does not commit any fewer false negatives because of the monotonicity of cB and cA. Thus for any error function with CFP ą 0, the threshold classifier σB dominates σ. Similarly, for all σ ą σA, the learner commits false negative errors on candidates from group A, since σA is optimal for group A classification. She also commits more false negatives on group B while committing no fewer false positives. Thus for any error function with CFN ą 0, the threshold classifier σA dominates σ. For all σ P rσB, σAs, the learner trades off false negatives on group B for false positives on group A, 103 and we call this range of threshold strategies undominated. Proof of Proposition 2 We compute the cost of a learner’s threshold strategy σ P rσB, σAs by first examining its performance on each group individually. Recall from Proposition 1 that the optimal learner threshold that perfectly classifies all B candi- dates is σB. Thus for all threshold strategies based on σ P pσB, σAs, the learner commits false negative errors on group B. To compute which members of group B are subject to these errors, consider a learner classifier f based on a threshold σ. In order to manipulate to reach the feature threshold σ, a group B candidate must have an unmanipulated x such that cBpσq ´ cBpxq ď 1, x ě c ´1 B pcBpσq ` 1q “ ℓBpσq. We know that τB ď ℓBpσq by monotonicity of cB, and thus for all group B candidates with feature x P rτB, ℓBpσqq, the learner issues classification fpxq “ 0, even though hBpxq “ 1. These are the false negative errors issued on group B for which the learner bears cost CFNpBPx„DB“ x P rτB, ℓBpσqq‰ (A.1) Following the same reasoning, notice that since σA is the optimal threshold policy for a learner facing only group A candidates, a classifier f based on any σ P rσB, σAq commits false positive errors on some group A candidates. Then repeating the steps that we carried out for group B, we see that 104 for all group A candidates with x such that x ě c´1 A pcApσq ` 1 “ ℓApσq the classifier f issues a positive classification; fpxq “ 1. Since ℓApσq ď τA, candidates with features x P rℓApσq, τAq, have true label hApxq “ 0, and the learner commits false positive errors that bear cost CFPpAPx„DA“x P rℓApσq, τAq‰ (A.2) Combining (A.1) and (A.2), the total cost of any classifier f based on a threshold σ P rσB, σAs, we obtain our desired result. Proofs of Corollaries 1 and 2 These results follow by considering strategies σB, which commits no errors on group B and thus only bears the cost given in (A.2), and σA, which commits no errors on group A and thus only bears the cost given in (A.1). Proof of Proposition 3 Under the assumption of uniform feature distributions for both groups, minimizing a classifier’s probability of error amounts to choosing the threshold σ as arg min σPrσB,σAs ℓBpσq ´ ℓApσq. 105 With proportional group costs cApxq “ qcBpxq for q P p0, 1q, we have that ℓ1 Bpσq “ pcBq1pσq ´ cB¯1´`cB˘´1`cBpσq ´ 1 ˘¯ “ pcBq1pσq ´ cB¯1´ ℓBpσq ¯ and ℓ 1 Apσq “ pcAq1pσq ´cA¯1´`cA˘´1`cApσq ´ 1 ˘¯ “ pqcBq1pσq ´qcB¯1´` cA˘´1` cApσq ´ 1 ˘¯ “ pcBq1pσq ´cB¯1´` cA˘´1` cApσq ´ 1 ˘¯ “ pcBq1pσq ´cB¯1´ℓApσq ¯ . When cA and cB are strictly concave, since ℓBpσq ą ℓApσq, pcBq1pℓApσqq ą pcBq1pℓBpσqq and therefore ℓ1 Apσq ă ℓ1 Bpσq for all σ P rσB, σAs, and the quantity ℓBpσq ´ ℓApσq is monotonically increasing in σ. Thus the optimal classifier threshold is σ˚ “ σB. Similarly, when cA and cB are strictly convex, ℓ1 Apσq ą ℓ1 Bpσq for all σ P rσB, σAs, and the quantity ℓBpσq ´ ℓApσq is monotonically decreasing in σ. Thus the optimal classifier threshold is σ˚ “ σA. Thus the optimal classifier threshold is σ˚ “ σA. Finally, when cA and cB are affine, ℓ1 Apσq “ ℓ1 Bpσq for all σ P rσB, σAs, and the quantity ℓBpσq ´ ℓApσq is constant for all σ P rσB, σAs. Thus the learner is indifferent between all thresholds σ P rσB, σAs. 106 A.1.2 Proofs from Section 2.3.2 Proof of Lemma 1 Consider a candidate with unmanipulated feature x P r0, 1sd and manipulation cost řd i“1 cixi who faces a classifier fpyq with linear decision boundary given by řd i“1 giyi “ g0. Recall that the utility a candidate receives for presenting feature y ě x is given by fpyq ´ cpx, yq. When fpxq “ 1, it is trivial that the candidate’s best response to select y “ x. ✓ Notice that if for all i P rds, fpx ` 1 ci eiq “ 0, then we have that g⊺x ` gk ck ă g0, so ckpg0 ´ g⊺xq gk ą 1 The manipulation from x to y “ x ` ř iPK ti ci ei such that g⊺y “ g0 entails cost cpyq ´ cpxq “ ÿ iPK ti “ ckpg0 ´ g⊺xq gk ą 1 and manipulating to achieve a positive classification using only components in K would require a cost ą 1. By definition, keeping the sum ř iPK ti, but selecting different ti such that some i R K, ti ą 0 would yield an even lower value g⊺x ` řd i“1 giti ci . Thus manipulating from x to y such that fpyq “ 1 entails a cost cpyq ´ cpxq ą 1, and the candi- date would not move at all, since the utility for moving 1 ´ pcpyq ´ cpxqq ă 0 makes her worse-off than being subject to a negative classification without expending any cost on feature manipulation. Thus she selects y “ x. ✓ Now we consider the case where fpxq “ 0 and there exists i P rds such that fpx ` 1 ci eiq “ 1. Let k P K “ arg maxiPrds gi ci . We prove that the best-response manipulation for candidates with 107 these x moves to y “ x ` dÿ i“1 ti ci ei (A.3) where ti ě 0, tj “ 0 for all j R K, and g⊺px ` ř iPK ti ci eiq “ g0. Note that such a y may not be unique—there may be multiple best-response manipulated features that achieve the same candidate utility, since they all result in the same candidate cost, and thus regardless of choices i P K, we have that ÿ iPK ti “ ckpg0 ´ g⊺xq gk (A.4) The utility of any move to y satisfying (A.3) is given by fpy˚q ´ cpx, y˚q “ 1 ´ ÿ i“1 ti Let us pick any such y and call it y˚ since we will show that all other manipulations that are not of the form given in (A.3) generate lower utility for the candidate than y˚. We now show that for any manipulation to y, řd i“1 ti ď 1. By assumption, for some i, we have fpx ` 1 ci eiq “ 1 ùñ g⊺x ` gi ci ě g0 Thus by (A.4), we have that řiPK ti ď ck gi ci gk . By definition of k, this is at most one since gk ck ě gi ci for all i P rds. ✓ Suppose on the contrary that there exists another manipulated feature ˆy ‰ y˚ that is optimal and is not of the form (A.3): fpˆyq ´ pcpˆyq ´ cpxqq ě 1 ´ ckpg0 ´ g⊺xq gk ě 0 Then it must be the case that moving to ˆy achieves a positive classification with a lower cost bur- 108 den. We write ˆy “ x ` ÿ i“1ˆtiei where ei is the ith standard basis vector, andˆtj “ ˆyj ´ xj to highlight the components that have been manipulated from x to ˆy. First, we suppose that ˆy is such that there exists some component ˆyj ą 0 where j R K “ arg maxiPrds gi ci . Now we construct a feature ˆy1 by selecting this component, and decreasingˆtj “ 0 and increasing a component k P K by cjˆtj ck . That is ˆy1 “ ˆy ´ ˆtjej ` cjˆtj ck ek The cost of manipulation from x to ˆy1 is the same as that for manipulation to ˆy: cpˆy1q ´ cpxq “ dÿ i“1 ciˆyi ´ ˆtjcj ` ck cjˆtj ck “ dÿ i“1 ciˆyi Notice that now we have dÿ i“1 giˆy 1 i “ dÿ i“1 giˆyi ´ gjˆtj ` gkcjˆtj ck ą dÿ i“1 giˆyi ě g0. Thus the candidate can manipulate to ˆy1 by expending the same cost with dÿ i“1 giˆy 1 i ą g0 Then by continuity of g, there must exist some ¯y ď ˆy1 such that řd i“1 gi¯yi P rg0, řd i“1 giˆy1 iq. Thus since costs are monotonically increasing, cpx, ¯yq ă cpx, ˆyq and since ¯y reaches the same classification, and we have shown that ˆy could not have been optimal, which is a contradiction. ✓ 109 Now we consider the case where ˆy “ x ` řd i“1 ˆtiei is such thatˆtj “ 0 for all j R K, but g⊺ˆy ‰ g0. If g⊺ˆy ă g0, then ˆy is negatively classified and thus trivially receives a lower utility than manipulating to any feature y that is positively classified and associated with total cost ř i ti ď 1. If g⊺ˆy ą g0, then there are two possibilities: If cpˆyq ´ cpxq ě 1, then once again, she receives at most a utility of 0, and thus manipulating to ˆy is a suboptimal move. If cpˆyq ´ cpxq ă 1, then we show the optimal manipulation is the one that moves from x to y “ x ` ÿ i“1 tiei where g⊺y “ g0 and tj “ 0, @j R K—the move dictated by (A.3). This feature y also achieves a positive classification, but we argue that it does so at a lower cost than ˆy. Since g⊺ˆy ą g0, we can define Δ “ g⊺ˆy ´ g0 ą 0 The manipulation from x to ˆy ´ Δ gk ek for any choice of k attains a higher utility since it receives the same classification since g⊺pˆy ´ Δ ck ekq “ g0 but does so at a cost cpyq ´ cpxq “ cpˆyq ´ cpxq ´ Δ Since we already showed that all manipulations to y of the form given in (A.3) bear the same cost, then we have shown that all such y are preferable to ˆy. By monotonicity of cpyq´cpxq and řd i“1 gixi, all manipulations with lower cost entail a negative classification and thus a lower utility, and such only those manipulations to y are optimal. 110 Proof of Theorem 1 We first prove that a learner who has access to the linear decision boundary for the true classifier can construct a classifier that commits no errors on any candidates from a single group; thus, in our setting, perfect classifiers exist for groups A and B. We then prove that all undominated classifiers commit no false positives on group B and no false negatives on group A. Suppose true classifiers are given by hA and hB based on decision boundaries řd i“1 wA,ixi “ τA and řd i“1 wB,ixi “ τB, costs are cApxq “ řd i“1 cA,ixi and cBpxq “ řd i“1 cB,ixi. Claim 1: When facing candidates from a single group, a learner who has access to true decision boundary řd i“1 wixi “ τ and manipulation costs řd i“1 cixi can construct a perfect classifier. Proof. Consider those features ¯x P r0, 1sd that lie on the true decision boundary řd i“1 wixi “ τ and thus have true labels 1. For each of these ¯x, we construct Δp¯xq as defined in (2.12) to represent the candidate’s space of potential manipulation to form the set tΔp¯xqu for all ¯x on the boundary. Notice that when all candidates face the same cost, the set of jth vertices of each of the simplices Δp¯xq, given by vjp¯xq “ ¯x ` 1 cj ej, are coplanar. Each of these hyperplanes can be described as a set # y : dÿ i“1 wiyi “ τ ` wj cj + . Let k P arg maxj wj cj . We define g1 to be a notational shortcut for the hyperplane corresponding to feature k, so g1 “ $ & %y : dÿ j“1 g1,jyj “ g1,0 , . - , where g1,0 “ τ ` wk ck and g1,i “ wi for all i P t1, ..., du. We define a classifier f1 based on the 111 hyperplane g1: f1pyq “ $ ’’& ’’% 1 řd j“1 g1,jyj ě g1,0, 0 řd j“1 g1,jyj ă g1,0. (A.5) To show that f1 is a perfect classifier of all candidates with these generic costs, we show that it commits no false positive errors and no false negative errors. Notice that since g1 was constructed to be precisely the hyperplane that contains all vertices vkp¯xq “ ¯x ` 1 ck of the simplices Δp¯xq where k P arg maxjPrds wj cj , then all ¯x on the true decision boundary řd i“1 wixi “ τ can indeed manipulate to vkp¯xq and reach g1 to gain a positive classification. Similarly, all candidates with features x such that řd i“1 wixi ą τ, can move to the kth vertex of the simplex Δpxq given by vkpxq “ x ` 1 ck ek in order to be classified positively since dÿ i“1 wivk,ipxq ą τ ` wk ck ùñ dÿ i“1 g1,jvk,ipxq ą g1,0. Thus f1 correctly classifies all these candidates positively and permits no false negatives. ✓ Consider the optimal manipulation for all true negative candidates x. By Lemma 1, the optimal manipulation would be either to not move at all, guaranteeing a negative classification, or to move x to some point y “ x ` řd i“1 ti ci ei where tj “ 0 for all j R arg maxjPrds g1,j cj . But since řd i“1 wixi ă τ, then for all such y, dÿ i“1 wiyi ď dÿ i“1 wixi ` wk ck ă τ ` wk ck ùñ dÿ i“1 g1,jyj ă g1,0 and thus the classifier based on the hyperplane g1 also issues a classification f1pxq “ 0 and admits no false positives. ✓ Thus we have shown that the hyperplane g1 supports a perfect classifier f1 as defined in (A.5). 112 Now we move on to group-specific claims, where groups have distinct costs and potentially dis- tinct true decision boundaries, but we continue to use the constructions of f1 and g1 from Claim 1. Claim 2: Let fA 1 be the classifier based on boundary g1 for group A, and let fB 1 be the classifier based on boundary g1 for group B, as in (A.5), but with group-specific costs and true decision boundary parameters. Then @y P r0, 1sd, fA 1 pyq “ 1 ùñ f B 1 pyq “ 1. Proof. We first prove the claim for the case in which hA “ hB with decision bounday řd i“1 wixi “ τ. We then show that it also holds when the two are not equal. By the cost condition that cApyq ´ cApxq ď cBpyq ´ cBpxq for all x P r0, 1sd and y ě x, we know that for any given x, ΔBpxq Ď ΔApxq. Let kA P arg maxjPrds wj cA,j and kB P arg maxjPrds wj cB,j , so that gA 1 and gB 1 are defined as dÿ i“1 wiyi “ τ ` wkA cA,kA ðñ g A 1 : dÿ j“1 g A 1,jyj “ gA 1,0, dÿ i“1 wiyi “ τ ` wkB cB,kB ðñ g B 1 : dÿ j“1 gB 1,jyj “ g B 1,0. Then since for all i P rds, cA,i ď cB,i, we must have that τ ` wkA cA,kA ě τ ` wkB cA,kB ě τ ` wkB cB,kB , and thus gA 1,0 ě gB 1,0. Since fA 1 is the classifier based on gA 1 and fB 1 is based on gB 1 , we have that @y P 113 r0, 1sd, fA 1 pyq “ 1 ùñ f B 1 pyq “ 1. Now consider the case in which hA and hB differ. Recall the assumption hApxq “ 1 ùñ hBpxq “ 1 for all x P r0, 1sd. Thus for all x P r0, 1sd, ÿ i“1 wA,ixi ě τA ùñ ÿ i“1 wB,ixi ě τB. (A.6) Recall that the hyperplanes gA 1 , gB 1 are constructed as shifts of ři“1 wA,ixi ě τA and ř i“1 wB,ixi ě τB by the set of simplices tΔAp¯xAqu and tΔBp¯xBqu for ¯xA such that ř i“1 wA,i¯xA,i “ τA and ¯xB such that ř i“1 wB,i¯xB,i “ τB. Since ΔBpxq Ď ΔApxq, gA 1 and gB 1 support classifiers fA 1 and fB 1 such that f A 1 pyq “ 1 ùñ fB 1 pyq “ 1. Claim 3: All undominated classifiers commit no false negative errors on group A members and no false positive errors on group B members when candidates best respond. Proof. Fix a classifier f and consider a group A candidate with true feature vector ¯x who manipulates to best response ¯y such that hAp¯xq “ 1 but fp¯yq “ 0. Thus the classifier f makes a false negative error on this candidate. We show that we can construct another classifier ˆf that correctly classifies ¯x under its optimal manipulation with respect to ˆf. We prove that ˆf commits no more errors than does f and commits strictly fewer errors since it commits no false negatives on group A candidates. 114 Construct the classifier ˆf such that ˆfpyq “ $ ’’& ’’% 1 fpyq “ 1 or fA 1 pyq “ 1, 0 otherwise, (A.7) where fA 1 pyq is based on the boundary řj“1 gA 1,jyj “ gA 1,0. We first argue that f and ˆf make exactly the same set of false positive errors. Consider a potential false positive error that ˆf issues on a candidate with feature x from group A. Such a candidate cannot manipulate to a feature y to “trick” classifier fA 1 , since we have shown in Claim 1 that fA 1 perfectly classifies all group A candidates, and thus does not admit false positives. Thus any potential false positive error must be due to fpyq “ 1, in which case ˆf and f issue the same false positive error. Now we consider a potential false positive error that ˆf issues on a candidate with feature x from group B. By Claim 2, fA 1 pyq “ 1 ùñ fB 1 pyq “ 1, and thus we would have that the candidate with feature x was able to manipulate to some feature y such that fB 1 pyq “ 1. But this is a contradiction, since we know that fB 1 commits no false positives on group B members, and thus fA 1 pyq does not commit false positives on group B. Thus if ˆf commits a false positive, then it must be the case that f committed the same false positive. Consider a potential false negative error that ˆf issues on a candidate with feature x from group B. Then it must be the case that x can manipulate to some y such that both fpyq “ 0 and fA 1 pyq “ 0, and thus it be the case that f commits the same false negative. Lastly, consider a potential false negative error on a candidate from group A. By claim 1, this candidate must have been able to manipulate to some feature vector y such that fA 1 pyq “ 1, since fA 1 commits no errors on group A members. Thus when a candidate with unmanipulated feature x can manipulate to some y such that fA 1 pyq “ 1 yet can only present a (possibly different) feature y 115 such that fpyq “ 0, then ˆf correctly classifies this candidate positively, even when f does not. Thus ˆf makes no false negative errors on group B. Thus ˆf commits strictly fewer errors than f—none of which are false negatives on group A members— and f is dominated by ˆf. ✓ The second half of the claim can be proved through an analogous argument. Combining Claims 1 and 3, we conclude that we can construct perfect classifiers for group A that commit only false negative errors on group B and perfect classifiers for group B that commit only false positive errors on group A. fA 1 and fB 1 are examples of such classifiers, though they are not unique. Proof of Lemma 2 ùñ direction: Assume a group m candidate with feature x can move to y such that fpyq “ 1 and cmpyq ´ cmpxq ď 1, we show that necessarily x ě ℓ for some ℓ P Lmpgq. If x can move to y, then x P Δ´1pyq. By the definition of ℓmpyq, x ě ¯x for some ¯x P ℓmpyq. Then by monotonicity of g, we have that dÿ i“1 gixi ě dÿ i“1 gi¯xi ě min xPℓmpyq dÿ i“1 gixi Thus x ě ℓ for some ℓ P Lmpgq. ✓ ðù direction: Assume some group m candidate has feature x ě ℓ for some ℓ P Lmpgq. Then she can move to some y such that fpyq “ 1 and cmpyq ´ cmpxq ď 1. 116 If x ě ℓ for some ℓ P Lmpgq, then dÿ i“1 gixi ě dÿ i“1 giℓi, where ℓ P Δ´1 m pyq for some y such that ři“1 giyi “ g0 and fpyq “ 1. Since ℓ is defined as arg minxPℓmpyq ři“1 gixi, then we have dÿ i“1 giℓi “ dÿ i“1 ` giyi ´ max ti dÿ i“1 giti cm,i ˘, where ti ě 0 and ř i“1 ti “ 1 as shown before. Then substituting řd i“1 giyi “ g0, we have that ÿ i“1 giℓi ` gkm cm,km “ g0, where km P arg maxi“rds gi ci . Since x ě ℓ, x can also manipulate to some y with fpyq “ 1, bearing a cost ď 1. Proof of Proposition 4 If a learner publishes an undominated classifier f, then by Theorem 1, the hyperplane g : g⊺x “ g0 that supports this classifier can only commit inequality-reinforcing errors: only false positives on group A members and only false negatives on group B members. As proved in Lemma 2, the set Lmpgq determines the effective threshold on unmanipulated fea- tures x for a candidate of group m. We have already shown that for any two ℓ1, ℓ2 P Lmpgq, ÿ i“1 giℓ1,i “ ÿ i“1 giℓ2,i “ g0 ´ gkm ckm 117 where km P arg maxi“rds gi cm,i . For any ℓ P LBpgq, we have dÿ i“1 giℓi ` gkB cB,kB “ g0 Thus combining these results, those group B candidates with features x P r0, 1sd in the intersection g ⊺x ă g0 ´ gkB cB,kB č w⊺ Bx ě τB are classified as false negatives. For group A, we consider ℓ P LApgq: dÿ i“1 giℓi ` gkA cA,kA “ g0 and thus group A candidates with features x P r0, 1sd in the intersection w⊺ Ax ă τA č g⊺x ě g0 ´ gkA cA,kA are classified as false positives. Thus the cost publishing g is CFNPx„DB“x P ` g⊺x ă g0 ´ gkB ckB č w ⊺ Bx ě τB ˘‰ ` CFPPx„DA“x P `w ⊺ Ax ă τA č g⊺x ě g0 ´ gkA ckA ˘‰ 118 A.1.3 Proofs from Section 2.4 Reduction from the d-dimensional setting to the one-dimensional setting We first show that under certain conditions of a learner’s equilibrium classifier strategy, a d-dimensional subsidy analysis is equivalent to a one-dimensional subsidy analysis. In general d-dimensions, those features y attainable from an unmanipulated feature x P r0, 1sd, where fpxq “ 0, is given by y ď x ` dÿ i“1 ti ci ei where dÿ i“1 ti “ 1 where the right hand side gives the simplex Δpxq of potential manipulation. By Lemma 1, if a can- didate moves from x to y ‰ x, then she selects t such that tj “ 0 for all j R K “ arg maxi“rds gi ci . Staying within the simplex implies řd i“1 ti ď 1. Increasing the candidate’s available cost to expend from 1 to n increases her range of motion such that now she can move to any y ď x ` dÿ i“1 ti ci ei where dÿ i“1 ti “ n She continues to manipulate in the spirit of Lemma 1—optimal moves entail choices of t such that tj “ 0 for all j R K—however now, she is willing to manipulate if Di P rds such that fpx ` n ci eiq “ 1 and thus chooses t such that ři“1 ti ď n. Since offering a subsidy does not change the form of the group B cost function, a candidate from group B will pursue the same manipulation strategy given by the vector t under subsidy regimes as long as the classifier’s decision boundaries stay the same. By definition, all such choices of y resulting 119 from a manipulation via t have equivalent values g⊺y. When costs are subsidized through a flat α or a proportional β subsidy, a candidate with feature x can manipulate to any yα, yβ ě x that satisfies yα P rx, x ` dÿ i“1 ti ci eis where dÿ i“1 ti “ 1 ` α (A.8) yβ P rx, x ` dÿ i“1 ti ci eis where dÿ i“1 ti “ 1 β (A.9) We can pursue a dimensionality reduction by mapping each feature x P r0, 1sd to g⊺x P R`. Rather than considering an optimal manipulation in d-dimensions from x to y, we instead consider the relationship between the cost of the manipulation and the change from g⊺x to g⊺y: dÿ i“1 cipyi ´ xiq ðñ dÿ i“1 gipyi ´ xiq where gi gives the coefficients of the linear decision boundary that supports f, and x optimally ma- nipulates to y. We want to show that such a relationship is linear. Consider optimal manipulations: If a candidate chooses not to manipulate at all, she will incur a cost of 0 and will also move from řd i“1 gipyi ´ xiq “ 0. Since optimal manipulations (under any “budget” constraint) only are along kth components, a move from x to y always entails a total cost of dÿ iPK cipyi ´ xiq accompanied with dÿ iPK gipyi ´ xiq “ g⊺py ´ xq 120 Thus we can write her total cost c for a move from x to y as ck gk pg ⊺y ´ g⊺xq (A.10) for any k P K. Recall that by Lemma 1, optimal non-stationary manipulations move from x to y ą x such that řd i“1 giyi “ g0, so in these cases, we can also write the above as ck gk pg0 ´ g ⊺xq Thus we can consider candidates’ unmanipulated d-dimensional features x as one-dimensional fea- tures g⊺x and classifiers f based on d-dimensional hyperplanes g : řd i“1 gixi “ g0 as imposing one-dimensional thresholds g0. However a learner may also choose a different optimal subsidy strategy, thus publishing a clas- sifier that now admits candidates differently. Formally, suppose a learner first publishes a classifier f1 based on a decision boundary g1 : řd i“1 g1,ixi “ g1,0 to which a candidate’s optimal response follows the form given in Lemma 1 with k1 P arg maxiPrds g1,i ci . If a learner then chooses to change her strategy when implementing a subsidy, thus publishing a different classifier f2 based on decision boundary g2 : řd i“1 g2,ixi “ g2,0, a candidate’s optimal manipulation strategy will continue to adhere to Lemma 1, however, now, k2 P arg maxiPrds g2,i ci . Whereas the corresponding one- dimensional cost function cpyq ´ cpxq for best-response manipulations when facing classifier f1 was given by ck1 g1,k1 pg⊺ 1 py ´ xqq Her corresponding cost function when facing classifier f2 is ck2 g2,k2 pg ⊺ 2py ´ xqq 121 When these cost functions are the same, as when the coefficients g1,i “ g2,i for all i, the agent’s strategies when facing f1 and f2 are identical when reduced to one-dimension. This case arises, for example, when the learner continues to perfectly classify a single group in both the non-subsidy regime and the subsidy regime. In these cases, we can transition to considering just one-dimensional manipulations from g⊺y to g⊺x, where candidates bear linear costs of manipulation given in (A.10). Proof of Proposition 5 Working from the subsidy and no-subsidy comparisons given in Proposition 2, we show that all three parties would have preferred the outcomes of a non-manipulation world to those in both of the manipulation cases. To facilitate comparisons of welfare across classification regimes, we formalize group-wide utili- ties in the following definition. Definition 7 (Group welfare under a proportional subsidy). The average welfare of group B under classifier fprop and a proportional subsidy with parameter β is given by WBpfprop, βq “ ż R1 Px„DBpxqdx ` ż R2 `1 ´ βpcBpypxqq ´ cBpxqq ˘Px„DBpxqdx, WApfprop, 1q “ ż R1 Px„DApxqdx ` ż R2 `1 ´ pcApypxqq ´ cApxqq˘Px„DApxqdx, where ypxq is the best response of a candidate with unmanipulated feature x, R1 sums over those can- didates who are positively classified by fprop without expending any cost, and R2 sums over those can- didates who are positively classified after manipulating their features. Since group A members do not receive subsidy benefits, their welfare form is the same across no-subsidy and subsidy regimes. We use WApfpropq to denote WApfprop, 1q, the average welfare for group A under classifier fprop with 122 no subsidy. Definition 8 (Group welfare in a non-manipulation setting). The average welfare of group m under classifier f0 in a non-manipulation setting is given by Wmpf0q “ ż R Px„Dmpxqdx where R sums over candidates who are positively classified by f0. Proposition 10. There exist cost functions cA and cB satisfying the cost conditions, learner distribu- tions DA and DB, true classifiers with threshold τA and τB, population proportions pA and pB, and learner penalty parameters CFN, CFP, and λ, such that WApf ˚ propq ă WApf˚ 0 q, WBpf ˚ prop, β˚q ă WBpf ˚ 0 q, WApf˚ 1 q ă WApf ˚ 0 q, WBpf˚ 1 q ă WBpf˚ 0 q, Cpf˚ prop, β˚q ą Cpf˚ 0 q, Cpf˚ 1 q ą Cpf˚ 0 q where f0 is the equilibrium classifier in the non-manipulation regime, f˚ 1 is the equilibrium classifier in the manipulation regime, and (f˚ prop, β˚) is the equilibrium classifier in the subsidy regime. The average welfare of each group, Wmp¨q, as well as the learner, 1 ´ Cp¨q, is higher at the equilibrium of the non-manipulation game compared with the equilibria of the Strategic Classification Game with proportional subsidies and compared with the equilibrium of the Strategic Classification Game with no subsidies. Example 2. Now we consider a case in which candidates have linear cost functions cApxq “ 3x and cBpxq “ 4x. To show that diminished welfare for both candidate groups can occur without requiring distortions of probability distributions or cost functions, we consider a learner who seeks to avoid errors 123 on group B in both the subsidy and the non-subsidy regimes by penalizing false negatives twice as much as false positives, with CFN “ 2 3 , CFP “ 1 3 , and λ “ 3 4 . As in the previous example, we assume that the underlying unmanipulated features for both groups are uniformly distributed with pA “ pB “ 1 2 , and that τA “ 0.4 and τB “ 0.3. Now the equilibrium learner classifier without subsidies is based on threshold σ˚ 1 “ σB “ 0.55, which perfectly classifies all candidates from group B, while permitting false positives on candidates from group A with features x P r0.217, 0.4q. Under a proportional subsidy intervention, the learner’s equilibrium action is to choose threshold σ˚ prop “ σβ B « 0.552 and β˚ “ 0.994, which again perfectly classifies B candidates. Notice that now her optimal threshold commits fewer false positive errors on group A members, while still committing false positives on those members with features x P r0.219, 0.4q. Here, even when the learner has a cost penalty that is explicitly concerned with mistakenly exclud- ing group B candidates and then seeks to offer a subsidy benefit to further alleviate their costs, group B members are still no better off. They receive the same classifications as before and it can be shown that all candidates who manipulate must spend more to reach the higher threshold, even while accounting for the subsidy benefit! Some group A candidates are also worse off since the threshold has increased, and they receive no subsidy benefits. As before, only the learner gains from the intervention. Example 3. This example is based on Example 2. Now we consider the case a learner seeks σ˚ 1 P rσB, σAs where σA “ 0.733 and σB “ 0.55. Suppose she seeks to equalize the number of false posi- tives she commits on group A and the number of false negatives for group B and thus chooses σ˚ 1 “ 0.64 such that ℓApσ˚ 1 q “ 0.31 ℓBpσ˚ 1 q “ 0.39 Thus group B candidates with features x P r0.3, 0.39q are mistakenly excluded, and group A candi- 124 dates with features x P r0.31, 0.4q are mistakenly admitted. Upon implementing a subsidy and minimizing the same error penalty as in Example 1, the learner selects an optimal proportional β subsidy such that σ˚ prop “ σA “ 0.733; β “ 0.806 Under this regime, group B members are worse-off because many more candidates now receive false negative classifications x P r0.3, 0.423q Others who do secure positive classifications must pay more to do so. Candidates in group A are now perfectly classified, though this actually entails a welfare decline, since some candidates lose their false positive benefits. The learner is also strictly better off with a total penalty decline Cpσ˚ 0 q “ 0.183 Ñ Cpσ˚ prop, β˚q “ 0.128 Recall that the learner’s utility is given by 1 ´ Cp¨q. Thus we have that WApσ˚ prop, β˚q ă WApσ˚ 1 q WBpσ˚ prop, β˚q ă WBpσ˚ 1 q Cpσ˚ 1 q ą Cpσ˚ prop, β˚q Now consider a non-manipulation regime, in which the learner selects to equalize the number of false negatives for group B and the number of false positives for group A, she now chooses a threshold on un- 125 manipulated features σ˚ 0 “ 0.35 Some group A candidates lose false positive benefits in the manipulation regime, though on the whole, the group fares better off because all those candidates with features x P r0.39, 0.64q need not expend any costs in order to receive a positive classification. Group B candidates are strictly better off since they both receive fewer false negatives and need not pay to manipulate. The learner is also better off here because she reduces her error down to Cpτ˚q “ 0.1. Thus comparing the non- manipulation regime, the no-subsidy manipulation regime, and the subsidy regime, we have that util- ity comparisons for all three parties is given by WApσ˚ 0 q ą WApσ˚ 1 q ą WApσ˚ prop, β˚q WBpσ˚ 0 q ą WBpσ˚ 1 q ą WBpσ˚ prop, β˚q 1 ´ Cpσ˚ 0 q ą 1 ´ Cpσ˚ prop, β˚q ą 1 ´ Cpσ˚ 1 q A.1.4 Flat Subsidies Here we give analogous definitions and results for flat subsidies in which the learner absorbs up to a flat α amount from each group B candidate’s costs and show that qualitatively similar results hold. Definition 9 (Flat subsidy). Under a flat subsidy plan, the learner pays an α ą 0 benefit to all members of group B. As such, a group B candidate who manipulates from an initial score x to a final score y ě x bears a cost of maxt0, cBpyq ´ cBpxq ´ αu. 126 A learner’s strategy now consists of both a choice of α and a choice of classifier f to issue. The learner’s goal is to minimize her penalty CFP ÿ mPtA,Bu pmPx„Dmrhmpxq “ 0, fpyq “ 1s ` CFN ÿ mPtA,Bu pmPx„Dmrhmpxq “ 1, fpyq “ 0s ` λcostpf, αq, We can define ℓα Bpyq “ c´1 B ´ cBpyq ´ p1 ` αq¯ . Under the α subsidy, for an observed feature y, the group B candidate must have unmanipulated feature x ě ℓα Bpyq. From these functions, we define σα B and σβ B such that ℓα Bpσα Bq “ τB, and ℓβ Bpσβ Bq “ τB. Under a flat α subsidy, setting a threshold at σα B correctly classifies all group B members; under a proportional β subsidy, a threshold at σβ B correctly classifies all group B members. From this, we define σα B such that ℓα Bpσα Bq “ τB. Under a flat α subsidy, setting a threshold at σα B correctly classifies all group B members. In order to compute the cost of a subsidy plan, we must determine the number of group B can- didates who will take advantage of a given subsidy benefit. Since manipulation brings no benefit in itself, candidates will still only choose to manipulate and use the subsidy if it will lead to a positive classification. For the flat α subsidy, costpf, αq is given by ż σ c´1 B pcBpσq´αq rcBpσq ´ cBpxqsPDBpxqdx ` α ż c ´1 B pcBpσq´αq ℓα Bpσq PDBpxqdx, where σ is the threshold for classifier f. The first integral refers to the benefits paid out to candidates with manipulation costs less than the α amount offered. The latter refers to the total sum of full α payments offered to those with costs greater than α. Definition 10 (Group welfare under a flat subsidy). The average welfare of group B under classifier f 127 and a flat subsidy with parameter α is given by WBpf, αq “ ż R1 Px„DBpxqdx ` ż R2p1 ´ cBpypxq ´ cBpxqqqPx„DBpxqdx where ypxq is the best response of a candidate with unmanipulated feature x, R1 sums over those candi- dates who are positively classified without expending any cost, and R2 sums over those candidates who are positively classified after manipulating their features. Note that under the flat subsidy, group B costs have the form maxt0, cBpyq ´ cBpxq ´ αu The formulation of average group A welfare is the same in this setting and follows the same form given in Definition 5. Theorem 7 (Subsidies can harm both groups). There exist cost functions cA and cB satisfying the cost conditions, learner distributions DA and DB, true classifiers with threshold τA and τB, population proportions pA and pB, and learner penalty parameters CFN, CFP, and λ, such that WApf˚ propq ă WApf˚ 0 q, WBpf˚ prop, α˚q ă WBpf˚ 0 q, where f˚ prop and α˚ are the learner’s equilibrium classifier and subsidy choice in the Strategic Classifica- tion Game with flat subsidies and f˚ 0 is the learner’s equilibrium classifier in the Strategic Classifica- tion Game with no subsidies. 128 A.2 Appendix for Chapter 3 Proof of Proposition 1 We want to show that the firm-set reputation threshold ˆΠt1 “ pH ´ Δt1, where t1 is the time since the last wage update, enforces a worker strategy of effort exertion akin to that of the one-shot game, in which a worker exerts high effort if she can afford to do so and low effort otherwise. The firm, by setting its reputation threshold « pH, is correctly restricting its membership to workers who appear to be consistently exerting high effort. By the Law of Large Numbers, a worker’s recent time t1 individual reputation Πt1 i Ñ pH almost surely as t1 Ñ 8 as long as she continuously exerts high effort at each time step. Moreover, since the relationship between effort exertion and G or B outcomes can considered Bernoulli trials with p “ pH, we use the law of the iterated logarithm to bound individual good workers’ reputational deviations away from the theoretical mean pH as t increases and have that for all t “ τ, |Πτ ´ ˆΠτ| ď bτ´1p2 ˚ 0.25log logτq (A.11) Rubinstein and Yaari 81 have shown that, for a similar setup of imperfect observability and moral hazard in repeated interactions between insurers and clients, the enforceability of the insurers’ strategies is dependent on the choice of the forgiveness buffer sequence. In our case, as long as Δτ ą aτ´1p0.5log logτq and the sequence Δt1 Ñ 0 monotonically, the Rubinstein-Yaari result carries over into employment relationships, and workers will always exert high effort when they can afford to do so. Importantly, our scenario does differ from theirs in two ways: 1) Workers do not stay in the labor market for an infinite number of rounds, 2) A firm must pay the labor-market-wide wage upon hiring a worker and cannot unilaterally deviate from the set price. Since workers exit the market according to a Poisson parameter λ and the wage premium wt “ wpgt1q ą 0 is set to always 129 provide a higher payoff for a worker than failing to be hired at all (due to the normalization with respect to the unskilled job wage), the memoryless death process ensures that a worker i with quali- fications ρ will always find it within her interest to pursue the skilled job as long as it is individually rational for her to do so, i.e. eρpθiq ď wtppH ´ pρq. Proof of Theorem 3 The TLM hiring constraint effects two guarantees: 1) It retains the fundamental equality of groups’ ability level distributions Fpθq within the labor market; 2) It results in statistical parity in the proportion of workers offered skilled jobs in the TLM. Since the instantaneous time t contributions to groups’ full population societal reputations πμ are equivalent to gμ up to the same constant factor (ℓ proportion who enter the TLM), showing that the gμ values converge is sufficient to show that group reputations πμ do as well. Consider gt`1 “ ξpgtq as a self-mapping ξ : X Ñ X where X is the unit interval r0, 1s. Groups μ and ν have the same functional form of ξ differing only in a few particular parameters, which will be addressed in the decomposition of ξ into two separate functions. Assuming the two groups begin with unequal societal reputations, we suppose that (without loss of generality) πν ă πμ. We want to show that regardless of initial values πν 0 ă πμ 0, hiring outcomes will converge to achieve equal group outcomes system-wide under labor market dynamics with the TLM fairness constraint. Due to effect 1) of the TLM hiring constraint and the fact that both groups experience the same labor-market-wide wage wpgtq, the PLM ability thresholds pθQ and pθU are also equivalent across groups. Thus the difference between the gμ t and gν t arises due to the different corresponding pro- portions of qualified workers γν t ă γμ t at time t. As such, we construct the function φ as a mapping of γt P r0, 1s to gt`1 P r0, 1s, such that gt`1 “ φpγtq. The function φ is generic across the two groups, and group differences are entirely encoded in the distinct inputs γμ t and γν t . Let’s call g μ t`1 “ φpγμ t q and gν t`1 “ φpγν t q, where we treat γμ and γν as distinct points of the 130 mapping φ. Then, we have g μ t`1 “pHr1 ´ Fp pθQqγμ t ´ Fp pθUqp1 ´ γμ t qs ` pQFp pθQqγμ t (A.12) ` pUFp pθUqp1 ´ γμ t q The difference |g μ t`1 ´ gν t`1| is thus equivalent to the following |φpγμ t q´φpγν t q| “ | ´ pHFp pθQqpγμ t ´ γν t q ` pHFp pθUqpγμ t ´ γν t q ` pQFp pθQqpγμ t ´ γν t q ´ pUFp pθUqpγμ t ´ γν t q| “ pγμ t ´ γν t q|pHrFp pθUq ´ Fp pθQqs ` pQFp pθQq ´ pUFp pθUq| We rewrite the quantity inside the absolute value: | Fp pθUqrpH ´ pUslooooooomooooooon Pp0,1q ` Fp pθQqrpQ ´ pHslooooooomooooooon Pp´1,0q | “ |εt| ă 1 Together, |g μ t`1 ´ gν t`1| “ |φpγμ t q ´ φpγν t q| ď |εt|pγμ t ´ γν t q, @γμ t , γν t P r0, 1s, and with the bound on ε, φ is a contraction mapping. Since group reputation considers the proportion of all members in a group who are producing good outcomes, statistical parity also has the upshot that a particular instantaneous time t group reputation πμ exactly scales with gμ as each group is proportionally represented within the labor market according to its population-wide demographic share, so we need only consider g μ t values to determine the feedback loop property of collective reputation πμ t and group cost functions cμ and cν. Thus, the mapping ψ : X Ñ X, which maps normalized gt`1 P X “ r0, 1s to γt`1 P X “ r0, 1s such that γt`1 “ ψpgt`1q, is a weakly contracting map. We can now rewrite the recursive system gt`1 “ ξpgtq as a composition: gt`1 “ ξpgtq “ 131 φpψpgtqq, where we have shown that φ is a contraction and ψ is a short map. Then their composi- tion ξ, which represents the recursive self-map determining the evolution of group-wide employ- ment outcomes, is also a contraction map. Then by the Banach Fixed Point Theorem, there is a unique fixed-point ˜g “ ξp˜gq such that all initial points gi P r0, 1s converge to ˜g via a sequence of applications of the recursive relation ξ as in (A.12): For any two group reputations πμ and πν corresponding to initial points gμ 0 and gν 0, there exists a T such that @t ą T, πμ t “ πν t “ ˜π (similarly with gμ). At equilibrium, there is a unique wage ˜w corresponding to ˜g, and the system admits group fairness. Proof of Theorem 4 To show that the contraction and convergence assured by statistical parity hiring is not guaran- teed under group-blind hiring, note that when πB ă πW, necessarily 1 ´ Fp rθBq ă 1 ´ Fp ĂθWq, and the composition of workers granted entry into the TLM does not satisfy statistical parity. We call the proportion of workers in the TLM belonging to groups B and W, kB and 1 ´ kB respectively. Similarly to the proof of Theorem 3, we decompose g μ t into the feed-forward labor market flow ef- fect and the feedback natural reputational effect. However, since γμ for the two groups are the same, and Fp rθμq values differ, we instead write labor market flow as a function of Fp rθμq, call it φ*. Then gW t`1 “ φpFp rθQqq and gB t`1 “ φpFp rθBqq, and |φpFp ĂθQqq ´ φpFp rθBqq| “ pFp rθBq ´ Fp rθQqqγppH ´ pQq Since γppH ´ pQq ă 1, φ thus also contracts in the feed-forward mechanism, however the func- tion only captures the proportional g μ t dynamics from the TLM into the PLM, which does not scale with group reputation πμ since statistical parity is not guaranteed. Instead, under group- *Note that in this proof, we also assume that rθB ă rθU, but the proof carries through in the exact same manner when this is not true. 132 blind hiring, group reputation, which captures the proportion of all workers in the group who are producing good outcomes in the skilled labor, is a function of kB, or the bottleneck of group proportionality created by the group-blind investment threshold. Thus the particular time t nor- malized group societal reputation πB t 9 kBgB t σB ă gB t and πW t 9 p1´kBqgW t 1´σB ą gW t , and as a result, |πW t ´ πB t | ą |gW t`1 ´ gB t`1|. Since the mapping from g μ t Ñ πμ t is not a contraction, the reputation feedback is not guaranteed to contract either. The system may thus reach an asymmetric equilib- rium in which groups B and W maintain distinct investment costs and equal group reputations are never recovered. We now show that this asymmetric outcome is Pareto-dominated by the hiring constraint-produced symmetric steady-state when PLM firms’ demand for workers is not saturated and wprgtq “ ¯w. For the two groups, B and W, group-blind hiring imposes a single investment threshold ˜η such that hired workers in both labor markets have the same probability of being qualified regardless of group membership: γμ “ γν “ γ. Suppose group reputations are not equal as in the case of the group- blind asymmetric equilibrium just proven, then group-blind hiring results in effective ability thresh- olds that may be ranked with respect to the threshold ¯θ under statistical parity hiring. If πB ă πW, then ĂθW ă ¯θ ă rθB. Note that throughout the chapter, it is assumed that not all workers in the TLM are able to be hired in the PLM; therefore the ability threshold for exerting on-the-job effort is greater than the ability threshold resulting from the investment threshold under statistical parity- constrained hiring: pθQ ą ¯θ. When pθQ ă rθB, then TLM group-blind hiring leaves behind high ability workers in group B who would have otherwise been hired in the PLM. In particular, all qualified workers in group B with ability level θ P r pθQ, rθBq are only hired in the fairness constrained equilibrium; under group-blind hiring, they are barred from entering the TLM. This result accords with the vicious circle of the asymmetric equilibrium, since the reputation gap |πB t ´ πW t | and consequently, differences in group investment costs are maintained. 133 Further, since 1 ´ Fgp pθρq ă 1 ´ Ffp pθρq where Fg and Ff are the ability CDFs under the group- blind and fair regime respectively, in a labor market that demands more workers yet cannot sustain a higher wage (˜w “ w)†, firms strictly prefer the steady-state equilibrium under the fairness con- straint. This is because the effective higher ability threshold for group B under the group-blind TLM strategy is inefficient, leaving behind an untapped resource of skilled and qualified individ- uals in group B who would have otherwise been hired in the PLM. Even those workers in group W with ability level θ P r ĂθW, pθQq who are only allowed to enter the TLM in the group-blind regime do not fare better, since all such workers have ability level lower than the PLM reputation thresh- old and are not hired at equilibrium anyway. Thus since some workers in group B are strictly better off and workers in group W no worse off, the asymmetric equilibria under group-blind hiring is Pareto-dominated by the symmetric one of the fair case. The proof of this result for the statistical discriminatory hiring regime follows similarly. If ξW ą ξB, then PpQ|W, ηq ą PpQ|B, ηq, and the groups face different incentive compatibility constraints. Self-confirming asymmetric equilibria also exist under this regime, 22 and using the same argument about lost efficiency due to inequitable ability thresholds in the TLM for group B, these equilibria are also Pareto-dominated by hiring that abides by statistical parity. †There are a variety of reasons why an association of firms that demand more workers would be unable or unwilling to raise its wage higher ˜w “ w: A higher wage may encourage lower ability workers to apply and exert effort, and in reality, probabilities of success pH may be variable according to ability; thus the firm may want to a priori exclude such workers. Wage caps may also result from firm-firm collusion on price. 134 A.3 Appendix for Chapter 4 A.3.1 Dual derivations of the ε-fair SVM program In this Appendix section, we walk through the preliminary setup of the ε-fair SVM program given in Section 5.1 and present intermediate derivations omitted from the main text. Recall that the fair empirical risk minimization program of central focus is minimize θθθ, b 1 2 ∥θθθ∥2 ` C nÿ i“1 ξi subject to yipθθθ ⊺xi ` bq ´ 1 ` ξi ě 0, (ε-fair Soft-SVM) ξi ě 0, fθθθ,bpx, yq ď ε The hyperplane parameters are θθθ P Rd and b P R. The non-negative ξi allow the margin constraints to have some slack—this is why these variables are commonly called “slack variables.” In the Soft- Margin (as opposed to the Hard-Margin) SVM, the margin is permitted to be less than 1. A slack variable ξi ą 0 corresponds to a point xi having a functional margin of less than 1. There is a cost associated with this margin violation, even though it need not correspond to a classification error. C ą 0 is a hyperparameter tunable by the learner to optimize this trade-off between preferring a larger margin and penalizing violations of the margin. When we combine the general Soft-Margin SVM with the covariance parity constraint in (4.4) 135 proposed by Zafar et al. 96, we have the program minimize θθθ, b 1 2 ∥θθθ∥2 ` C nÿ i“1 ξi subject to yipθθθ ⊺xi ` bq ´ 1 ` ξ ě 0, (ε-fair-SVM1-P) | 1 n nÿ i“1pzi ´ ¯zqpθθθ ⊺xi ` bq| ď ε where ¯z reflects the bias in the demographic makeup of X : ¯z “ 1 n řn i“1 zi. The corresponding Lagrangian is LPpθθθ,b, ξξξ, λλλ, μμμ, γ1, γ2q “ 1 2 ∥θθθ∥2 ` C nÿ i“1 ξi ´ nÿ i“1 λi ´ nÿ i“1 μipyipθθθ⊺xi ` bq ´ 1 ` ξiq ´ γ1`ε ´ 1 n nÿ i“1pzi ´ ¯zqpθθθ ⊺xi ` bq˘ (ε-fair-SVM1-L) ´ γ2`ε ´ 1 n nÿ i“1p¯z ´ ziqpθθθ⊺xi ` bq ˘ where θθθ P Rd, b P R, ξξξ P Rn are Primal variables. The (non-negative) Lagrange multipliers λλλ, μμμ P Rn correspond to the n non-negativity constraints ξi ě 0 and the margin-slack constraints yipθθθ⊺xi ` bq ´ 1 ` ξi ě 0 respectively. The multiplier μi relays information about the functional margin of its corresponding point xi. If the margin is greater than 1 in the Primal, i.e., there is slack in the constraint), then by complementary slackness, μi “ 0. Otherwise, if the constraint holds with equality, μi P p0, Cs. When the classifier commits an error on xi, yipθθθ⊺xi ` bq ă 1, and then by the KKT conditions, μi “ C. The multipliers γ1, γ2 P R correspond to the two linearized forms of the absolute value fairness constraint. Notice that these two constraints cannot simultaneously hold with equality for ε ą 0. 136 Thus, by complementary slackness again, we know that at least one of γ1, γ2 is zero, and the other is strictly positive. By the Karush-Kuhn-Tucker conditions, at the solution of the convex program, the gradients of L with respect to θθθ, b, and ξi are zero: BL Bθθθ :“ 0 ñ θθθ “ nÿ i“1 μiyixi ´ γ n p nÿ i“1pzi ´ ¯zqxiq BL Bb :“ 0 ñ nÿ i“1 μiyi “ γ n nÿ i“1pzi ´ ¯zq “ 0 BL Bξi :“ 0 ñ λi ` μi “ C, i “ 1, . . . , n Plugging in these optimality conditions, the dual Lagrangian is LDpθθθ, ξξξ, λλλ, μμμ, γ1, γ2q “ ´ 1 2 ∥ nÿ i“1 μiyixi ´ γ n nÿ i“1pzi ´ ¯zqxi∥ 2 ` nÿ i“1 μi ´ |γ|ε where we have γ “ γ1 ´ γ2, since at most one side of the fairness constraint binds, thereby ensuring that at least one of γ1 or γ2 is 0. The dual maximizes this objective subject to the constraints μi P r0, Cs for all i and ř i“1 μiyi “ 0. Hence, we derive the full dual problem maximize μμμ, γ, V ´ 1 2 ∥ nÿ i“1 μiyixi ´ γ n nÿ i“1pzi ´ ¯zqxi∥2 ` nÿ i“1 μi ´ Vε subject to μi P r0, Cs, i “ 1, . . . , n, (ε-fair-SVM1-D) nÿ i“1 μiyi “ 0, γ P r´V, Vs where we have introduced the variable V to eliminate the absolute value function |γ| in the objec- tive. Notice that when γ “ 0 and neither of the constraints bind, we recover the standard dual 137 SVM program. Since we are concerned with fair learning that does in fact alter an optimal solution, we consider cases in which V is strictly positive. From this program, we introduce additional dual variables β´ and β`, corresponding to the γ P r´V, Vs constraint and derive the Lagrangian Lpμμμ, γ, V, β´, β`q “ ´ 1 2 ∥ nÿ i“1 μiyixi ´ γ n nÿ i“1pzi ´ ¯zqxi∥2 ` nÿ i“1 μi ´ Vε ` γpβ´ ´ β`q ` Vpβ´ ` β`q Under KKT conditions, β´ ` β` “ ε and γ˚ “ npnpβ´ ´ β`q ` řn i“1 μiyixxi, uyq ∥u∥2 (A.13) where u “ řn i“1pzi ´ ¯zqxi gives some group-sensitive geometric “average” of x P X . We can subsequently rewrite (ε-fair-SVM1-D) as maximize μμμ,β´,β` ´ 1 2 ∥ nÿ i“1 μiyipI ´ Puqxi∥ 2 ` nÿ i“1 μi ` 2n ř i μiyixxi, uy ` n2pβ´ ´ β`q 2∥u∥2 pβ´ ´ β`q subject to μi P r0, Cs, i “ 1, . . . , n, nÿ i“1 μiyi “ 0, (ε-fair SVM2-D) β´, β` ě 0, β´ ` β` “ ε where I, Pu P Rdˆd. The former is the identity matrix, and the latter is the projection matrix onto the vector u. As was also observed by Donini et al., the ε “ 0 version of (ε-fair SVM2-D) 138 is equivalent to the standard formulation of the dual SVM program with Kernel Kpxi, xjq “ xpI ´ Puqxi, pI ´ Puqxjy. 28 Since we are interested in the welfare impacts of fair learning when fairness constraints do have an impact on optimal solutions, we will assume that the fairness constraint binds. For clarity of exposition, we assume that the positive covariance constraint binds, and thus that β´ “ 0 and β` “ ε in (ε-fair SVM2-D). This is without loss of generalization—the same analyses apply when the negative covariance constraint binds. The dual ε-fair SVM program becomes minimize μμμ 1 2 ∥ nÿ i“1 μiyipI ´ Puqxi∥2 ´ nÿ i“1 μi ` nεp2 ři μiyixxi, uy ´ nεq 2∥u∥2 subject to μi P r0, Cs, i “ 1, . . . , n, (ε-fair SVM-D) nÿ i“1 μiyi “ 0 A.3.2 Algorithms Finding the next breakpoint when |Mε| “ 0 When |Mε| “ 0, the standard procedure that finds the next breakpoint by computing sensi- tivities to μi in the margin (i P Mε) by inverting the matrix K in (4.12) fails. Without rε i, we also cannot compute changes to di for i not in the margin (i P tF, Euε) as defined in (4.18) to track when points enter the margin. As a result, we need a special procedure to find the next breakpoint when the margin becomes empty. If the solution is to remain optimal, it must continue to abide by KKT conditions; in particular řn i“1 μiyi “ 0. Notice then that if the margin is empty, we have that ř iPE ε μiyi “ 0 “ C ř iPE ε yi, 139 which means that there are equal numbers of `1 and ´1 vectors that are misclassified. Thus at the next breakpoint, both `1 and ´1 vectors will enter the margin at the same time, offsetting each other exactly to retain the optimality of the solution. Tracking how vectors enter the margin at the solution ppεq requires tracking sign changes of BDε Bμ : nÿ i“1 μiyipI ´ PuqxiyjpI ´ Puqxj ` nεyjxxj, uy ∥u∥2 ` byj ´ 1 F ε ż E ε 0 We can perturb ε by Δε and narrow the range of eligible optimal b. Consider how the SVM bound- ary splits the dataset. On the positive side of the boundary, we have b ą yi´1 ´ nÿ i“1 μiyipI ´ PuqxiyjpI ´ Puqxj ´ nεyjxxj, uy ∥u∥2 ¯ for i with yi “ `1 and yi P F ε, as well as yi “ ´1 and yi P E ε. Call this set of indices R. On the other hand, b ă yi´1 ´ nÿ i“1 μiyipI ´ PuqxiyjpI ´ Puqxj ´ nεyjxxj, uy ∥u∥2 ¯ for i with yi “ ´1 and yi P F ε, as well as yi “ `1 and yi P E ε. Call this set of indices L. Let spεq “ 1 ´ nÿ i“1 μiyipI ´ PuqxiyjpI ´ Puqxj ´ nεyjxxj, uy ∥u∥2 .Then we have the range b P Bε “ rmax iPR yispεq, min iPL yispεqs (A.14) Perturbations of Δε result in changes of tpΔεq “ ´yi nΔεxxj, uy ∥u∥2 140 so we can write BεpΔεq “ rmax iPR yispεq ´ tpΔεq, min iPL yispεq ´ tpΔεqs (A.15) In increasing the magnitude of Δε, the interval BεpΔεq shrinks until it collapses onto a single value of b. The Δε be the perturbation when max iPR yispεq ´ tpΔεq “ min iPL yispεq ´ tpΔεq (A.16) determines the next breakpoint. The indices k “ arg max iPR yispεq ´ tpΔεq, ℓ “ arg min iPL yispεq ´ tpΔεq (A.17) leave their respective sets and enter the margin. The partition is updated as: M ε`Δε “ tk, ℓu (A.18) tF, Euε`Δε “ tF, Euε ´ tk, ℓu (A.19) A.3.3 Additional Figures Figure 2 gives more information on the welfare impacts of ε-fair SVM-solutions on the Adult dataset. Increasing ε from left to right loosens fairness constraint, and classification outcomes become “less fair.” Paths level off at ε « 0.175 when constraint ceases to bind at the optimal solution. The top panel shows that the learner objective value monotonically decreases as the fairness constraint loosens. The bottom panel gives the group-specific welfare change at an ε-fair SVM solution given as an absolute change in the number of positively labeled examples compared to the unconstrained solution baseline. 141 Figure A.1: Impact of fair SVM learning on learner objective value (top panel) and group welfare given as absolute wel‐ fare changes for female and male groups (bottom panel) on the Adult dataset. A.3.4 Results on the Correspondence between Loss Minimization and Social Welfare Maximization In the Planner’s Problem, a planner maximizes a social welfare functional (SWF) given as a weighted sum of individual utilities, W “ řn i“1 wiui. An individual i’s contribution to society’s total welfare is a product of her utility ui and her social weight wi P r0, 1s normalized so that řn i wi “ 1. Utility functions ui : X Ñ R` assign positive utilities to a set of attributes or goods xi. We suppose a utility function is everywhere continuous and differentiable with respect to its inputs. Since a planner who allocates a resource h impacts her recipients’ utilities, she solves hSWFpx; wwwq :“ arg maxhhh řn i“1 wiupxi, hiq under a budget constraint: řn i“1 hi ď B. Since we consider cases of social planning in which a desirable good is being allocated, it is natural to suppose that u is strictly mono- tone with respect to h. As is common in welfare economics, we take u to be concave in h, so that 142 receiving the good exhibits diminishing marginal returns. Further, we require that the social welfare functional W be symmetric: Wphhh; x, wwwq “ Wpσphhhq; σpxq, σpwwwqq for all possible permutations of σp¨q. This property implies that the utility functions in the Planner’s Problem are not individual- ized. In the case of binary classification, the planner decides whether to allocate the discrete good to individual i or not (hi P t0, 1u). To highlight the correspondence between the machine learning and welfare economic approaches to social allocation, we first show that we can understand loss minimizing solutions to also be wel- fare maximizing ones, albeit under a particular instantiation of the social welfare function. Since so- cial welfare is given as the weighted sum of individuals’ utilities, it is clear that manipulating weights www significantly alters the planner’s solution. Thus just as we can compute optimal allocations under a fixed set of welfare weights, we can also begin with an optimal allocation and find welfare weights that would support them. In welfare economics, the form of www corresponds to societal preferences about what constitutes a fair distribution. For example, the commonly-called “Rawlsian” social wel- fare function named after political philosopher John Rawls, can be written as WRawls “ mini ui where ui gives the utility of individual i. This function is equivalent to the general form řn i“1 wiui where the individual i with the lowest utility ui has welfare weight wi “ 1 and all individuals k ‰ i have weight wk “ 0. On the other hand, the commonly-called “Benthamite” social welfare function named after the founder of utilitarianism Jeremy Bentham, aggregates social welfare such that an extra unit of utility contributes equally to the social welfare regardless of who receives it. Benthamite weights are equal across all individuals: wi “ 1 n for all i P rns. Thus associating an optimal (possibly fairness constrained) loss minimizing allocation with a set of welfare weights that would make it socially optimal lends insight into how socially “fair” a classification is from a welfare economic perspective. The following Proposition formally states this correspondence between loss minimization and social welfare maximization. Proposition 11. For any vector of classifications hMLpxiq that solves a loss minimization task, there 143 exists a set of welfare weights www with řn i“1 wi “ 1 such that the planner who maximizes social welfare W with a budget B selects an optimal allocation hSWFpxiq “ hMLpxiq for all i P rns. Proof. First, we know that since Wpx, wwwq is a weighted sum of functions u, which are concave in h, the planner can indeed find a social welfare maximizing allocation hhhSWF. Let hMLpxq be the empiri- cal loss-minimizing classifier for txi, zi, yiun i“1. With these allocations given, we can invert the social welfare maximization problem to find the weights that www support them. For a given utility function u, we evaluate Bupx,hq Bh \f \f \f txi,hMLpxiqu “ mi @i P rns, which gives the marginal gain in utility for individual i from having received an infinitesimal additional allocation of h. Notice that at a welfare maximizing allocation hhh, we must have that wi Bupx, hq Bh \f \f \ftxi,hiu “ wj Bupx, hq Bh \f \f \f txj,hju for all i, j P rns (A.20) When the allocation hMLpxq has been fixed, we must have that wimi “ wjmj “ k, where the constant k is set by the planner’s budget B, for all i, j along with řn i“1 wi “ 1. Since u is strictly monotone with respect to h, mi ą 0 for all i. We thus have a non-degenerate system of n equations with n variables, and there exists a unique solution of welfare weights www that support the allocation. Note that in the case of binary classification hMLpxq P t´1, `1, u, so allocations are not awarded at a fractional level. Thus rather than the partial Bupx,hq Bh , the planner must consider the margin gain of receiving a positive classification. Nevertheless, Proposition 1 still holds, and the proof carries through with Δupx, hpxqq “ upx, 1q ´ upx, 0q in place of partial derivatives Bupx,hq Bh . The equations given in (A.20) set an optimality condition for the planner. Its structure, though simple, reveals that welfare weights must be inversely proportional to an individuals’ marginal utility gain from receiving an allocation. This result is formalized in the Proposition below. 144 Proposition 12. For any set of optimal allocations hhh “ arg maxhhh řn i“1 ¯wiupxi, hiq with strictly monotonic utility function u concave in h, the supporting welfare weights have the form ¯wi “ k mi where mi “ Bupxiq Bh |txi,hiu and k ą 0 is a constant set by the planner’s budget B “ řn i“1 hi. By associating a set of classification outcomes with a set of implied welfare weights, one can in- quire about the social fairness of the allocation scheme by investigating the distribution of welfare weights across individuals or across groups. While there may not be a single distribution of welfare weights that can be said to be “most fair,” theoretical and empirical work in economics has been conducted on the range of fair distributions of societal weights. 37,82 This research has considered weights as implied by current social policies, 1,99,21 philosophical notions of justice, 2,38 and individ- uals’ preferences in surveys and experiments. 1,65,82 They thus offer substantive notions of fairness currently uncaptured by many current algorithmic fairness approaches. An Algorithm that Records All Possible Labelings In the previous section, we showed that for any vector of classifications, one can compute the implied societal welfare weights of the generic SWF that would yield the same allocations in the Planner’s Problem. In this section, we work in the converse direction: Beginning with a planner’s social welfare maximization problem, does there exist a classifier hML P H that generates the same classification as the planner’s optimal allocation such that for all i P rns, hMLpxiq “ hSWFpxiq? We answer this question for the hypothesis class of linear decision boundary-based classifiers by providing an algorithm that accomplishes a much more general task: Given a set X , containing n d-dimensional nondegenerate data points x P Rd, our algorithm enumerates all linearly separable labelings and can output a hyperplane parameterized by θθθ P Rd and b P R that achieves that set 145 of labels. In order to build intuition for its construction, we first consider a hyperplane separation technique that applies to a very specific case: a case in which a hyperplane separates sets A and B, intersecting A at a single point and intersecting B at d ´ 1 points. Lemma 4. Consider linearly separable sets A and B of points x P Rd. For any d ´ 1-dimensional hyperplane hV with hV X A “ v and hV X B “ P where |P| “ d ´ 1 that separates A and B into closed halfspaces ¯h ` V and ¯h´ V , one can construct a d ´ 1-dimensional hyperplane h that separates A and B into open halfspaces h` and h´. Because its techniques are not of primary relevance for this Section, we defer the full proof of this Lemma to the Appendix but provide a brief exposition. The construction on which the Lemma relies is a “pivot-and-translate” maneuver. A hyperplane as described can separate points in open halfspaces by first pivoting (infinitesimally) on a d ´ 2-dimensional facet P of a convex hull CpBq away from v P CpAq and then translating (infinitesimally) back toward v and away from CpBq. We show that all separable convex sets can be separated by such a hyperplane and procedure. Note that since we seek enumerations of all labelings achievable by a linear separator on a given dataset, we are not a priori given convex hulls to separate. That is, we want to know which points can be made into distinct convex hulls and which cannot. Thus we take the preceding procedure and invert it—the central idea is to begin with the separators and from there, search for all possible convex hulls: Beginning with an arbitrary d ´ 1-dimensional hyperplane h defined by d data points, we construct convex hulls out of the points in each halfspace created by h. Then we can use the pivot-and-translate procedure to construct a separation of the two sets into two open halfspaces. We must show that such a procedure is indeed exhaustive. Theorem 8. Given a dataset X consisting of n nondegenerate points x P Rd, Algorithm 2 enumer- ates all possible labelings achievable by a d ´ 1-dimensional hyperplane in Opnddq time and outputs hyperplane parameters pθθθ, bq that achieve each one. 146 ALGORITHM 2: Record all possible labelings on a dataset X by linear separators Input: Set X of n data points x P Rd Output: All possible partitions A, B attainable via linear separators; supporting hyperplane h for all V Ă X with |V| “ d do Construct d ´ 1-dimensional hyperplane hV defined by v P V; for each point v P V do P “ Vzv; h “ pivotphV, P, vq ; // hV pivots around the d ´ 2-dimensional plane P away from v h “ translateph, vq ; // h translates toward v Record A “ tx|x P h `u, B “ tx|x P h ´u, h; end end Proof. We have already shown that the pivot-and-translate construction is sufficient to linearly sepa- rate two sets A and B in the very specific case given in the preceding Lemma. But we must prove that all linearly separable sets can be constructed via Algorithm 2. We prove it is exhaustive by contradic- tion. Suppose there exists a separation of X that is not captured by Algorithm 2. Then there exists disjoint sets A and B such that their convex hulls CpAq and CpBq do not intersect. By the hyperplane separating theorem, there exists a d ´ 1-dimensional hyperplane hV1 that separates A and B, defined by a set V1 of d vertices v, at least one of which is on the boundary of each convex hull. Without loss of generality, we assume that for all x P A, x P h ` V1 and for all x P B, x P h´ V1. Notice that this hyperplane is indeed “checked” by the Algorithm, and this hyperplane hV1 correctly separates x P X zV1 into the two sets A and B. Thus if the separation is not disclosed via the procedure, the omission must occur due to the pivot-and-translate procedure’s being incomplete. In Algorithm 2, the set V1 is partitioned so that V1 “ vf,1 Y P1 where vf,1 is the “free vertex” and P1 is the pivot set consisting of d ´ 1 vertices. This partition occurs d times so that each vertex v P V1 147 has its turn as the “free vertex.” Thus we can view the pivot-and-translate procedure as constituting a second partition—a partition of the d vertices that define the initial separating hyperplane. By contradiction, we claim that there exists a partition D1, E1 Ă V1 such that D1 š E1 “ V1 where D1 Ă A and E1 Ă B that is unaccounted for in the d pivot-and-translate operations applied to hV1. Thus |D1|, |E1| ě 2. We use a “gift-wrapping” argument, a technique common in algorithms that construct convex hulls, to show that the partition A and B is indeed covered by Algorithm 2. Select v P D1 to be the free vertex vf,1, and let the pivot set P1 “ V1zvf,1. We pivot around P1 and away from vf,1 so that vf,1 P h ` V1. Rotations in d-dimensions are precisely defined as being around d ´ 2-dimensional planes. Thus pivoting around the ridge P1 away from vf,1 is a well-defined rotation in Rd. Since hV1 is a supporting hyperplane to CpBq, E1 constitutes a |E1| ´ 1-dimensional facet of CpBq. There exists a vertex vE P CpBq such that E1 Y vE gives a |E1|-dimensional facet of CpBq. Let hV2 be defined by the set V2 “ P1 Y vE. hV2 continues to correctly separate all x P X zV2. We once again partition V2 into sets D2 and E2 whose members must be ultimately classified in sets A and B respectively. Notice that |D2| “ |D1| ´ 1, since hV2 correctly classifies vf,1 as belonging to set A. Thus with each iteration of the pivot procedure, the separating classifier unhinges from a vertex in CpAq and “wraps” around CpBq just as in the gift wrapping algorithm to attach onto another vertex in CpBq. At each step, the hyperplane defined by d vertices continues to support and separate CpAq and CpBq. Thus process iterates until in the |D1| ´ 1-th round, the hyperplane hV|D1|´1 has partition D|D1|´1 and E|D1|´1 with |D|D1|´1| “ 1. Applying the full pivot-and-translate procedure ensures the desired separation of sets A and B into open halfspaces. Thus starting from a separable hyperplane defined by d vertices on the convex hulls CpAq and CpBq, which must exist in virtue of the separability of sets A and B, we were able to use the pivot procedure in order to “gift-wrap” around one convex hull until we arrived at a d-dimensional sep- arating hyperplane with only one vertex vf P CpAq. This hyperplane is obviously checked by the first for-loop of Algorithm 2. The subsequent for-loop that performs the second partition of the d 148 vertices into the free vector vf and the pivot set P then directly applies and performs the pivot-and- translate procedure given in Algorithm 2 to achieve the desired separation. Degeneracies in the dataset can be handled by combining Algorithm 2 with standard solutions to degeneracy problems in geometric algorithms, which perform slight perturbations to degenerate data points to transform them into nondegenerate ones. 31 In concert with these solutions, Algo- rithm 2 automatically reveals which social welfare maximization solutions are attainable on a given dataset X via hyperplane-based classification and the 0 ´ 1 accuracy loss each entails. A.3.5 Proofs Proof of Proposition 7 Proof. For all j P F ε, remaining in F ε`Δε after the perturbation requires that BD Bμj ą 0 after the per- turbation. Let με i be the optimal μi solution at ppεq. Then following (4.10), we rewrite the quantity BD Bμj as gj “ 1 ´ ´ nÿ i“1 μ ε iyipI ´ PuqxiyjpI ´ Puqxj ` nεyjxxj, uy ∥u∥2 ` byj¯ ă 0 If djΔε ą 0, then j P F ε`Δε. Otherwise, for djΔε ă 0, if Δε ă gj dj , then BD Bμ ε`Δε j ą 0, and j P F ε`Δε after the perturbation. ✓ The same reasoning follows for j P E ε, except we have that gj ą 0. Thus if djΔε ă 0, then j P E ε`Δε. Otherwise, for djΔε ą 0, if Δε ă gj dj , then BD Bμ ε`Δε j ą 0, and j P E ε`Δε after the perturbation. ✓ To ensure that margin vectors do not escape the margin, we can directly look to rj “ Bμj Bε . Since for all j P Mε, με j P r0, Cs, then staying in the margin and set Mε`Δε depends on the sign of rj and 149 requires that rj ă 0 ÝÑ C ´ με j rj ă Δε ă ´με j rj (A.21) rj ą 0 ÝÑ ´με j rj ă Δε ă C ´ με j rj (A.22) Thus taking the minimum of the positive quantities gives an upper bound, while taking the maxi- mum of the negative quantities gives a lower bound on Δε perturbations, such that {F, M, Euε “ tF, M, Euε`Δε. Let mj “ $ ’’’’’’’’’’’’’’& ’’’’’’’’’’’’’’% $ ’’& ’’% gj dj , j P F, dj ą 0 ´8, j P F, dj ă 0 mint C´με j rj , ´με j rj u, j P M $ ’’& ’’% ´8, j P E, dj ą 0 gj dj , j P E, dj ă 0 , Mj “ $ ’’’’’’’’’’’’’’& ’’’’’’’’’’’’’’% $ ’’& ’’% 8, j P F, dj ą 0 gj dj , j P F, dj ă 0 mint C´με j rj , ´με j rj u, j P M $ ’’& ’’% gj dj , j P E, dj ą 0 8, j P E, dj ă 0 Thus all perturbations of ε within the range Δε P ` max j mj, min j Mj˘ satisfy the necessary conditions to ensure stable sets tF, M, Eu. Stable classifications ˆyi follow. Proof of Corollary 3 Proof. For all Δε in the stable region given in (4.16), Wipεq “ Wipε ` Δεq where i gives group membership z “ i. Thus the groups are welfare-wise indifferent between classifications at ε and Δε. For all Δε ă 0, where the fairness constraint is tightened,ppεq ď ppε ` Δεq. Since the learner prefers 150 lower loss, we have that ppεq ľ ppε ` Δεq. Comparing the triples at each ε value, we thus have tppεq, W0pεq, W1pεqu ľ tppε ` Δεq, W0pε ` Δεq, W1pε ` Δεqu as desired. Proof of Proposition 9 Proof. Following much of the exposition in the main text, recall we have that the perturbation func- tion in (4.21) is given as ppεq ě sup μμμ,γ tLpμμμ ˚, γ˚q ´ ε|γ˚|u which gives a global lower bound. Thus when a perturbation Δε ă 0 causes Lpμμμ˚, γ˚q ´ ε|γ˚| to increase, then ppε ` Δεq is guaranteed to increase by at least Δε|γ˚|. Thus when |γ˚| \" 0, ppε ` Δεq ´ ppεq \" 0. The learner experience a significant increase in her optimal value ppεq (which she wishes to minimize). On the other hand, when Δε ą 0, then Lpμμμ˚, γ˚q ´ ε|γ˚| decreases. But the decrease gives only the lower bound, and thus when |γ˚| is small, her optimal value ppεq decreases but it is guaranteed not to decrease by much. Proof of Proposition 8 Proof. Fix ε P p0, 1q and consider the stable region of Δε perturbations given by pbL, bUq. Suppose bL “ gj dj with j P E, then if yj “ ´1, ˆyj “ `1. Thus at the breakpoint Δε “ bL, j moves into Mε`bL and ˆyj “ `1 and uzjpε ` bLq ă uzjpεq where zj gives the group membership of xj. Since no other points transition, u¯zpε ` bLq “ u¯zpεq for all ¯z ‰ zj. Since bL ă 0, the fairness constraint is tightened and associated with a shadow price given by γ ą 0 such that ppε ` bLq ă ppεq. ✓ 151 Suppose bL “ C´με j rj and j P Mε with yj “ `1, then j moves into j P E ε`bL such that ˆyj “ ´1. Thus uzjpε ` bLq ă uzjpεq and u¯zpε ` bLq “ u¯zpεq where zj is the group membership of xj and ¯z ‰ zj, and ppε ` bLq ď ppεq. ✓ Suppose bU “ gj dj ą 0 where j P E ε, yj “ `1, and ˆyj “ ´1. At the breakpoint, j moves into Mε`bU such that yj “ ´1. Then uzjpε ` bUq ą uzjpεq where zj is the group membership of xj. For ¯z ‰ zj, u¯zpε ` bUq “ u¯zpεq, and since bU ą 0, the fairness constraint is loosened and ppε ` bUq ą ppεq. Suppose bU “ C´με j rj ą 0 where j P Mε and yj “ ´1. At the breakpoint, j moves into E ε`bU such that ˆyj “ `1. Then uzjpε ` bUq ą uzjpεq where zj gives the group membership of xj. For ¯z ‰ zj, u¯zpε ` bUq “ u¯zpεq, and since bU ą 0, the fairness constraint is loosened and ppε ` bUq ě ppεq. ✓ Proof of Theorem 5 Proof. Theorem 5 follows from Lemma 3.2, Proposition 7, Corollary 3, and Proposition 8. Proof of Lemma 4 from Appendix Section 6.4 Proof. Let A and B be a pair of disjoint non-empty convex sets that partition X Ă Rd: A š B “ X . Then by the hyperplane separation theorem, there exists a pair (θθθ, b) such that for all x P A, θθθ⊺x ě b—call this closed halfspace ¯h`—and for all x P B, θθθ⊺x ď b—call this closed halfspace ¯h´. One such hyperplane can be constructed to separate the convex hulls of A and B CpAq “ ␣ |A|ÿ i“1 αixi|xi P A, αi ě 0, |A|ÿ i“1 αi “ 1 ( CpBq “ ␣ |B|ÿ i“1 αixi|xi P B, αi ě 0, |B|ÿ i“1 αi “ 1 ( 152 Let hV be the d ´ 1-dimensional hyperplane defined by the set V with |V| “ d such that V X CpAq ‰ H and V X CpBq ‰ H. In order for the hyperplane to separate CpAq and CpBq, hV must also support each hull—we know that such a hyperplane always exists. In order to separate CpAq and CpBq so they are contained within open halfspaces h` V and h´ V , we wiggle the hyperplane so that it no longer passes through vertices v P V but still maintains convex hull separation. This “wiggle” step is the final step of separating A and B. Suppose V can be partitioned into a single vertex vA in CpAq and a set P “ tv|v P CpBqu with |P| “ d ´ 1. The set P defines a ridge on CpBq, since it is a d ´ 2-dimensional facet of CpBq. Rota- tions in d-dimensions are precisely defined as being around d ´ 2-dimensional planes. Thus pivoting hV around the ridge P away from vA is a well-defined rotation in Rd. Selecting any infinitesimally small rotation angle ρ will be enough to have CpAq P h` V . After the pivot, we translate hV away from the ridge P back toward vA. An infinitesimal translation is sufficient, since we simply wish to dislodge hV from the ridge P, so that CpBq P h´ V . 153 References [1] Ackert, L. F., Martinez-Vazquez, J., & Rider, M. (2007). Social preferences and tax policy design: some experimental evidence. Economic Inquiry, 45(3), 487–501. [2] Adler, M. (2012). Well-being and fair distribution: beyond cost-benefit analysis. Oxford University Press. [3] Agarwal, A., Beygelzimer, A., Dudík, M., Langford, J., & Wallach, H. (2018). A reductions approach to fair classification. arXiv preprint arXiv:1803.02453. [4] Akyol, E., Langbort, C., & Basar, T. (2016). Price of transparency in strategic machine learn- ing. CoRR arXiv:1610.08210. [5] Altonji, J. G. & Blank, R. M. (1999). Race and gender in the labor market. Handbook of Labor Economics, 3, 3143–3259. [6] Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). Machine bias. ProPublica, May, 23. [7] Antonovics, K. (2006). Statistical discrimination and intergenerational income mobility. Unpublished manuscript, University of California at San Diego. [8] Arrow, K. et al. (1973). The theory of discrimination. Discrimination in Labor Markets, 3(10), 3–33. [9] Auer, P. & Cesa-Bianchi, N. (1998). On-line learning with malicious noise and the closure algorithm. Annals of mathematics and artificial intelligence, 23(1-2), 83–99. [10] Ballwieser, W., Bamberg, G., Beckmann, M., Bester, H., Blickle, M., Ewert, R., Feichtinger, G., Firchau, V., Fricke, F., Funke, H., et al. (2012). Agency theory, information, and incen- tives. Springer Science & Business Media. [11] Bechavod, Y. & Ligett, K. (2017). Learning fair classifiers: A regularization-inspired ap- proach. arXiv preprint arXiv:1707.00044. [12] Becker, G. S. (1971). The economics of discrimination. University of Chicago Press. 154 [13] Binns, R. (2018). Fairness in machine learning: Lessons from political philosophy. In Con- ference on Fairness, Accountability and Transparency (pp. 149–159).: PMLR. [14] Bowles, S., Loury, G. C., & Sethi, R. (2014). Group inequality. Journal of the European Economic Association, 12(1), 129–152. [15] Brückner, M. & Scheffer, T. (2011). Stackelberg games for adversarial prediction problems. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. [16] Cain, G. G. (1986). The economic analysis of labor market discrimination: A survey. Hand- book of Labor Economics, 1, 693–785. [17] Calmon, F. P., Wei, D., Ramamurthy, K. N., & Varshney, K. R. (2017). Optimized data pre-processing for discrimination prevention. arXiv preprint arXiv:1704.03354. [18] Card, D. & Rothstein, J. (2007). Racial segregation and the black–white test score gap. Journal of Public Economics, 91(11–12), 2158–2184. [19] Chaudhuri, S. & Sethi, R. (2008). Statistical discrimination with peer effects: can integration eliminate negative stereotypes? The Review of Economic Studies, 75(2), 579–596. [20] Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidi- vism prediction instruments. Big data, 5(2), 153–163. [21] Christiansen, V. & Jansen, E. S. (1978). Implicit social preferences in the norwegian system of indirect taxation. Journal of Public Economics, 10(2), 217–245. [22] Coate, S. & Loury, G. C. (1993). Will affirmative-action policies eliminate negative stereo- types? The American Economic Review, (pp. 1220–1240). [23] Corbett-Davies, S. & Goel, S. (2018). The measure and mismeasure of fairness: A critical review of fair machine learning. arXiv preprint arXiv:1808.00023. [24] Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., & Huq, A. (2017). Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 797–806).: ACM. [25] Datta, A., Fredrikson, M., Ko, G., Mardziel, P., & Sen, S. (2017). Proxy non-discrimination in data-driven systems. CoRR arXiv:1707.08120. [26] Diehl, C. P. & Cauwenberghs, G. (2003). Svm incremental learning, adaptation and opti- mization. In Neural Networks, 2003. Proceedings of the International Joint Conference on, volume 4 (pp. 2685–2690).: IEEE. 155 [27] Dong, J., Roth, A., Schutzman, Z., Waggoner, B., & Wu, Z. S. (2018). Strategic classification from revealed preferences. In Proceedings of the ACM Conference on Economics and Compu- tation. [28] Donini, M., Oneto, L., Ben-David, S., Shawe-Taylor, J., & Pontil, M. (2018). Empirical risk minimization under fairness constraints. arXiv preprint arXiv:1802.08626. [29] Dutta, S., Wei, D., Yueksel, H., Chen, P.-Y., Liu, S., & Varshney, K. (2020). Is there a trade- off between fairness and accuracy? a perspective using mismatched hypothesis testing. In International Conference on Machine Learning (pp. 2803–2813).: PMLR. [30] Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012). Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (pp. 214–226).: ACM. [31] Edelsbrunner, H. & Mücke, E. P. (1990). Simulation of simplicity: a technique to cope with degenerate cases in geometric algorithms. ACM Transactions on Graphics (tog), 9(1), 66–104. [32] Ensign, D., Friedler, S. A., Neville, S., Scheidegger, C., & Venkatasubramanian, S. (2018). Runaway feedback loops in predictive policing. In Proceedings of the Conference on Fairness, Accountability and Transparency. [33] Esteban, J. & Ray, D. (2006). Inequality, lobbying, and resource allocation. American Economic Review, 96(1), 257–279. [34] Eubanks, V. (2018). Automating inequality: How High-tech Tools Profile, Police, and Punish the Poor. St. Martin’s Press. [35] Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., & Venkatasubramanian, S. (2015). Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining (pp. 259–268).: ACM. [36] Fish, B., Kun, J., & Lelkes, Á. D. (2016). A confidence-based approach for balancing fairness and accuracy. In Proceedings of the 2016 SIAM international conference on data mining (pp. 144–152).: SIAM. [37] Fleurbaey, M. & Maniquet, F. (2011). A theory of fairness and social welfare, volume 48. Cambridge University Press. [38] Fleurbaey, M., Maniquet, F., et al. (2015). Optimal taxation theory and principles of fairness. Technical report, Université catholique de Louvain, Center for Operations Research and ? [39] Frankel, A. & Kartik, N. (Forthcoming, 2018). Muddled information. Journal of Political Economy. 156 [40] Friedler, S. A., Scheidegger, C., & Venkatasubramanian, S. (2016). On the (im)possibility of fairness. arXiv preprint arXiv:1609.07236. [41] Fryer, R. G., Pager, D., & Spenkuch, J. L. (2013). Racial disparities in job finding and offered wages. The Journal of Law and Economics, 56(3), 633–689. [42] Gaebler, J., Cai, W., Basse, G., Shroff, R., Goel, S., & Hill, J. (2022). A causal framework for observational studies of discrimination. Statistics and Public Policy, (just-accepted), 1–61. [43] Grgic-Hlaca, N., Zafar, M. B., Gummadi, K. P., & Weller, A. (2018). Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning. In Proceedings of the AAAI Conference on Artificial Intelligence. [44] Hardt, M., Megiddo, N., Papadimitriou, C., & Wootters, M. (2016a). Strategic classification. In Proceedings of the ACM Conference on Innovations in Theoretical Computer Science. [45] Hardt, M., Price, E., Srebro, N., et al. (2016b). Equality of opportunity in supervised learn- ing. In Advances in Neural Information Processing Systems (pp. 3315–3323). [46] Hastie, T., Rosset, S., Tibshirani, R., & Zhu, J. (2004). The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5(Oct), 1391–1415. [47] Hedden, B. (2021). On statistical criteria of algorithmic fairness. Philosophy and Public Affairs, 49(2). [48] Heidari, H., Ferrari, C., Gummadi, K. P., & Krause, A. (2018). Fairness behind a veil of igno- rance: A welfare analysis for automated decision making. arXiv preprint arXiv:1806.04959. [49] Hellman, D. (2020). Measuring algorithmic fairness. Virginia Law Review, 106(4), 811– 866. [50] Hu, L. & Kohler-Hausmann, I. (2022). Of misdefined causal questions: The case of race and multi-stage outcomes. Unpublished manuscript. [51] Johnson, K. D., Foster, D. P., & Stine, R. A. (2016). Impartial predictive modeling: Ensuring fairness in arbitrary models. CoRR arXiv:1608.00528. [52] Joseph, M., Kearns, M., Morgenstern, J. H., & Roth, A. (2016). Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems (pp. 325– 333). [53] Kamishima, T., Akaho, S., & Sakuma, J. (2011). Fairness-aware learning through regular- ization approach. In Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on (pp. 643–650).: IEEE. 157 [54] Karasuyama, M., Harada, N., Sugiyama, M., & Takeuchi, I. (2012). Multi-parametric solution-path algorithm for instance-weighted support vector machines. Machine learn- ing, 88(3), 297–330. [55] Kearns, M. & Li, M. (1993). Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4), 807–837. [56] Kearns, M., Neel, S., Roth, A., & Wu, Z. S. (2018). Preventing fairness gerrymandering: Au- diting and learning for subgroup fairness. In International Conference on Machine Learning (pp. 2564–2572).: PMLR. [57] Kearns, M., Roth, A., & Wu, Z. S. (2017). Meritocratic fairness for cross-population selec- tion. In International Conference on Machine Learning (pp. 1828–1836). [58] Kephart, A. & Conitzer, V. (2015). Complexity of mechanism design with signaling costs. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems. [59] Kephart, A. & Conitzer, V. (2016). The revelation principle for mechanism design with reporting costs. In Proceedings of the ACM Conference on Economics and Computation. [60] Kilbertus, N., Carulla, M. R., Parascandolo, G., Hardt, M., Janzing, D., & Schölkopf, B. (2017). Avoiding discrimination through causal reasoning. In Advances in Neural Informa- tion Processing Systems. [61] Kleinberg, J., Mullainathan, S., & Raghavan, M. (2017). Inherent trade-offs in the fair deter- mination of risk scores. In Proceedings of the 8th Innovations in Theoretical Computer Science Conference (pp. 43:1–43:23).: ACM. [62] Kleinberg, J. & Raghavan, M. (2018). How do classifiers induce agents to invest effort strate- gically? CoRR arXiv:1807.05307. [63] Knox, D., Lowe, W., & Mummolo, J. (2020). Administrative records mask racially biased policing. American Political Science Review, 114(3), 619–637. [64] Kusner, M. J., Loftus, J., Russell, C., & Silva, R. (2017). Counterfactual fairness. In Ad- vances in Neural Information Processing Systems (pp. 4066–4076). [65] Kuziemko, I., Norton, M. I., Saez, E., & Stantcheva, S. (2015). How elastic are preferences for redistribution? evidence from randomized survey experiments. American Economic Review, 105(4), 1478–1508. [66] Laffont, J.-J. & Martimort, D. (2009). The Theory of Incentives: The Principal-Agent Model. Princeton University Press. [67] Levin, J. et al. (2009). The dynamics of collective reputation. The BE Journal of Theoretical Economics, 9(1), 1–25. 158 [68] Liu, L., Dean, S., Rolf, E., Simchowitz, M., & Hardt, M. (2018). Delayed impact of fair machine learning. In International Conference on Machine Learning (pp. 3156–3164). [69] Long, R. (2021). Fairness in machine learning: against false positive rate equality as a mea- sure of fairness. Journal of Moral Philosophy, 1(aop), 1–30. [70] Loury, G. C. & Kim, Y.-C. (2014). Collective reputation and the dynamics of statistical discrimination. [71] Loury, G. C. & Loury, G. C. (2009). The anatomy of racial inequality. Harvard University Press. [72] Mayson, S. G. (2018). Bias in, bias out. Yale lJ, 128, 2218. [73] Milli, S., Miller, J., Dragan, A. D., & Hardt, M. (Forthcoming, 2019). The social cost of strategic classification. [74] Mitchell, S., Potash, E., Barocas, S., D’Amour, A., & Lum, K. (2021). Algorithmic fairness: Choices, assumptions, and definitions. Annual Review of Statistics and Its Application, 8, 141–163. [75] Mullainathan, S. (2018). Algorithmic fairness and the social welfare function. In Proceedings of the 2018 ACM Conference on Economics and Computation (pp. 1–1).: ACM. [76] Neal, D. A. & Johnson, W. R. (1996). The role of premarket factors in black-white wage differences. Journal of Political Economy, 104(5), 869–895. [77] O’Neil, C. (2016). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books. [78] Phelps, E. S. (1972). The statistical theory of racism and sexism. The American Economic Review, 62(4), 659–661. [79] Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., & Weinberger, K. Q. (2017). On fairness and calibration. In Advances in Neural Information Processing Systems (pp. 5680–5689). [80] Qureshi, B., Kamiran, F., Karim, A., & Ruggieri, S. (2016). Causal discrimination discovery through propensity score analysis. CoRR arXiv:1608.03735. [81] Rubinstein, A. & Yaari, M. E. (1983). Repeated insurance contracts and moral hazard. Jour- nal of Economic Theory, 30(1), 74–97. [82] Saez, E. & Stantcheva, S. (2016). Generalized social marginal welfare weights for optimal tax theory. American Economic Review, 106(1), 24–45. 159 [83] Sen, A. (1980). Equality of What? Cambridge University Press: Cambridge. Reprinted in John Rawls et al., Liberty, Equality and Law (Cambridge: Cambridge University Press, 1987). [84] Spence, M. (1973). Job market signaling. The Quarterly Journal of Economics, 87(3), 355– 374. [85] Spence, M. (1978). Job market signaling. In Uncertainty in Economics (pp. 281–306). [86] Spremann, K. (1987). Agent and principal. In Agency theory, information, and incentives (pp. 3–37). Springer. [87] Sweeney, L. (2013). Discrimination in online ad delivery. Queue, 11(3), 10. [88] Tirole, J. (1996). A theory of collective reputations (with applications to the persistence of corruption and to firm quality). The Review of Economic Studies, 63(1), 1–22. [89] Wang, G., Yeung, D.-Y., & Lochovsky, F. H. (2007). A kernel path algorithm for support vector machines. In Proceedings of the 24th international conference on Machine learning (pp. 951–958).: ACM. [90] Wang, L., Zhu, J., & Zou, H. (2006). The doubly regularized support vector machine. Statis- tica Sinica, 16(2), 589. [91] Wick, M., Tristan, J.-B., et al. (2019). Unlocking fairness: a trade-off revisited. Advances in neural information processing systems, 32. [92] Winfree, J. A. & McCluskey, J. J. (2005). Collective reputation and quality. American Journal of Agricultural Economics, 87(1), 206–213. [93] Woodworth, B., Gunasekar, S., Ohannessian, M. I., & Srebro, N. (2017). Learning non- discriminatory predictors. arXiv preprint arXiv:1702.06081. [94] Yang, C. S. & Dobbie, W. (2020). Equal protection under algorithms: A new statistical and legal framework. Michigan Law Review, 119(2), 291–395. [95] Zafar, M. B., Valera, I., Gomez Rodriguez, M., & Gummadi, K. P. (2017). Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreat- ment. In Proceedings of the 26th International Conference on World Wide Web (pp. 1171– 1180).: International World Wide Web Conferences Steering Committee. [96] Zafar, M. B., Valera, I., Rodriguez, M. G., & Gummadi, K. P. (2015). Fairness constraints: Mechanisms for fair classification. arXiv preprint arXiv:1507.05259. [97] Zemel, R., Wu, Y., Swersky, K., Pitassi, T., & Dwork, C. (2013). Learning fair representa- tions. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) (pp. 325–333). 160 [98] Zhao, Q., Keele, L. J., Small, D. S., & Joffe, M. M. (2022). A note on posttreatment selection in studying racial discrimination in policing. American Political Science Review, 116(1), 337–350. [99] Zoutman, F. T., Jacobs, B., & Jongen, E. L. (2013). Optimal redistributive taxes and redis- tributive preferences in the netherlands. Erasmus University Rotterdam. 161 T his thesis was typeset using LATEX, originally developed by Leslie Lamport and based on Donald Knuth’s TEX. The body text is set in 11 point Egenolff-Berner Garamond, a revival of Claude Garamont’s humanist typeface. The above illustration, “Sci- ence Experiment 02”, was created by Ben Schlit- ter and released under cc by-nc-nd 3.0. A template that can be used to format a PhD the- sis with this look and feel has been released un- der the permissive mit (x11) license, and can be found online at github.com/suchow/Dissertate or from its author, Jordan Suchow, at su- chow@post.harvard.edu. 162 ProQuest Number: INFORMATION TO ALL USERS The quality and completeness of this reproduction is dependent on the quality and completeness of the copy made available to ProQuest. Distributed by ProQuest LLC ( ). Copyright of the Dissertation is held by the Author unless otherwise noted. This work may be used in accordance with the terms of the Creative Commons license or other rights statement, as indicated in the copyright statement or in the metadata associated with this work. Unless otherwise specified in the copyright statement or the metadata, all rights are reserved by the copyright holder. This work is protected against unauthorized copying under Title 17, United States Code and other applicable copyright laws. Microform Edition where available © ProQuest LLC. No reproduction or digitization of the Microform Edition is authorized without permission of ProQuest LLC. ProQuest LLC 789 East Eisenhower Parkway P.O. Box 1346 Ann Arbor, MI 48106 - 1346 USA 29206785 2022","libVersion":"0.0.0","langs":"","hash":"","size":0}