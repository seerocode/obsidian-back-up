{"path":".obsidian/plugins/text-extractor/cache/ka/zoterolibrary-files-3618-katzenbach-2021-ai-will-fix-this-the-technical-discursive-and-political-turn-to-ai-in-pdf-e5359ad1842852ea78362979a384d97c.json","text":"“AI will ﬁx this”– The Technical, Discursive, and Political Turn to AI in Governing Communication Christian Katzenbach1,2 Abstract Technologies of “artiﬁcial intelligence” (AI) and machine learning (ML) are increasingly presented as solutions to key pro- blems of our societies. Companies are developing, investing in, and deploying machine learning applications at scale in order to ﬁlter and organize content, mediate transactions, and make sense of massive sets of data. At the same time, social and legal expectations are ambiguous, and the technical challenges are substantial. This is the introductory article to a special theme that addresses this turn to AI as a technical, discursive and political phenomena. The opening article contextualizes this theme by unfolding this multi-layered nature of the turn to AI. It argues that, whereas public and economic discourses position the widespread deployment of AI and automation in the governance of digital communication as a technical turn with a narrative of revolutionary breakthrough-moments and of technological progress, this development is at least similarly dependent on a parallel discursive and political turn to AI. The article positions the current turn to AI in the longstanding motif of the “technological ﬁx” in the rela- tionship between technology and society, and identiﬁes a discursive turn to responsibility in platform governance as a key driver for AI and automation. In addition, a political turn to more demanding liability rules for platforms further incen- tivizes platforms to automatically screen their content for possibly infringing or violating content, and position AI as a solution to complex social problems. This article is a part of special theme on The Turn to AI. To see a full list of all articles in this special theme, please click here: https://journals.sagepub.com/page/bds/collections/theturntoai Introduction When Facebook CEO Marc Zuckerberg was pressed in the 2018 Senate Hearing upon issues of misinformation, hate speech and privacy, he was eager to present a solution: “AI will ﬁx this!” (Katzenbach, 2019). In this hearing, sena- tors asked Zuckerberg not only about what had happened in previous years, but also demanded to hear about the com- pany’s plans to respond adequately and responsibly in the future to the challenges posed by disinformation cam- paigns, the spread of hate speech, terrorist propaganda and other problematic content. In different phrases, Zuckerberg repeatedly referred to the development and increasing use of AI-powered systems to detect hate speech, terrorism and misinformation: “In the future, we’re going to have tools that are going to be able to iden- tify more types of bad content.”1 He reassured the senators that future systems will also cope much better with the dif- ﬁcult contextual and nuanced classiﬁcation of language: “Over a 5 to 10-year period, we will have A.I. tools that can get into some of the nuances – the linguistic nuances of different types of content to be more accurate in ﬂagging things for our systems. But, today, we’re just not there on that.” Whatever the challenge, Zuckerberg positioned new technology as the answer to complex social challenges. Zuckerberg is not alone with this resort to technology. Technologies of “artiﬁcial intelligence” (AI) and machine learning (ML) are regularly presented as solutions to key 1Centre for Media, Communication and Information Research (ZeMKI), University of Bremen 2Alexander von Humboldt Institute for Internet and Society (HIIG), Berlin Corresponding author: Christian Katzenbach, Centre for Media, Communication and Information Research (ZeMKI), University of Bremen, Linzer Str. 4-6, 28359 Bremen, Germany Email: katzenbach@uni-bremen.de Creative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 4.0 License (https:// creativecommons.org/licenses/by/4.0/) which permits any use, reproduction and distribution of the work without further permission provided the original work is attributed as speciﬁed on the SAGE and Open Access page (https://us.sagepub.com/en-us/nam/open-access-at-sage). Editorials Big Data & Society July–December: 1–8 © The Author(s) 2021 Article reuse guidelines: sagepub.com/journals-permissions DOI: 10.1177/20539517211046182 journals.sagepub.com/home/bds problems of our societies. Companies are developing, investing in, and deploying machine learning applications at scale in order to ﬁlter and organize content, mediate transactions, and make sense of massive sets of data. At the same time, social and legal expectations are ambiguous, and the technical challenges are substantial. In particular, addressing issues such as misinformation and hate speech with AI technologies evokes particular problems and harms, since the contextual nature of these types of content limits the accuracy of traditional algorithmic systems and thus augments harms such as overblocking. Machine learning technologies might indeed better ﬁt with fuzzy distinctions between legitimate and illegitimate content, yet observation of the rapid deployment of AI tech- nologies in other sectors suggests that problems of equality and diversity, discrimination and bias are often rather amp- liﬁed by automated technologies than diminished. Thus, it is of utter importance to integrate social and legal expertise into the debate about how technologies reorder communica- tion and society – and how (and if) they should be applied to thorny social problems at scale. In this context, this special theme for Big Data & Society investigates the rapid turn to AI in ordering communication online. The resulting set of research articles and commen- taries draws on perspectives from multiple disciplines ranging from media and communications to law and com- puter science. This introductory article contextualizes this theme by identifying the multi-layered nature of this turn. Whereas public and economic discourses position this development as a technical turn to AI with a narrative of revolutionary breakthrough-moments and of technological progress (Bareis and Katzenbach, 2021), the widespread deployment and normalization of AI and automation in the governance of digital communication is at least simi- larly dependent on a parallel discursive and political turn to AI. This opening article develops this argument by ﬁrstly positioning the current turn to AI in the longstanding motif of the “technological ﬁx” in the relationship between technology and society. The piece will, secondly recon- struct the discursive turn to AI, and, thirdly, identify a pol- itical turn to AI in governing communication, before introducing the articles of this special theme. The technological Fix – positioning technologies as solutions to social problems With his reaction at the Senate hearing 2018, Facebook CEO Marc Zuckerberg resorted to a regular motif in the relationship between technology and society, which actors from business and technology powerfully introduce into discourses time and again: the “technological ﬁx”. This motif positions tech- nology as a necessary and functional solution to social pro- blems and challenges. Rudi Volti pointed out the ubiquity of this motif in the 1990s: “The list of technologies that have been or could be applied to the alleviation of social pro- blems is an extensive one, and examples could be supplied almost indeﬁnitely. What they have in common is that they are ’technological ﬁxes’, for they seek to use the power of technology in order to solve problems that are nontechnical in nature.” (Volti, 2014, p. 30) Blind faith in the effect of tech- nology is usually combined with ignorance of the causes and dynamics of existing social problems: “In this view, trafﬁc management systems cope with the increasing number of cars in cities (and car trafﬁc is not reduced), food imports keep the poorest from starving (and the causes are not com- bated), cattle are culled (and industrial factory farming is not adopted)” (Degele, 2002, p. 25, translation by the author). In the context of digitalization, Zuckerberg is by no means the only one to carry on this narrative – rather, it is a central motif of the basic narrative traits of the US-dominated digital industry (Daub, 2020). Evgeny Morozov, for example, describes how Silicon Valley companies treat social pro- blems or complex contexts such as health and mobility as problems that can be solved functionally. By providing ever new services and apps, they promise to optimize processes and social interactions. What Volti calls “techno- logical ﬁx”, Morozov calls “solutionism”: “Recasting all complex social situations either as neat problems with deﬁn- ite, computable solutions or as transparent and self-evident processes that can be easily optimized – if only the right algorithms are in place!” (Morozov, 2013, p. 5). The current turn towards addressing complex questions of shaping and ordering public communication through algo- rithms and artiﬁcial intelligence is thus by no means unique, but is part of a long-lasting motif in the relationship between society and technology. This ﬁnding is important for the classiﬁcation of the current algorithmic turn, as it already indicates that this turn cannot be explained by tech- nical progress alone. Rather, it is a recurring motif that gains or loses importance depending on the social problem situation. The impetus for the prominent positioning of a technological ﬁx as a powerful pattern of interpretation and explanation can be triggered by technical impulses, but it can carry nor explain such a development alone. So while there are clearly techno- logical advancements in identifying and classifying content (Gorwa et al., 2020; Cardon et al., 2018), there is more to this turn to AI than technological progress. In the following, we thus reconstruct a discursive, and then a political develop- ment that both foreground the role of AI in improving society and governing communication online. The discursive turn to Ai – answering a turn to responsibility with the technological ﬁx On the discursive level, a conjunction of two developments enables the rise of AI and automation as a seemingly 2 Big Data & Society adequate solution to a complex social problem. The ﬁrst is the massive general increase of attention and visibility for AI in public discourse since the mid-2010s. Existing studies of media reporting identify that new products and supposed innovations clearly dominate the coverage, with business actors and tech entrepreneurs featuring much more often in AI reporting than other stakeholders (Brennen, Howard, and Nielsen, 2018; Chuan, Tsai, and Cho, 2019; Fast and Horvitz, 2017; Puschmann and Fischer 2021). This reporting style builds on longstanding narratives that attribute magical properties to technologies, and speciﬁcally to AI (Bory, 2019; Cave and Dihal, 2019). Even governmental strategies and communication adopt this narrative by positioning AI as an inevitable and mas- sively disrupting technological development with high eco- nomic opportunities (Bareis and Katzenbach, 2021; Zeng, Chan and Schäfer, 2020). This “enchanted determinism” in the general AI discourse (Campolo and Crawford, 2020) constitutes a particularly strong expression of the technological ﬁx motif. This general AI discourse functions as a sounding board for the second discursive development relevant here, and that is the remarkable shift in platform governance dis- course since 2015 −16: for a few years now, public and pol- itical actors have been increasingly demanding that platforms take responsibility for the content and communi- cation dynamics on their services (responsibility turn). As a consequence, platform operators are responding to this growing pressure with the promise of a technical solution. In this context, the general AI discourse greatly eases social media platforms’ rhetorical work to position AI as a technological ﬁx solution to their own problems. Platform companies such as Facebook, Twitter and Google, but also Uber and AirBnB, had long been very suc- cessful in positioning themselves as neutral intermediaries (Gillespie, 2010). The absolute enabling of free expression and the “information-wants-to-be-free” mantra had consti- tuted the central guiding principles of Silicon Valley com- panies (Vaidhyanathan, 2012). Whether it was search results, news feeds or simply providing the opportunity to express oneself publicly: Until well into the 2010s, platform companies portrayed their services as value-free offerings and positioned themselves as tech companies, not media organizations (Napoli and Caplan, 2017). The algorithms used to sort and prioritize content would deliver objective and thus neutral results (Ames, 2018). This “spiritual defer- ral to algorithmic neutrality” (Morozov, 2011) was expressed, for example, in Google’s systematic refusal to take responsibility for search result lists and to change them manually, even if they showed racist or discriminatory content as the top search results (Gillespie, 2014: 180-181; Noble, 2018). Until 2015, Twitter made it clear in the very ﬁrst words of its “Twitter Rules”, which are both a self- portrayal and a set of rules, that users are responsible for their own content and that Twitter remains neutral as a provider: “We respect the ownership of the content that users share and each user is responsible for the content he or she provides. Because of these principles, we do not actively monitor and will not censor user content”. 2 This positioning as neutral intermediaries was highly attractive for platform providers, as they could thus establish them- selves as a self-evident and soon seemingly indispensable element of everyday communication, but at the same time could neither be held socially responsible nor legally liable for the content circulating across their services. \"They do so strategically, to position themselves both to pursue current and future proﬁts, to strike a regulatory sweet spot between legislative protections that beneﬁt them and obligations that do not, and to lay out a cultural imaginary within which their service makes sense\" (Gillespie, 2010: 348). This positioning was a major factor in the rapid growth of platforms into central institu- tions of social communication. However, at the latest since the 2016 US elections, and the increasing political and social conﬂicts on migration issues since 2015, a turn to responsibility in the debate on platforms can be observed. In these years, controversies about the role and adequate regulation of platforms have increased signiﬁcantly (Katzenbach, 2021). In particular, the intense debates around misinformation or “fake news” (Righetti, 2021) and hate speech (Tworek, 2021) have placed questions of the power and responsibility of plat- forms high on public and political agendas. Providers are now perceived in the discourse ﬁrst and foremost as active actors who organize their services in speciﬁc ways, pursuing their own interests (quantitative optimization on interactions, monetization through advertising) as well as generating external effects (reinforcing dynamics of misin- formation, hate comments). In the meantime, platform provi- ders have adapted noticeably to this “responsibility turn” by admitting mistakes, accepting responsibility and – for example Facebook – interpreting their mission of connecting all people more qualitatively than quantitatively in external communication (Lischka, 2019; Haupt, 2021). Twitter also changed their opening statement signiﬁcantly. Since 2019 it reads: “Twitter’s purpose is to serve the public conversation. Violence, harassment and other similar types of behaviour discourage people from expressing themselves, and ultim- ately diminish the value of global public conversation. Our rules are to ensure all people can participate in the public con- versation freely and safely.” 3 With its stark contrast to the ori- ginal wording, these words clearly illustrate the responsibility turn in the public understanding of platforms. Twitter acknowledges the responsibility for the content on its service, and justiﬁes restrictions on freedom of expression. So while there is now a broad consensus that platforms have responsibility for the content and communication dynamics on their services, the shape of this responsibility is by no means clear. How and according to which criteria should platforms judge and, if necessary, block Katzenbach 3 controversial and problematic content? These complex questions are exacerbated by the massive size and content volume of these services, demanding to carrying out these often-difﬁcult balances between freedom of expression on the one hand and, for example, personal rights on the other hand at scale, i.e. million or even billion times day by day. In this situation, with masses of potentially prob- lematic content on the one hand and growing attribution of responsibility to platforms on the other, the technological ﬁx now appeals to many as the only way out. Providers and regulatory actors have in recent years mutually reinforced the belief that technology, especially AI, can solve these problems of social media platforms. Not only has Facebook’s CEO Zuckerberg regularly promised AI as a technological ﬁx in hearings in North America and Europe (Katzenbach, 2019; Lischka, 2019; Russell, 2019). Yann LeCun, the world’s leading AI researcher in Facebook’s service, considers “fake news” or misinforma- tion as technically solvable (Seetharaman, 2016); journal- ists regularly follow him in this view (cf. e.g. \"Why fake news is a tech problem\", Elgan 2017); and political actors, high-level courts and regulatory initiatives increas- ingly assume the efﬁcient and balanced functioning of auto- matic ﬁltering systems when drafting decisions and regulations, often without substantially taking into account their limitations and their side effects on freedom of expression (cf. the following section). In consequence, this twofold discursive development of a general discourse that positions AI as a solution to social problems and an increasing attribution of responsibility to social media plat- forms constitutes a powerful discursive turn to AI in gov- erning communication. The political turn to Ai – from liability privilege to the search for a new regime At the political-regulatory level, political actors and regula- tory bodies have in recent years initiated a search move- ment, particularly in the European context but increasingly also in the USA, that parallels the responsibil- ity turn on the discursive layer. In their quest for translating the growing demand for responsibility into law and regula- tion, political actors, courts and regulatory agencies are turning to governance mechanism that hold social media platforms more and more accountable and, in some cases, liable for the content that they host – which in turn further pushes platforms to automatically screen their content for possibly infringing or violating content. This development constitutes a remarkable move away to the paradigm that has dominated Internet regulation in the past twenty-ﬁve years. Since the late 1990s intermediar- ies and platforms have operated under a liability privilege and the notice-and-takedown procedure: Only when provi- ders have knowledge of illegitimate content or unlawful conduct on their services, they do have to take action by ﬁl- tering content or blocking users’ access. This paradigm has been established both in Europe with the EU E-Commerce Directive 2001 (Kuczerawy and Ausloos, 2015) and in the US in the US Digital Millennium Copyright Act (DMCA) and with even more extensive freedoms in Section 230 of the US Communication Decency Act (CDA) (Citron and Wittes, 2017; Gasser and Schulz, 2015; Holland et al., 2015) since around the turn of the millennium. In the Europen Union (EU), this supranational directive has been installed in national regulations. In Germany, for example, this downstream responsibility of social media providers has been expressed, for example, in the Telemedia Act (TMG).4 Telemedia providers in Germany, including platforms, are legally responsible only for their “own information” they provide (Section 3, §7). Since the function of social media is usually understood in case law to mean that they – in the words of the TMG – only “trans- mit or […] provide access to use”“third-party” information, the content in dispute does not fall under Section 3, §7. However, according to Section 3, §8 TMG, they are not responsible for the content they mediate and cannot be held liable for it. Only when they become aware of illegal content do they have to intervene (Section 3, §10). For almost ten years, however, there has been a develop- ment in Europe towards a much narrower interpretation of this liability privilege and even a turning away from this paradigm. This “road to responsibilities” (Síthigh, 2020; Kuczerawy, 2019) is expressed both in court rulings and in regulatory initiatives, ﬁrst in national rulings and law, but increasingly also on the European level. In Germany, for example, the Federal Supreme Court (BGH) reformu- lated the liability privilege in a ruling on the ﬁle hoster Rapidshare in a demanding way by imposing a \"market monitoring obligation\" on the provider, which obliges it to \"determine, using suitably formulated search queries and […] using web crawlers, whether there are indications of further infringing links on its service with regard to the speciﬁc works to be checked\". In German legislation the development of increasingly strong joint liability of provi- ders culminated in the Network Enforcement Act (NetzDG), which, primarily as a reaction to increased hate speech and misinformation in social media, obliges the major platforms to respond to complaints from netizens in short time windows (24 h for \"obviously illegal\" content, seven days for all others) and to delete content if necessary (Schulz, 2018). The new State Treaty on the Modernization of the Media Order in Germany (\"Media State Treaty\") con- cluded by the federal states at the end of 2019 also follows this route: social media providers will be integrated into the federal German system of broadcast media regulation as \"media intermediaries\" (in the new §2 para. 2 no. 16) once this State Treaty has been passed by the state parlia- ments. As a result, they will have to disclose the \"central cri- teria of aggregation, selection and presentation of content 4 Big Data & Society and their weighting\" (§ 93 Transparency). In addition, they must ensure that individual content providers are not \"dis- criminated against\", i.e. the declared criteria must be applied indiscriminately to all content (§ 94 Freedom from discrimination). However, the impact of these new forms of regulation has yet to be seen. While the freedom from discrimination is a regulatory novelty, the transpar- ency requirements and the increased joint liability are not a German peculiarity. In other countries such as France and England, legislators and regulation are also moving in this direction (Bunting, 2018, Sitingh 2020). Institutions at the European level have also gradually moved away from the paradigm of liability privilege in court rulings of the highest instance and legislative initia- tives, and have introduced stronger requirements and liabil- ity rules. The European Court of Justice (ECJ), in rulings such as that on the \"right to be forgotten\", has signiﬁcantly increased the responsibility of platforms for the content they provide (Kuczerawy and Ausloos, 2015), and increasingly obliges providers to use automated procedures to prevent the re-publication of statements and content once notiﬁed as unlawful with the same content (Glawischnig-Piesczek vs. Facebook, ECJ, C-18/8, cf. Kuczerawy 2019). The EU Commission and the European Parliament are signiﬁ- cantly increasing the legal co-responsibility of social media providers through the introduction of self-obligations of providers to immediately delete and block content glori- fying terrorism and violence, through the adoption of the new copyright directive with signiﬁcantly more far- reaching liability provisions (Directive EU 2019/790) and in the planned comprehensive EU Digital Services Act. In the US, regulatory initiatives to revise both the DMCA and the CDA also point towards this to road to responsibil- ity and more procedural accountability (Keller, 2020), but have not yet gained enough political momentum and consti- tutional consistence (Keller, 2021). These various measures at the political-regulatory level constitute a search movement aimed at translating the con- sensual demand for more responsibility of platforms into adequate regulatory measures. We can clearly observe a gradual departure from the pure paradigm of liability priv- ilege. The destination of this new route is not yet foresee- able. A switch to the antithesis, the liability of platforms for all content they convey, appears neither desirable nor realistic. The encroachment on freedom of opinion and information and the loss of diverse public spheres would be too serious (Schulz, 2019). What is already clearly fore- seeable, however, is the noticeably narrower and much more demanding conditions of the liability privilege, which at least require proactive measures from providers to block illegal or infringing content that is already known. Added to this are the increasing demands to effect- ively detect and combat even difﬁcult-to-classify issues such as copyright infringement, hate speech or misinforma- tion increase (Bloch-Wehba, 2020). These growing political-regulatory demands on platforms massively favour the algorithmic turn presented here. For how can the categorization and, if necessary, ﬁltering of content be achieved at scale? In view of the scale of content and the size of the problems, automated procedures alone seem to be able to help. In this way, the current political-regulatory development is strongly pushing forcing the algorithmic turn in the governance of platforms. Contributions to this special section Thus, there is more to the turn to AI in platform governance and the regulation of digital communication than pure technological progress. It is an entanglement of discursive and political developments, the existing technological advancements and strong economic interests. Against this backdrop, this special theme features a diverse set of articles that addresses this multi-layered turn to AI from various perspectives. The ﬁrst two articles approach the theme by investigat- ing public discourses and expectations in the context of AI. In their piece, Jonathan Roberge, Marius Senneville and Kevin Morin mobilize Callon’s concept of “translation” (Callon, 1986) to surface the rhetorical and discursive work that tech entrepreneurs and policy makers employ in order to position AI as an essential and indispensable element of contemporary societies. Despite substantial criticism, these actors successfully translate very different, often long- existing, sometimes buggy technologies into AI as a coher- ent rhetorical device that promises a better tomorrow, as the authors can show in their case studies on the Montreal Declaration for a Responsible Development of AI, the Zuckerberg Hearing at the US Senate and the integration of armed drones and robots into the military. Aphra Kerr, Marguerite Barry and John D Kelleher approach the theme of public discourses and expectation by focusing on the emerging debate on AI and ethics. In their article, the authors ask how societal expectations on an emerging technology such as AI are structured and how this in turn informs the further development of the phenomenon. Their study combines an analysis of documents published by key actors in AI research and policy and a public survey on awareness of AI, expectations and ethical consid- erations. The ﬁndings show that a range of actors construct the expectation that AI brings about massive economic opportunities but also ethical concerns. In many ways, expectations take a performative function here with actors “talking AI into being” (Bareis and Katzenbach, 2021) but saying little about the challenges of applying these tech- nologies in particular social contexts. In consequence, the authors call for a less technology-oriented and much more situated and practice-based approach to the integration of AI into society, including more domain appropriate AI tools, updated professional practices, digniﬁed places of work and robust regulatory and accountability frameworks. Katzenbach 5 The second focus area of this special theme focuses on such a speciﬁc domain: the automated moderation and regu- lation content on social media platforms. Robert Gorwa, Reuben Binns and Christian Katzenbach give an overview on the technical foundations and the practical deployment of automated tools in the moderation of content on social media platforms. The authors offer a typology that differ- entiates matching and classiﬁcation as key technological logics and different types of regulations such as blocking, (de-)monetization, downranking and ﬂagging. Social media platforms have long used such systems for identify- ing possible copyright infringement, they now increasingly screen their content automatically as well for hate speech, misinformation, terrorist propaganda and other harmful or controversial content. While many take issue with the often erroneous outcome of these systems, the authors make a much more fundamental argument: Even if or when such systems operate perfectly from a technical per- spective, three highly political issues will always exacerbate rather than relieve: algorithmic content moderation threa- tens to further increase opacity, to further complicate out- standing issues of fairness and justice and to re-obscure the fundamentally political nature of speech decisions being executed at scale. With these high issues at stake, Joanne E Gray and Nicolas P Suzor turn themselves to AI technologies to understand the social media platforms’ automated content moderation practices. With a methodo- logical experimentation built on machine learning technolo- gies, they investigate a dataset of almost 80 million YouTube for patterns in removal of content. In the substan- tial dimension, the article shows that content is mostly blocked because of user account termination (4%). 0.77% of content is being blocked because YouTube’s Content ID automatically ﬂagged the content as copyright infringe- ment; only 0.57% is taken due to infringements of YouTube’s Terms of Service and 0.11% because of copy- right owners’ DMCA requests. On the methodological dimension, the article offers a fruitful approach to not only investigate content moderation at scale but to proper understand at scale content moderation. The third and last part of the special theme offers analyses of the political ramiﬁcations and challenges of the turn to AI in communication governance. In her commentary, Emma J Llansó argues that despite the current automated measures no amount of AI in content moderation will solve ﬁltering’s prior-restraint problem. Llansó reminds us that the legal concept of prior restraint – that a speaker must seek approval from some empowered third party – is typically considered to be incompatible with international human rights law. The current policy and technological turn to AI and proactive measures in content moderation is exposing more and more speech online to evaluation and approval, and thus constitu- tes not only a road to responsibility but also a return trip to prior restraint. In consequence, Llansó calls on governments and regulators to strictly restrain from ﬁltering mandates, and on companies to recognize the human rights risks inherent in their content moderation systems and to really thrive to miti- gate them. Niva Elkin-Koren approaches these challenges from a different angle in her research article. The author agrees with Llansó’s analysis that ﬁlters carry censorial power, potentially bypassing traditional checks and balances secured by public law and easily escaping oversight due to their opaque and dynamic nature. Her suggestion, though, is to counter these challenges with rather more than less AI: with public interest driven AI technologies that operate with an adversarial logic. Such an initiative could counterbal- ance the single optimization standard of current content removal systems by introducing other trained AI systems that follow different logics such as pluralism or existing case law. The concluding commentary by Tarleton Gillespie ﬁnally articulates the elephant in the room: the issue of scale. The turn to AI is often justiﬁed as a response to scale: social media platforms have grown so large that only automated measures seem to be able to handle the massive amounts of content, and that AI systems thus appear desirable, even inevitable. At the same time, as the articles of this special theme have shown, a great many pro- blems remain – even with highly optimized and efﬁcient systems. But what is the consequence then? Maybe, if mod- eration is so challenging at scale, Gillespie argues, we should conceive of this as a limiting factor on the “growth at all costs” mentality. With these different perspectives on the multi-layered turn to AI, the articles of this special theme may not always offer clear answers but they articulate the stakes of the current development and raise the key questions. Given the already deep integration of these technologies into our daily lives (Hepp, 2020) and the massive economic power of the major companies, the medium- and long-term challenge will be to continually establish and address these fundamental political questions on the public and political agenda. The history of science and technology teaches us that once a technological ﬁx is successfully installed – i.e., established as successful, regardless of its actual effects – such questions tend to disappear from the space of debate and negotiation, and become infrastructures that are taken for granted. We can already anticipate that algorithmic mod- eration and regulation will become more and more seam- lessly integrated into our social lifes. We hope that with this special theme, we contribute to counteract such normal- ization of automated decisions and to keep questions of content moderation and the automation of the social on the public and political agenda – because we know at least one thing: AI will never ﬁx this, whatever this is. Declaration of conﬂicting interests The author(s) declared no potential conﬂicts of interest with respect to the research, authorship, and/or publication of this article. 6 Big Data & Society Funding Research underlying this article has received funding from the European Union’s Horizon 2020 research and innovation pro- gramme under grant agreement number 870626 (“ReCreating Europe”) and from the German Research Foundation (DFG) as part of the multinational scheme “Open Research Area” (ORA) under grant number 440899634. ORCID iD Christian Katzenbach https://orcid.org/0000-0003-1897-2783 Notes 1. Cf. the video and ofﬁcial documentation of the hearing on the US Senate website (https://www.judiciary.senate.gov/meetings/faceb ook-social-media-privacy-and-the-use-and-abuse-of-data), as we ll as the verbatim transcript at: https://en.wikisource.org/wiki/Zu ckerberg_Senate_Transcript_2018. 2. Cf. Twitter Rules in the version of 2009 as archived by the Platform Governance Archive (Katzenbach et al. 2021), avail- able at: https://pga.hiig.de/view/twitter/cg/2009-01-18, ll. 4-8. 3. Cf. Twitter Rules in the version of 2019 as archived by the Platform Governance Archive (Katzenbach et al. 2021), avail- able at https://pga.hiig.de/view/twitter/cg/2019-06-07, ll. 2-5. 4. Available at: https://dejure.org/gesetze/TMG References Ames MG (2018) Deconstructing the algorithmic sublime. Big Data & Society 5(1): 1–4. Bareis J and Katzenbach C (2021) Talking AI into being: The nar- ratives and imaginaries of national AI strategies and their per- formative politics. Science, Technology, & Human Values:1– 27. https://doi.org/10.1177/01622439211030007. Bloch-Wehba H (2020) Automation in moderation. Cornell International Law Journal 53: 41–96. Brennen JS, Howard PN and Nielsen RK (2018) An industry-Led debate: How UK Media cover artiﬁcial intelligence. Reuters Institute for the Study of Journalism: 10. Bory P (2019) Deep new: The shifting narratives of artiﬁcial intelli- gence from deep blue to AlphaGo. Convergence:1–16. https:// doi.org/10.1177/1354856519829679. Bunting M (2018) From editorial obligation to procedural accountability: Policy approaches to online content in the era of information intermediaries. Journal of Cyber Policy 3(2): 165–186. Callon M (1986) Some elements of a sociology of translation: Domestication of the scallops and the ﬁshermen of St brieuc Bay. In: Law J (eds) Power, Action, and Belief: A New Sociology of Knowledge? London: Routledge and Kegan, 196–233. Campolo A and Crawford K (2020) Enchanted determinism: Power without responsibility in artiﬁcial intelligence. Engaging Science, Technology, and Society 6: 1. Cardon D, Cointet J-P and Mazières A (2018) Neurons spike back: The invention of inductive machines and the artiﬁcial intelli- gence controversy. Réseaux 211(5): 173. Cave S and Dihal K (2019) Hopes and fears for intelligent machines in ﬁction and reality. Nature Machine Intelligence 1(2): 74. Chuan C-H, Tsai W-HS and Cho SY (2019) Framing Artiﬁcial Intelligence in American Newspapers. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society - AIES ‘19, pp.339–344. https://doi.org/10.1145/3306618. 3314285. Citron DK and Wittes B (2017) The internet will Not break: Denying Bad samaritans section 230 immunity. Fordham Law Review 86(2): 401–423. Daub A (2020) What Tech Calls Thinking: An Inquiry Into the Intellectual Bedrock of Silicon Valley. New York: Farrar Strauss & Giroux. Degele N (2002) Einführung in die Techniksoziologie. München: Fink. Elgan M (2017) why fake news is a tech problem. With fake news wrecking everything, silicon valley is our last hope. Computer World. https://www.computerworld.com/article/ 3162020/why-fake-news-is-a-tech-problem.html. Fast E and Horvitz E (2017) Long-Term Trends in the Public Perception of Artiﬁcial Intelligence. In: Proceedings of the Thirty- First AAAI Conference on Artiﬁcial Intelligence (AAAI-17). Fischer S and Puschmann C (2021) Wie Deutschland über Algorithmen Schreibt. Eine Analyse des Mediendiskurses über Algorithmen und Künstliche Intelligenz (2005–2020). Gütersloh: Bertelsmann Stiftung. https://doi.org/10.11586/2021003. Gasser U and Schulz W (2015) Governance of online intermediar- ies: Observations from a series of national case studies. Berkman Center Research Publication 2015(5): 1–27. https:// doi.org/10.2139/ssrn.2566364. Gillespie T (2010) The politics of ‘platforms’. New Media & Society 12(3): 347–364. Gillespie T (2014) The relevance of algorithms. In: Gillespie T, Boczkowski P and Foot K (eds) Media Technologies. Cambridge, MA: MIT Press, 167–193. Gorwa R, Binns R and Katzenbach C (2020) Algorithmic content moderation: Technical and political challenges in the automa- tion of platform governance. Big Data & Society 7(1): 1–15. Haupt J (2021) Facebook futures: Mark Zuckerberg’s Discursive con- struction of a better world. New Media & Society 23(2): 237–257. Hepp A (2020) Deep Mediatization. London: Routledge. Holland A, Bavitz C, Hermes J, et al. (2015) NoC Online Intermediaries Case Studies Series: Intermediary Liability in the United States. 70. Berkman Centre for Internet & Society. Cambridge, MA. https://perma.cc/2QAY-UTDY. Katzenbach C (2019) AI will ﬁx this. In: Kettemann M and Dreyer S (eds) Busted! The Truth About the 50 Most Common Internet Myths. Internet Governance Forum, Berlin: Leibniz Institute for Media Research | Hans-Bredow-Institute, 194–195. https://www.hiig.de/wp-content/uploads/2019/11/dgzmogf_ KettemannDreyerInternetMyths2019-1.pdf. Katzenbach C (2021) Die Öffentlichkeit der plattformen: Wechselseitige (Re-)institutionalisierung von Öffentlichkeiten und plattformen. In: Eisenegger M, Prinzing M, Ettinger P and Blum R (eds) Digitaler Strukturwandel der Öffentlichkeit. Springer Fachmedien Wiesbaden, 65–80. Katzenbach C, Magalhães JC, Kopps A, et al. (2021) The Platform Governance Archive (PGA). Berlin. https://doi.org/10.17605/ OSF.IO/XSBPT. Katzenbach 7 Keller D (2020) CDA 230 Reform Grows Up: The Pact Act Has Problems, But It’s Talking About The Right Things. Stanford: Center for Internet and Society. http://cyberlaw.stanford.edu/ blog/2020/07/cda-230-reform-grows-pact-act-has-problems-it’s- talking-about-right-things. Keller D (2021) Six Constitutional Hurdles For Platform Speech Regulation. Stanford: Center for Internet and Society. http:// cyberlaw.stanford.edu/blog/2021/01/six-constitutional-hurdles- platform-speech-regulation-0. Kuczerawy A (2019) General monitoring obligations: A New cornerstone of internet regulation in the EU? (SSRN Scholarly Paper ID 3449170). Social Science Research Network. https://papers.ssrn.com/abstract = 3449170. Kuczerawy A and Ausloos J (2015) From notice-and-takedown to notice-and-delist: Implementing google Spain. Colorado Technology Law Journal 14(2): 219–258. Lischka JA (2019) Strategic communication as discursive institu- tional work: A critical discourse analysis of mark Zuckerberg’s Legitimacy talk at the european parliament. International Journal of Strategic Communication 13(3): 197–213. Morozov E (2011) Don’t be evil. The New Republic. http:// www.newrepublic.com/article/books/magazine/91916/google- schmidt-obama-gates-technocrats. Morozov E (2013) To Save Everything, Click Here: The Folly of Technological Solutionism. New York: PublicAffairs. Napoli P and Caplan R (2017) Why media companies insist they’re not media companies, why they’re wrong, and why it matters. First Monday 22(5): 1–16. https://doi.org/10.5210/ fm.v22i5.7051. Noble SU (2018) Algorithms of Oppression: How Search Engines Reinforce Racism, 1 edition New York: NYU Press. Righetti N (2021) Four years of fake news. A Quantitative Analysis of the Scientiﬁc Literature [Preprint]:1–24. SocArXiv. https://doi.org/10.31235/osf.io/buemr. Russell FM (2019) The New gatekeepers. Journalism Studies 20(5): 631–648. Seetharaman D (2016) Facebook looks to harness artiﬁcial intelligence to weed Out fake news. Company executives say the social network ﬁrst needs a policy on how to responsibly apply such capabilities. Wall Street Journal. https://www.wsj. com/articles/facebook-could-develop-artiﬁcial-intelligence-to- weed-out-fake-news-1480608004?mod = pls_whats_news_us_ business_f Schulz W (2018) Regulating Intermediaries to Protect Privacy Online – the Case of the German NetzDG. HIIG Discussion Paper Series, 2018–01. https://www.hiig.de/wp-content/ uploads/2018/07/SSRN-id3216572.pdf. Schulz W (2019) Roles and Responsibilities of Information Intermediaries. Fighting Misinformation as a Test Case for a Human Rights-Respecting Governance of Social Media Platforms. Hoover Institution, Stanford University, USA: Aegis series Paper 1904. Síthigh DM (2020) The road to responsibilities: New attitudes towards internet intermediaries. Information & Communications Technology Law 29(1): 1–21. Tworek HJS (2021) Fighting hate with speech Law: Media and German visions of democracy. The Journal of Holocaust Research 35(2): 106–122. Vaidhyanathan S (2012) The Googlization of Everything (And Why We Should Worry). Berkeley: University of California Press. Volti R (2014) Society and Technological Change. 1995. Dufﬁeld: Worth Publishers. Zeng J, Chan C and Schäfer MS (2020) Contested Chinese dreams of AI? Public discourse about artiﬁcial intelligence on WeChat and People’s Daily online. Information, Communication & Society:1–22. https://doi.org/10.1080/ 1369118X.2020.1776372. 8 Big Data & Society","libVersion":"0.0.0","langs":"","hash":"","size":0}