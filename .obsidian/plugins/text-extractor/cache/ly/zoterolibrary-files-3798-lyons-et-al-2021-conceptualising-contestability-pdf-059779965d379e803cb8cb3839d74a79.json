{"path":".obsidian/plugins/text-extractor/cache/ly/zoterolibrary-files-3798-lyons-et-al-2021-conceptualising-contestability-pdf-059779965d379e803cb8cb3839d74a79.json","text":"Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions HENRIETTA LYONS, The University of Melbourne, Australia EDUARDO VELLOSO, The University of Melbourne, Australia TIM MILLER, The University of Melbourne, Australia As the use of algorithmic systems in high-stakes decision-making increases, the ability to contest algorithmic decisions is being recognised as an important safeguard for individuals. Yet, there is little guidance on what ‘contestability’–the ability to contest decisions–in relation to algorithmic decision-making requires. Recent research presents different conceptualisations of contestability in algorithmic decision-making. We contribute to this growing body of work by describing and analysing the perspectives of people and organisations who made submissions in response to Australia’s proposed ‘AI Ethics Framework’, the first framework of its kind to include ‘contestability’ as a core ethical principle. Our findings reveal that while the nature of contestability is disputed, it is seen as a way to protect individuals, and it resembles contestability in relation to human decision-making. We reflect on and discuss the implications of these findings. CCS Concepts: • Human-centered computing → Human computer interaction (HCI). Additional Key Words and Phrases: contestability; algorithmic decision-making; artificial intelligence; algorithmic fairness, account- ability, and transparency ACM Reference Format: Henrietta Lyons, Eduardo Velloso, and Tim Miller. 2021. Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 106 (April 2021), 26 pages. https://doi.org/10.1145/3449180 1 INTRODUCTION Algorithmic systems, including machine learning and artificial intelligence (AI) systems, are increasingly being used in high consequence decision-making including sentencing [7], hiring [40], performance evaluation [1], and loan applications [34]. In many cases, these systems are trained on data about past decisions and employ machine learning to produce a prediction, or ‘output’ [16, 46, 71]. In some instances decisions are made autonomously, while in others, outputs are used to assist humans to make decisions. Algorithmic systems have the potential to enhance decision-making by making it more efficient, scalable, consistent, objective, accurate, and cost effective [20, 38]. Yet, the use of algorithmic systems in decisions that significantly impact lives has also raised concerns relating to fairness and justice [10, 21, 28], human dignity [32, 33], and autonomy [32, 33]. Over the past few years there have been high profile examples of algorithmic systems making discriminatory decisions based on race [7], enhancing inequality [22], and being used in a way that violates due process rights [1]. Mulligan et al. [46] reflect that “there is a growing sense that our tools, if not checked, will undermine our choices, our values, and our public policies”. There have been calls for algorithmic systems to be designed responsibly [20], and in a way that ensures that they are fair, accountable, and transparent [2, 53]. Consequently, numerous principles and guidelines for creating responsible, trustworthy, and ‘ethical AI’ have been developed [32]. One safeguard that is gaining traction within these ethical guidelines is ‘contestability’—the ability to contest, appeal, or challenge algorithmic decisions Authors’ addresses: Henrietta Lyons, hlyons@student.unimelb.edu.au, The University of Melbourne, Parkville Campus, Melbourne, Victoria, 3010, Australia; Eduardo Velloso, eduardo.velloso@unimelb.edu.au, The University of Melbourne, Parkville Campus, Melbourne, Victoria, 3010, Australia; Tim Miller, tmiller@unimelb.edu.au, The University of Melbourne, Parkville Campus, Melbourne, Victoria, 3010, Australia. Manuscript submitted to ACM 1 2 Henrietta Lyons, Eduardo Velloso, & Tim Miller [32, 46]. For example, the European Union’s ‘Ethics Guidelines for Trustworthy AI’ sets out that AI systems must be developed, deployed, and used in a fair way, which includes the ability to contest decisions made by, or involving, AI [50]. Australia’s AI Ethics Framework includes ‘Contestability’ as one of its eight core principles for designing, developing and implementing ethical AI [48]. In addition to these voluntary guidelines, Article 22(3) of the European Union’s General Data Protection Regulation (GDPR) provides a legal ‘right to contest’ in relation to decisions made using solely automated processes. Little is known about what contestability in relation to algorithmic decisions entails, and whether the same processes used to contest human decisions, which range from informal complaints mechanisms to formal legal processes, are suitable for algorithmic decision-making. The ethical guidelines do not provide guidance on what contestability is, what the process of contestation should look like, or how algorithmic systems should be designed, developed, or implemented to ensure that decision subjects can contest decisions made using algorithms. Turning high-level principles into implementable actions is a challenging task for those designing and deploying AI systems [41, 44]. Kaminski highlights that a lack of guidance could disadvantage decision subjects—individuals impacted by algorithmic decisions—who are likely to have different interests to the decision maker: “This raises the question of whether a company whose interests do not always align with its users’ will be capable of providing adequate process and fair results. There is room for substantially more policy development in fleshing out this contestation right” [33]. Emerging research presents different conceptualisations and interpretations of what contestability in algorithmic decision-making requires and how these sociotechnical systems can be designed to support contestability (e.g. [5, 8, 30]). Our goal is to contribute to this growing body of work by analysing submissions made in response to a discussion paper, titled “Artificial Intelligence: Australia’s Ethics Framework”, released by the Australian Government. To our knowledge, this is the first AI ethics framework of its kind to propose contestability as a core principle. We seek to understand how contestability is conceptualised as well as how the operation of contestability is envisioned by those who provided submissions. The submission responses provide a broad range of different perspectives, with submissions from the Australian public sector, the private sector (including multinational companies such as Microsoft and Adobe as well as small to medium enterprises), academics, universities, research institutes, community organisations, industry associations, and individuals [48]. The submissions provide a greater diversity of viewpoints than if we had focused solely on academic literature. Further, many of the submissions are from entities that are likely to implement contestation processes in relation to algorithmic decision-making in the future: there is value in understanding how these organisations view contestability and envision it operating. Through a thematic analysis of 65 submissions, we found that while there was a lack of consensus on what the true nature of contestability is, there was agreement that contestability is about protecting individuals. Contestability in algorithmic decisions was also seen as requiring the same processes as contestability in human decisions. To operationalise contestability, submissions highlighted the need for decision makers to address a range of policy considerations such as what can be contested, in addition to providing a contestation process that meets certain design requirements such as accessibility. This data helps to map a minimum range of conceptual debates around contestability in this space. We reflect on the submission responses, using them to prompt discussion around a range of challenges and opportunities that designing for contestability in algorithmic decision-making presents. Contestability in algorithmic decision-making is a complex and nuanced emerging area of research. Through a rich description of how submissions to Australia’s AI Ethics Framework conceptualise contestability in algorithmic Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 3 decision-making and an analysis of the implications of these findings, we map a range of conceptual debates around contestability that can serve as a trigger for discussion. 2 RELATED WORK 2.1 Contesting algorithmic decisions: Process challenges In democratic societies, the ability to contest decisions that have a significant impact on a person’s life, whether made by a human or algorithm, is an important way to uphold human dignity and autonomy [5, 12, 33, 41, 71]. However, recent work indicates that people impacted by decisions made using algorithmic systems can be severely limited in their ability to contest these decisions. There are a number of jurisdictions, including Australia, the United Kingdom and the United States, where government decision-making is regulated. For example, in the United States, state governments must not “deprive any person of life, liberty, or property, without due process of law” (U.S. Const. amend. XIV, § 1). ‘Due process’ generally includes the ability to contest a decision, amongst other rights. Despite being constitutionally enshrined, the AI Now Institute [31, 54] reports numerous violations of due process rights by states using algorithmic decision-making. Breaches that directly impact a person’s ability to contest an algorithmic decision include: a failure to provide notice that a decision has been made; inadequate explanation for a decision that has been made; and, insufficient information about how to appeal [31, 54]. In these circumstances, a person has grounds to litigate, however barriers such as cost, time, and lack of knowledge of legal rights can stop individuals from appealing decisions in court. These barriers are particularly concerning when algorithmic decision-making disproportionately impacts vulnerable communities [22]. Algorithmic decision-making in the private sector is largely unregulated in a direct manner, a notable exception to this is the European Union’s General Data Protection Regulation. However, in light of the numerous ethical AI guidelines in current circulation [32], there is a growing expectation that companies design and deploy AI systems responsibly. There are also reputational benefits to providing mechanisms for appeal and dispute resolution. Work relating to content moderation on social media platforms highlights some of the difficulties people face when trying to appeal algorithmic decisions in the private sector. Myers West [47] surveyed 519 social media platform users to understand their experience with content moderation systems, which include both human and automated content review and removal. Myers West found that some platforms did not provide a review process at all. Where a review process did exist, many of the 230 users who chose to make an appeal experienced difficulties with the process. For example, users were often provided with no instructions or information about how to initiate an appeal. Complaints were also made about the failure of decision makers to provide a response or resolution after a decision subject lodged a request for review. Our work contributes to this literature by exploring perspectives from a range of sectors to better understand the expectations around how a contestation process in relation to algorithmic decision-making could operate. 2.2 Different conceptualisations of contestability 2.2.1 Interactive algorithmic systems. Work in human-computer interaction (HCI) has explored the ability of expert users to interact with a system and to correct errors in order to enhance their understanding of the system and to improve its functioning [6, 15, 17, 26, 61]. There has also been work exploring how users can override or correct low impact algorithmic decisions, such as recommendations, by adjusting a rating [30], or providing feedback via user Manuscript submitted to ACM 4 Henrietta Lyons, Eduardo Velloso, & Tim Miller created tags [6, 65]. Recent work in HCI extends these avenues of research to consider how algorithmic decision-making systems can be designed to enable users to interactively contest significant decisions made by these systems [30]. Hirsch et al. [30] propose that contestability as a design principle is important because it helps to ensure that a person who might be affected by a system outcome has a chance to correct errors and record disagreement with the system. This is particularly important if there is a risk that the output of the system could be used “as a blunt assessment tool” [30]. Hirsch et al.’s work focuses on the development of a system that evaluates the delivery of psychotherapy by psychotherapists [30]. In this work, the ‘user’, who is able to interact with the system, is an expert, but also a decision subject. To design for contestability, Hirsch et al. suggest four ‘design strategies’: 1) increase model accuracy through testing; 2) ensure legibility through the use of explanation, confidence scores, and the ability to trace the output; 3) provision of training for users; 4) ongoing monitoring of the system, to expose any systemic issues [30]. Mulligan and Kluttz [46] propose that designing for contestability can also contribute to the responsible use of algorithmic systems. This is done by encouraging interactivity between expert user and the system, allowing for critique and correction of the system’s reasoning as well as promoting a feeling of shared responsibility [46]. Based on this approach, contestability can considered to be a governance mechanism [46]. 2.2.2 The General Data Protection Regulation. Article 22(1) of the GDPR sets out a right to not be subject to a decision that has been made using solely automated processing if the decision has a legal (or similar) effect. In the case that a person consents to having a solely automated decision made, or such a decision needs to be made in order to enter into a contract (Article 22(2)), Article 22(3) specifically sets out the right to contest such a decision: “...[T]he data controller shall implement suitable measures to safeguard the data subject’s rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and to contest the decision”. While this restriction on automated processing is not new, in comparison to its predecessor, Article 15 of the 1995 Data Protective Directive, Article 22(3) provides stronger safeguards for data subjects [41]. Guidance suggests that the right to contest under the GDPR requires an internal review process conducted by the decision-making entity, but how this review is carried out is not specified [33, 52]. In recent research interpreting Article 22(3), it is suggested that the ability to contest an outcome will be aided by algorithmic systems being designed to support contestability [5, 8, 55]. There is acknowledgment that not just the final decision should be rendered contestable, but the entire decision-making process [8]. Almada [5] proposes that to ensure data subjects are provided with the rights the GDPR seeks to protect, including self-determination, contestation should not only be considered a post-decision mechanism, such as an appeal process. Rather, algorithmic systems should be designed to be contestable, an approach Almada calls ‘contestability by design’. This approach—of designing systems to enable and support contestation—goes beyond simply providing contestation processes such as reviews by the decision-making entity or a court; it is ‘preventative’ [5]. Almada suggests that participatory design be employed, where a representative sample of stakeholders partake in the design process, which will help to detect risks in the system and should reduce the need for contestation. Further, information pertaining to the decision and a pathway to contest should be provided, potentially through digital means such as an interface [5]. 2.2.3 Recourse. Recent work introduces the need for ‘recourse’ in relation to algorithmic decisions [60, 64]. Ustan et al. [60] define recourse as “the ability of a person to change the decision of the model through actionable input variables”. This definition differs from the notion of legal recourse, which focuses on righting a wrong, for example through providing compensation. Ustan et al. [60] present a toolkit that provides a “flipset”, which is a “list of actionable changes that an individual can make to flip the prediction of the classifier”. In effect, a flipset is an explanation. Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 5 Venkatasubramanian and Alfano [64] note that underlying Ustan et al.’s notion of recourse, “recourse narrowly conceived”, is the assumption that the decision was correct. Venkatasubramanian and Alfano [64] highlight that a decision may actually be incorrect because it is based on data that is “faulty or incomplete”, in which case correcting can occur through an “appeal”. In either case—appeal or recourse narrowly defined—it is envisioned that a person would receive a flipset that would describe the changes they need to make to alter the decision of the model [64]. In our view, contestability is a broader concept than recourse and appeal as described by Venkatasubramanian and Alfano [64]. Beyond the need for an explanation, contestability requires avenues through which decision subjects can elect to contest a decision [5], and processes that could be used to review algorithmic decisions [5, 39, 52, 55, 59, 66]. The above literature considers how algorithmic systems can be designed to support contestability and recourse. It also highlights different conceptualisations of contestability. Our work contributes to this literature by analysing empirical data to understand how contestability is conceptualised by those who made submissions in response to the Australian government’s proposed AI Ethics Framework. 3 METHOD Using thematic analysis, we analysed submissions made in response to a discussion paper released by the Australian Government, titled “Artificial Intelligence: Australia’s Ethics Framework” (which we refer to as ‘the Discussion Paper’ in the remainder of this paper). We explore how the submissions conceptualise contestability in relation to algorithmic decision-making and how they envision contestability operating. 3.1 Data collection We collected data from submissions made in response to the Discussion Paper which was funded by Australia’s Department of Industry, Innovation and Science (DIIS) [19]. The Discussion Paper proposed eight voluntary ethical principles to be taken into account in the design, development, deployment, and operation of AI in Australia (see Appendix). Principle 7 is “Contestability: When an algorithm impacts a person there must be an efficient process to allow that person to challenge the use or output of the algorithm”. As far as we are aware, Australia’s ethics framework is the first to use contestability as a principle in its own right, creating a unique opportunity for examining reactions to it. In addition to the voluntary ethical principles, the Discussion Paper suggests a ‘toolkit’ of steps that entities using AI can take to action the principles. A tool that is closely related to contestability is ‘recourse mechanisms’, which is described as: “Avenues for appeal when an automated decision or the use of an algorithm negatively affects a member of the public” [19]. Feedback on the Discussion Paper was sought between 5 April 2019 and 31 May 2019 via a ‘Consultation Hub’ on the DIIS website. Feedback could be provided by answering discussion questions in a survey (see Appendix for the list of questions) and/or uploading a written submission. More than 130 written submissions were made in response to the Discussion Paper. The submissions were from the Australian public sector, the private sector (including multinational companies such as Microsoft and Adobe as well as small to medium enterprises (SMEs)), academics, universities, research institutes, community organisations, industry associations, and individuals [48]. Of these submissions, 116 gave permission to be published and have been made available to the public via the DIIS Consultation Hub [48]. Those making submissions were asked to provide feedback on any or all of the content in the Discussion Paper, including Principle 7 ‘Contestability’. All 116 submissions were read in full by the first author who recorded the terms used that related to contestability or recourse mechanisms. All submissions were then reviewed again for references to the following terms: contest*, appeal, Manuscript submitted to ACM 6 Henrietta Lyons, Eduardo Velloso, & Tim Miller recourse, redress, remed*, compensation, Principle 7. Sixty five of the submissions included one or more of these terms. The sections of these 65 submissions that referred to these terms became the data set for our analysis. A breakdown of the sectors from which submissions were made is shown in Figure 1. A full list of the authors of these submissions is included in the Appendix. While we did explore the data based on sector, there were no discernible trends, with submissions focusing on a wide range of issues and taking different perspectives. Therefore, we chose to analyse the submissions as a whole rather than by sector. 8 10 6 9 10 6 3 4 9 Un i v e r s i t y SM E Re s e a r c h i n s t i t u t e La r g e o r g a n i s a t i o n In d u s t r y A s s o c i a t i o n / P e a k b o d y In d i vi d u a l Go v e r n m e n t Co m m u n i t y o r g a n i s a t i o n Ac ad e m ic Fig. 1. Breakdown of the number of submissions by sector As the data was collected from publicly available documents informed consent was not sought for this research. The submissions can easily be found using search engines. No login, passwords or registration is required to gain access. There are no statements contained on the website that prohibit the use of these submissions in research. 3.2 Data analysis The first author analysed the data using Braun and Clarke’s [11] six-stage approach to reflexive thematic analysis, which was chosen for its flexibility and clear procedural steps. Taking a realist approach to the data, the first author inductively coded submissions using NVivo 12. Semantic coding was used to provide a realistic and descriptive account of how contestability is conceptualised. The codes were then sorted into initial themes and all relevant coded data extracts collated into those themes. Themes were reviewed and refined iteratively. Given that this is an under-researched area, we chose to provide a rich thematic description of the entire data set [11]. Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 7 3.3 Limitations The submissions included in our analysis provide a rich and varied insight into views on contestability. However, it is important to note that the sample is not representative of the general public, with the majority of submissions being from the academic and private sector. While there are a number of submissions from peak bodies, community organisations, and industry associations that advocate on behalf of broader communities, there are many voices that have not been heard in these submissions. In particular, given that algorithmic decision-making has been found to have a disproportionate impact on vulnerable communities [22], to avoid perpetuating historical power imbalances it is imperative that members of these communities have their views on contestability taken into account. Future work should explore whether the insights and views presented in the submissions in relation to contestability are aligned to the views of the broader public, and more specifically, to those most likely to be impacted by algorithmic decision-making. 4 CONCEPTUALISING CONTESTABILITY IN ALGORITHMIC DECISION-MAKING Our first analysis of the submissions in response to the Discussion Paper focused on the research question: How is contestability in algorithmic decision-making conceptualised? We address this research question through a qualitative analysis of the submission responses and by reflecting on these responses in the Discussion. In our qualitative analysis we found that while there was no consensus as to the nature of contestability, with submissions variously viewing it as an end in itself or a means to an end, contestability is seen by the submissions as a way to protect individuals. This theme was manifest in talk about the power differential between the decision subject and decision maker, the need for enforceability and redress, and the need for contestation to be a part of a broader regulatory approach. Contestability in relation to algorithmic decision-making was also seen by a majority of submissions as similar to contestability in relation to human decision-making, requiring the same contestation processes, with an emphasis on human review. (Note: because the Discussion Paper, and the submissions in response, refer to “AI” rather than “algorithms” or “algorithmic decision-making” our results and discussion also use this language). 4.1 The nature of contestability is disputed While contestability is seen as important, there is no consensus on its true nature. Some submissions view it as an ethical principle in its own right; an end in itself. For others, it is a means to an end, either a practical mechanism used to implement other principles or a dimension of higher order ethical principles such as fairness, accountability, or autonomy. 4.1.1 An end in itself. Many of the submissions accept the Discussion Paper’s inclusion of contestability as an important ethical principle in its own right. While the majority of submissions do not describe why contestability is viewed this way, a couple of submissions do suggest that the ability to contest decisions that have a significant impact on a person’s life is a fundamental democratic right. “Fundamental to democracy and to a fair society is the principle that power to influence the lives of others should be openly reviewable, transparent, accountable, explicable, and contestable.” (John Harvey) One submission states that contestability is an important principle because it can be used to uncover systems that are not working as they should. This suggests that the ability to contest is seen as a way to regulate the use of AI. Manuscript submitted to ACM 8 Henrietta Lyons, Eduardo Velloso, & Tim Miller “[Contestability] is crucial. Without it, AIs that are failing to achieve what is expected of them can go undetected for long periods of time.” (The Automated Society Working Group, Monash University) 4.1.2 A means to an end. Many submissions suggest that contestability is best understood as a means to an end. The submissions provide two different approaches to conceptualising contestability in this way. First, some view contestability as a dimension of other ethical principles that have been included in the Discussion Paper, namely ‘accountability’ and ‘fairness’. “Transparency, explainability, and contestability are all entailed by accountability, and they all matter precisely to the extent that they contribute to accountability.” (Australian Academy of Science and the Australian National University) The second approach to conceptualising contestability as a means to an end is the view that contestability is a tool that supports the practical implementation of other principles and values such as fairness, justice, and autonomy. “It is important to recognise that notions of contestability, recourse and redress qualify merely as opportunities that can be given to people so they can exercise their autonomy for their own benefit.” (Gradient Institute) 4.2 Contestability is a way to protect individuals Decision makers are in a position of power over decision subjects. The use of (often opaque) AI systems in decision- making is seen as widening this power gap. The ability to contest a decision offers decision subjects some protection, allowing them to take back a little control, and to hold decision makers to account. Many submissions highlight the need for contestability to be an enforceable right. The provision of redress is also widely supported, as is the need for additional mechanisms to regulate the use of AI in decision-making. 4.2.1 Power differential. The submissions highlight the difference in power between decision subjects and decision makers. Though this is not a new observation, the opacity of AI decision-making systems, whether because of their complexity or due to proprietary claims, is seen as widening this power gap—decision makers are privy to information that is not available to decision subjects. “AI ushers in a new form of digital divide: between the types of actionable ‘knowledge’ available to those with access to the database and processing power and those without such access.” (The Automated Society Working Group, Monash University) Contesting a decision provides decision subjects with a way to hold decision makers to account. Many submissions highlight the need to ensure that the processes to contest a decision are easily accessible and support is available for decision subjects. “[E]asy appeal mechanisms are crucial given the disproportionate power imbalance between the developers and implementers of AI systems and those they affect who will remain significantly more vulnerable.” (Keith Dodds Consulting) Developers, designers, and those who deploy AI systems often rely on proprietary rights, such as trade secrets, to protect their AI systems from competitors. However, these protections also diminish the ability to scrutinise the systems, leaving decision subjects with very little information on which to base any grounds to contest. A number of submissions suggest that new laws be introduced to limit the use of these proprietary rights in order to provide decisions subjects with more equal footing for contestation. Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 9 4.2.2 Enforceability. Many submissions call for contestability, and the other core principles, to be made enforceable through legislation. There is concern that if the ethical principles remain voluntary, decision makers may be selective about how and whether they follow them, and decision subjects may not be provided with avenues to contest. Further, there are no consequences for decision makers if they do not follow the guidelines. “The Law Council is concerned that there can be no proper form of contestability without enforceability... Law Council recommends that an ethical and regulatory framework be implemented formally so as to provide for enforceability.” (Law Council of Australia) 4.2.3 Redress. Numerous submissions argue that while the ability to contest a decision is useful, decision subjects will require ‘redress’, ‘recourse’, or ‘remedy’ such as a new decision, compensation for any harm erroneously caused, or reversal of the effects of a decision. To ensure that redress is available, submissions emphasise the importance of enforceability. “While the proposal rightfully stresses the necessity of recourse, it is light on discussing what actual teeth relevant regulation might provide that would be able to give adequate remedy.” (Otzma Analytics Pty Ltd) 4.2.4 Part of a broader regulatory approach. There is acknowledgement in a number of submissions that contestability should not be the only tool used to regulate the use of AI because this would place too much of a burden on decision subjects, who need to initiate contestation. Other mechanisms are needed to provide more fulsome protection to decision subjects. “The onus of contest shouldn’t rest solely with victims...We believe that further consideration needs to be given to mechanisms that encourage organisations to proactively identify situations where harm has occurred and then introduce measures to address them as they arise.” (Future AI Forum (KPMG)) Further, there are aspects of an AI system that are difficult for individuals to contest, such as the data that was used to train the system. Discrimination and bias are also difficult to perceive at an individual level. The submissions recommend that mechanisms such as freedom of information, independent oversight, ongoing monitoring, and auditing be used in addition to contestability in order to effectively regulate the use of AI. Independent oversight is a particularly popular suggestion, which can be used as a tool to guide the implementation of AI systems and their outputs generally, and also to strengthen contestability. “We also suggest adding a ninth principle covering independent oversight. Contestability and accountability require an independent third party to ensure a fair and just outcome.” (Australian Library and Information Association) 4.3 Contestability in algorithmic decision-making resembles contestability in human decision-making By proposing contestation processes that already exist in relation to human decision-making, submissions indicate that contestability in algorithmic decision-making is envisioned in a way that resembles contestability in human decision-making. 4.3.1 Established review mechanisms. Submissions draw strongly from contestation processes currently used in relation to human decision-making. These review mechanisms can be divided into two broad categories. First, ‘internal review’ where the body responsible for making a decision reviews that decision. Second, ‘external review’, where a separate body to the original decision maker, such as a tribunal, ombudsman, or court, reviews the decision. Manuscript submitted to ACM 10 Henrietta Lyons, Eduardo Velloso, & Tim Miller While many submissions raise questions about what a contestation process could look like, in effect seeking clarity from the Australian Government around process requirements, some submissions clearly advocate for specific processes. The Law Council of Australia describes the contestation process that could be used for public decision makers (internal review, followed by independent regulator, then judicial review) and a different process for private sector decisions (internal dispute resolution then referral to an independent regulator). Both of these suggestions reflect processes currently in existence for human decision-making. The National Australia Bank goes further than suggesting similar processes, and states that decision-making using AI should not be distinguished from human decision-making and that “accountability and liability should be applied consistently across both.” That is, the same protections provided for human decision-making should be provided for decisions made using AI. Similarly, submissions from the medical sector argue that existing processes relating to human decision-making should be used for decisions made using AI. “[T]here are well established processes in medicine for a patient to contest a decision taken about their care, such as via AHPRA or the health ombudsman. These entities are experienced at dealing with the complexities and sensitivities of health. RANZCR believes that these existing processes ought to remain the primary method for resolution for a complaint about medical care.” (The Royal Australian and New Zealand College of Radiologists) One submission, from Deakin University, contemplates the potential need for new review mechanisms so that decision subjects can challenge the design of an algorithm. “[The ability to challenge the design of the algorithm]...might require new institutional mechanisms to offer consumers, patients, citizens, etc. individual and collective capacity to meaningfully contest the context where AI determines or influences outcomes.” (Deakin University) The AI Now Institute provides one of the few submissions that takes a different view of how contestability can be conceptualised: it contemplates designing due process mechanisms into AI systems, which would help to ensure that decision subjects can contest decisions. In addition, submissions from the health sector contemplate how clinicians (rather than decision subjects) can contest the output of AI systems within the system. This focus on designing systems to support contestability differs from the approach taken by the majority of submissions that envision review processes outside the AI system. 4.3.2 Human review. A feature of the processes used to review human decision-making is that the reviewer is always human. If AI can be used to make decisions, there is the possibility that AI can also be used to review or remake decisions. However, a number of submissions explicitly refer to the need for human review. One submission argues that there is an innate right to have a final decision made by a human. “All individuals have the right to a final determination made by a person” (Anand Tamboli, KNEWRON Technologies). Academics Matthew Thomas and Katie Miller argue that human review is a necessity even when there is a human- in-the-loop for the decision making: “[T]he requirements of transparency, explainability and the right to human review should apply whenever AI systems have been used to assist in the decision-making process, even if the final decision is made by a human.” However, submissions also outline challenges associated with human review, including decision fatigue, complacency, bias, and scalability. Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 11 “Contestability exposes decision making processes to undue bias and there can be a lack of reproducibility in the event that the contested decision is remade by a human.” (National Australia Bank) 5 HOW CONTESTABILITY IN RELATION TO AI COULD OPERATE The second part of our analysis focused on understanding how submissions envisioned contestability operating. There were three distinct requirements. First, there are policy decisions that need to be determined to provide the scope of contestation. Second, the contestation process requires a number of steps, including the provision of an explanation of the decision and a pathway to request a review. Third, the contestation process should be accessible, context aware, and aligned with relevant legislation. Figure 2 illustrates how these themes interact. Fig. 2. A summary of how contestability could operate based on the submission responses Manuscript submitted to ACM 12 Henrietta Lyons, Eduardo Velloso, & Tim Miller 5.1 The preliminaries: policy decisions The submissions raise a number of questions that relate to policy decisions, which either need to be addressed via government regulation or guidance, or determined by the decision-making entity (in the absence of government direction). These policy decisions include specifications around what can be contested, who is allowed to contest, who is held accountable for decisions, and the type of review that will be provided. 5.1.1 What can be contested. The submissions support the need for contestability in relation to algorithmic decisions that have a significant impact on a person. The need to define significant was raised in many submissions. The Consumer Policy Research Centre points out that this definition may change depending on who is defining it: “[I]t would be beneficial to provide more information on what constitutes ‘significant impact’ and who determines that - the consumer, the service, or another entity.” In addition, KPMG notes different types of impact, including reputational, financial, and physical health. The Discussion Paper proposes that a person should be allowed to challenge the “use or output of the algorithm”. While supporting these inclusions, many submissions advocated that this remit be extended. For example, Microsoft submitted that focusing on ‘the algorithm’ was too narrow, and the focus should be on the ‘AI system’ more generally. Submissions also suggested that the grounds to contest be extended to include: how AI systems are implemented and deployed, including the role of any human decision maker; group and systemic impacts such as discrimination; and, the design of the algorithm, including its features, training data, and decision rules. A number of submissions note the difficulties that individuals will face when trying to contest grounds such as the design of the AI system because of the complexity and opacity of many AI systems. 5.1.2 Who can contest. Some people may not have the ability or the resources to contest an algorithmic decision that significantly impacts them. A number of submissions suggest the need for allowing class actions on the basis that an AI system is more likely to impact a person because of their group membership rather than on an individual basis. Class actions are also more financially accessible than individual court cases. Submissions also highlight that the government, legal practitioners, and other groups should be able to contest a decision on an impacted person’s behalf. In their submission, the Office of the Victorian Information Commissioner goes so far as to suggest that people who may be impacted by an algorithm in the future should have the ability to contest. 5.1.3 Who is accountable. Numerous different parties could be held responsible for decisions made using AI, including those deploying the systems, or the developers or designers of the system. A number of submissions suggest that the entities that can influence a decision should be accountable for the decision. “Accountability needs to land with the right people or institutions - the people or institutions with the power to do something about the problem, or offer compensation or remediation.” (Professor Kimberlee Weatherall, Libby Young, Dr. Theresa Dirndorfer Anderson, Associate Professor Julia Powles) 5.1.4 Type of review. As described in Section 4.3, suggestions for the type of review that should be afforded are heavily influenced by existing processes for reviewing human decision-making including internal (complaints mechanism, internal review) and external review mechanisms (ombudsman, independent regulator, court). Internal review processes are seen as practical because the entity deploying a system is likely to be in the best position to reverse that decision or provide redress. However, Adobe highlights that a requirement for internal review could be burdensome on small businesses, which have less resources to manage review processes compared to large businesses. Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 13 Submissions suggesting ‘external review’ mechanisms, often highlight the importance of ‘independent’ review mechanisms where a different party or organisation to the decision maker reviews the decision. A number of submissions suggest the use of judicial review, which is an independent external review mechanism. However, the following disadvantages of going to court are also highlighted: expense, inefficient processes, reliance on underfunded and overworked legal aid organisations, and increased pressure on a strained system. Only a couple of submissions consider how a decision could be contested within the AI system itself, or using another AI system, as described in Section 4.3. 5.2 Process requirements 5.2.1 Steps in the process. Submissions describe a number of steps that need to be taken in order to contest a decision. First, a decision needs to be made. Second, in order to decide whether to contest the decision, a person needs to know that a decision has been made and that there is a contestation process. They will also need some information about the decision—an explanation—to assess whether contestation is required. Once a person has chosen to contest a decision, there needs to be a pathway to contest that enables access to the review process. None of the submissions provide detail on what this pathway might look like, for example whether the decision subject could request a review via a computer interface, or whether a more traditional process should be provided, such as lodging a request for review via letter. All that is apparent from the submissions is that whatever the process, it must be clear. Finally, a review of the decision will need to be undertaken. As detailed above, in Section 5.1, there is no consensus on what a review process should look like, or who should manage this process. Once a review has been completed, a new decision or outcome will be made, which again, will need to be provided to a decision subject, and so the process will continue until there are no further opportunities to contest. 5.2.2 Explanation. The provision of an explanation is not only seen as complementary to contestability, but as a necessary step that enables a person to contest a decision. A number of submissions highlight the link between contestability and explanation. An explanation helps people to understand a decision and allows experts to verify a system’s reasoning. Explanations also provide grounds upon which to contest. Many submissions highlight that the opacity, or ‘black box’ nature, of AI decision making systems is a major challenge for contestability because it makes it difficult to understand why a decision was made. Thus, the submissions see the ability to explain machine decisions—explainability—to be important. “...[I]n order for individuals to contest an algorithm...decision-making processes should be explicable.” (Office of the Victorian Information Commissioner) In terms of what needs to be included in an explanation, the submissions are scant on detail. However, a number of submissions note that an explanation of the decision-making processes should be provided. The submission from Microsoft provides more detail, suggesting the need for different explanations depending on the stakeholder. “In terms of transparency, rather than focusing on the algorithm, it would be more useful to pursue a contextual explanation geared to the needs of the particular stakeholders, which may require explanation of one or more of a number of elements of a system. For example, in some cases, a stakeholder may require an explanation of why the resulting model produced a particular output or prediction, or the ways in which the broader system used that output.” (Microsoft) Manuscript submitted to ACM 14 Henrietta Lyons, Eduardo Velloso, & Tim Miller 5.3 Design requirements 5.3.1 Accessibility. Submissions emphasise the need for the process to contest to be accessible. The Discussion Paper, in its descriptor for contestability, proposed that the process to contest be ‘efficient’. Submissions suggest that contestation and redress have the potential to become complex, drawn out processes that could cause further harm to people, so efficient processes are seen as important. “If...there are a flurry of objections raised by concerned individuals, would this stymie the governance chain? Would it block up decision-making processes? Would redressal be long and drawn out causing hardship to the individuals concerned?” (Values in Defence and Security Technology Group, University of New South Wales) However, many submissions suggest that more than efficiency is required, highlighting the importance of accessibility: the process needs to be clear, easy to access, and affordable. A couple of submissions suggest that support should be provided by the government to decision subjects to access contestation. Having appeal avenues that are easy to access is seen as a way to support decision subjects who lack power in comparison to decision makers. A number of submissions also view accessibility as a way to ensure equality between decision subjects, recognising that some individuals may be less able than others to contest a decision made using AI due to a lack of education or a lack of resources, such as money and time. “Principle 7 on contestability is very necessary...a commitment to facilitating and resourcing the process is crucial to ensure that all individuals have an equal opportunity to benefit from the principle.” (Wendy Rogers, Macquarie University) Numerous submissions also highlight the need for general education about AI to empower individuals to contest. Again, submissions suggest that it is the role of the government to provide this base level of education. “We recommend a government program to uplift the public understanding of AI to ensure: people understand the concept of a probabilistic system and how it may impact them; people understand their rights in terms of contestability where they are harmed by AI; and people are able to identify when their rights are being infringed upon.” (KPMG) 5.3.2 Consistent with legal requirements. In many contexts legislation already exists that provides people who have been affected by decisions with avenues for review. For example, decisions made by the public service that impact individuals is regulated in Australia, and decisions are usually reviewable. Similarly, appeal mechanisms are generally available for decisions made by judges. Existing contestation processes and legislation will influence the type of review available, and potentially who is accountable for a decision, who can contest, and what can be contested. Contestability is discussed in the submissions in relation to specific areas of law, including administrative law, fair work legislation, consumer protection regulations, medical law, human rights law, and criminal law. Submissions also highlight broader legal principles, such as procedural fairness and due process that apply in a number of contexts. “[I]t is essential that the process for challenging harmful algorithmic impacts not only be efficient but also consistent with broader equitable frameworks such as Due Process.” (AI Now Institute) 5.3.3 The context of the initial decision. Submissions suggest that how the initial decision is made will have an impact on the design of the contestation process. For example, academics, Matthew Thomas and Katie Miller suggest that the Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 15 power wielded by the government, coupled with an individual’s inability to disengage, call for a higher standard to apply to government decisions. Additionally, when referring to AI decisions some submissions imply automated decisions while others contemplate human-in-the-loop decisions. A couple of submissions suggest that the same protections will be required regardless of how the decision was made. “...[W]hat does it matter if the decision was made by a person or an algorithm? It is the basis of the decision that is more important, and the ability to appeal it that is more significant.” (The University of Queensland, Centre for Policy Futures) 6 DISCUSSION The importance of the ability to contest algorithmic decision-making has been highlighted in numerous ethical AI documents as well as in academic research. However, what contestability means in this context and how it should operate is under-researched. With the aim to build a more nuanced understanding of contestability in algorithmic decision-making, we analysed submissions made in response to Australia’s AI Ethics Framework Discussion Paper. Our analysis elucidated three key themes in relation to the conceptualisation of contestability: (1) its nature is disputed; (2) it’s a way to protect individuals; and (3) it resembles contestability in relation to human decision-making. Interestingly, a majority of the submissions viewed the ability to contest as a post-hoc mechanism such as an appeal process, rather than a mechanism that can be designed into an AI system. In relation to the operationalisation of contestability, submissions outline a need to clarify preliminary policy questions, such as ‘what can be contested?’; provide a process that includes notification of a decision and an explanation from which grounds to contest can be sourced; and ensure that the process provided is accessible, in line with relevant legislation, and context-aware. In the discussion below, we reflect on these findings, using them as a trigger for highlighting a range of challenges and opportunities that designing for contestability in algorithmic decision-making presents. 6.1 What is contestability really? The lack of consensus as to the nature of contestability is not limited to these submissions, but can be seen in ethical AI guidelines more broadly. In their review of 84 AI ethical guidelines, Jobin et al [32] cite at least 15 guidelines that classify contestability, redress and remedy variously, sometimes as standalone principles [23, 69] and sometimes as mechanisms to action principles including transparency [3, 25, 57], accountability [3, 27, 50, 51, 56], empowerment of the individual [18], explicability [57], and fairness [50]. Perhaps what can be surmised from this lack of consensus is that contestability is important for many reasons: it is a way to hold decision makers to account; it is a way to provide procedural fairness to decision subjects; it upholds the values of dignity and autonomy, and as such, is an important mechanism in democratic societies. Further research could investigate whether targeting one of these goals specifically impacts how the contestation process is designed. From a practical perspective, having contestability as a standalone principle highlights its importance and makes it difficult for a decision maker utilising an algorithmic system to selectively choose whether to provide the ability to contest. As a dimension of, or mechanism that supports, other principles, the ability to contest could be replaced by other tools that allow decision makers to satisfy the principles. For example, providing an explanation may satisfy the principle of transparency with contestability becoming an optional requirement. Manuscript submitted to ACM 16 Henrietta Lyons, Eduardo Velloso, & Tim Miller 6.2 Problems with seeing contestability as protection Noting the power that decision makers wield when they make decisions that significantly impact individuals’ lives, many of the submissions view contestability as a way to protect decision subjects. Yet, at the same time, there is acknowledgement that contesting places a burden on the decision subject: the onus is on the individual to pursue an appeal. If viewed solely as a form of protection, we must consider how effective contestation will be. As the Gradient Institute points out in its submission: “It is important to recognise that notions of contestability, recourse and redress qualify merely as opportunities that can be given to people so they can exercise their autonomy for their own benefit. Some people have less knowledge, resources or time than others and most certainly there will be significant differences in the extent to which people leverage those opportunities.” Algorithmic decision-making, particularly by governments, often disproportionately impacts vulnerable members of society [22]. As an example, over a period of four years, Australia used an automated system to detect and automatically raise “debts” by matching data relating to welfare payments to tax returns (now colloquially referred to as ‘robo-debt’ in Australia). The key legal issue with the system was that it reversed the onus of proving a debt [14]. This onus rests on the government, but the debt letters pressed individuals to prove that they did not owe a debt. Impacted individuals had the ability to seek review of the debt notice through Australia’s administrative appeals tribunal. Yet, despite decisions being made in this tribunal to overturn debts (based on systemic issues) [14] and the Australian public generally being dissatisfied with its use, the system continued to be used. Only after a class action was launched did the Australian Government concede that the system was unlawful and agree to refund debt payments to individuals. In this case, the ability to contest individual decisions helped some individuals overturn the decision, but the system was allowed to remain in place, placing financial pressure on thousands of Australia’s most vulnerable people, many of whom did not contest the debt notice. Contestability can act as a protection, but it should be considered to be the last line of defence. A better approach to protecting individuals is to ensure that from its very conception, a system is designed lawfully and in consultation with those it will impact. Almada’s ‘contestability by design’ concept highlights the importance of participatory design and emphasises preventing harm rather than simply redressing it [5]. The submissions also highlight the need for additional mechanisms that provide “protection” such as independent oversight, auditing and monitoring of AI systems. It is important that contestability is not used as an excuse by governments and organisations to avoid designing systems with due care: the AI Now Institute [54] highlights how governments have used the existence of an appeals process “to resist oversight and reform, claiming that standard appeals are enough to correct any errors.” While contestation can be a safeguard and a way to hold decision-makers to account, it can also be thought of as a way to exercise autonomy by allowing individuals to challenge decisions that significantly impact them: a right rather than a protection. This framing is consistent with the purpose behind the GDPR’s Article 22 and its predecessor Article 15 of the 1995 Data Protective Directive, which limits the use of solely automated decision-making: “The primary catalyst for Art. 15 was the potential weakening of the ability of persons to exercise influence over decision-making processes that significantly affect them, in light of the growth of automated profiling practices\" [41]. 6.3 ‘Humanity-fetishism’? The focus on existing processes and human review Our qualitative analysis indicates that contestability in algorithmic decision-making is conceptualised in the same way as contestability in relation to human decisions. Submissions suggest the use of contestation processes that already exist in relation to human decision-making such as complaints mechanisms, ombudsman, administrative review bodies, Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 17 and courts. Few submissions consider whether existing contestation methods, including human review, will be fit for purpose in relation to decisions made using algorithmic systems. Existing contestation processes have been designed to review human decision-making and to address human errors and biases. More thought needs to be given to whether these existing contestation processes provide the best review mechanisms given the unique challenges algorithmic decision-making faces, such as opacity. There are a number of ways that this preference for existing processes can be interpreted. For instance, perhaps these are the best processes for contestation relating to algorithmic decisions. Alternatively, this could be an example of the mere exposure effect, which is where people develop a preference for things they are exposed to [72]. Or potentially submissions are falling back on known options because they cannot imagine other ways of providing for contestability; their thinking is anchored by processes that already exist [45, 63]. The submissions also emphasise the need for human review. Providing human review of decisions made using algorithms that significantly impact a person’s life, is arguably a way to “uphold human dignity” [5, 29, 41]. Human review can be seen as a quality control mechanism: humans are needed to ensure that the algorithmic models being used are functioning properly and have not made any errors, especially because computers cannot (currently) solve problems within their own systems [5, 12]. Underlying the call of human review may also be a lack of trust in the decisions made using algorithmic systems. Arguably, the submissions display a degree of ‘humanity-fetishism’; where human decision makers are viewed as unique and special [12]. Only a few submissions note that bringing humans into the review process can introduce problems such as human bias, an inability to scale review, and increased review time and cost. At a practical level, it is important to consider how human review of algorithmic decisions would work. For example, what would be reviewed? It is clear that the input data could be checked, but what else? The functioning of the algorithm? Or, would a completely new decision be made with an independent system? If a new decision, what decision rules should be followed? Would the human reviewer be able to rely on information taken into account for the algorithmic decision? These are policy decisions that the decision-maker will need to address. There is also scope for the use of technology in the review process. Almada suggests that the review process could involve automation, for example by utilising a “trusted third party algorithm to automatically review a decision” [5]. This type of review could consider whether decision rules were followed [36], while protecting proprietary rights because it does not require ‘opening the black box’. Alternatively, building on the work investigating contestability as a design objective, algorithmic systems that enable decision subjects to interact with them via an interface could be developed [5, 30, 35, 46]. These technological review processes will likely face the same criticisms raised in relation to decisions made using algorithms in the first instance relating to individualised justice, dignity, autonomy, and fairness. 6.4 The opacity of decision-making The submissions highlight opacity as a key challenge in designing for contestability. Many algorithmic decision-making systems now rely on advanced computational techniques, such as deep learning. The decision rules and inputs of these “black box” systems are often obscured due to their technical complexity or are only interpretable by experts, if at all [13]. Additionally, the workings of algorithmic decision-making systems may be intentionally hidden from view by the developers and those deploying these systems to protect proprietary interests [13]. Opacity, in its various forms, results in limited information being provided to decision subjects, which makes it difficult to understand why a decision has been made, and consequently, to contest it in any meaningful way [1, 13, 55, 62, 67]. Veale [62] highlights this difficulty: “A loan applicant denied credit by a credit-scoring [machine learning] algorithm Manuscript submitted to ACM 18 Henrietta Lyons, Eduardo Velloso, & Tim Miller cannot easily understand if her data was wrongly entered, or what she can do to have a greater chance of acceptance in the future, let alone prove the system is illegally discriminating against her (perhaps based on race, sex, or age).” Arguably, opacity is not unique to algorithmic decision-making systems, but is also a challenge with human decision- making. As Kaminski [33] states “[h]uman decision-making can be deeply, terribly flawed. Human decision makers can be outright discriminatory; can hold deep-seated biases about race, gender, or class; and can exhibit a host of cognitive biases that invisibly influence outcomes”. Zerilli [73] also notes that even judges, the “most esteemed reasoners”, are subject to prejudice and can make poor decisions. However, there has been much effort made to understand how humans make decisions, what might go wrong, and to devise systems to meet the challenges of bias and opacity in human decision-making. For example, significant decisions are often guided by written rules that clearly set out decision criteria; decision-makers are frequently required to attend training sessions to learn about unconscious bias and how to make better decisions; and significant decision-making is often conducted by panels to increase objectivity. As Waldman [67] suggests “‘big data’–powered algorithms and machine learning are just as prone to mistakes, biases, and arbitrariness as their human counterparts.” Yet, people often view technology as more objective and accurate than humans, which “shields algorithmic systems from critical interrogation” [67]. The impact of this is that people may not even contemplate that a decision is wrong, and so will not contest. In comparison to human decision-making, we are still in the process of uncovering the problems that can be associated with using algorithmic decision-making, like embedded bias [7], adaptivity [8], and spurious correlation [8, 24]. Further, the ability to use these systems that make decisions at scale presents new issues for contestation and redress. For example, how will humans reviewers keep up with a high demand for review? 6.5 The relationship between explainability and contestability Submissions highlight the need for an explanation to be provided about a decision. Many scholars have also noted the importance of an explanation in determining whether the algorithmic decision was justified (e.g. [71]), and to provide grounds for review (e.g. [44, 66, 73]). As the submissions point out, the opacity of many algorithmic systems is a major challenge for understanding how a decision was made. The field of explainable AI (XAI), seeks to distil explanations from these “black box” systems. Research in XAI has begun to focus more on human-centred approaches to developing explanations [2, 4, 42, 43, 68]. However, these works have not focused on providing explanations specifically for contestation, with the exception of Wachter et al [66]. There is limited discussion in submissions about what an explanation used for contestation should include. Research is needed to understand explanation requirements in this context. Guidelines would be particularly useful here because there is a difference between providing an explanation in relation to a specific outcome and justifying why that decision was made [9]. Further, there may be a difference between what a decision maker and decision subject see as a useful justification for the decision [9]. Another unanswered question is whether an explanation produced to initially explain a decision and help a decision subject decide whether to contest is sufficient for the review process, or whether further explanation is required. Something to be cautious of is relying on the basic and often technical explanations that are currently producible by XAI because these will limit the grounds for contestation [49]. For example, counterfactual explanations are unlikely to provide enough information to be able to contest a decision in court [49]. In addition, the information provided in an explanation, and the way it is provided, can in theory be manipulated to decrease the likelihood that a person will contest a decision [44]. Despite these challenges, we suggest that explainability can be viewed as an enabler for Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 19 contestability. Contestability requires an explanation, and ensuring that the decisions and outputs of systems are explainable is a step in the right direction. 6.6 Designing contestation processes: The role of HCI practitioners Decision-making occurs in many different contexts, so there will be no one-size fits all contestation process. For example, as the submissions highlight, any existing legal requirements will need to be considered in the design of the process. Further, many of the parameters of the contestation process, such as what can be contested and the type of review that can be offered, are policy questions. These questions need to be answered either by government through the provision of regulation or guidelines, or in the absence of such guidance, by decision makers. So, while our qualitative analysis provides triggers for discussion and a minimum range of conceptual issues to consider, decision-makers will need to design their contestation processes to suit their specific needs and by taking into account the views of the people impacted by the decision. There is a role for HCI practitioners here, to design novel contestation experiences that are informed by human-centred values and practices. The HCI community is well placed to conduct research that can be used by government to inform these policy decisions [37, 58, 70] or to assist decision makers to make these choices [70]. 7 CONCLUSION Algorithmic decision-making systems are increasingly being used to make significant decisions. The need for these systems to be developed and deployed in an ‘ethical’ and responsible way is pressing. One safeguard being proposed is the ability to contest algorithmic decisions, ‘contestability’. Our work contributes to the emerging body of work considering contestability by exploring the views of organisations and individuals that made submissions responding to Australia’s AI Ethics Framework, the first framework of its kind to propose ‘contestability’ as a core principle. Through a qualitative analysis and a reflective discussion we outline a range of conceptual debates around contestability that can serve as a trigger for further discussion. We reflect on whether contestability offers the protection submissions envision, whether we should view contestability in algorithmic decision-making differently to contestability in human decision-making, and how opacity can hinder a person’s ability to contest an algorithmic decision. ACKNOWLEDGMENTS We would like to thank our reviewers for their valuable feedback on previous versions of this paper. This research was partly funded by Australian Research Council Discovery Grant DP190103414. Henrietta Lyons is supported by the Faculty of Engineering and Information Technology Ingenium scholarship program. Eduardo Velloso is the recipient of an Australian Research Council Discovery Early Career Researcher Award (Project Number: DE180100315) funded by the Australian Government. REFERENCES [1] 2017. Houston Federation of Teachers, Local 2415, et al v Houston Independent School District, 251 F.Supp.3d 116. [2] Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y Lim, and Mohan Kankanhalli. 2018. Trends and trajectories for explainable, accountable and intelligible systems: An HCI research agenda. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, Article 582. [3] ACM. 2017. Statement on Algorithmic Transparency and Accountability. https://www.acm.org/binaries/content/assets/public-policy/2017_usacm_ statement_algorithms.pdf [4] Amina Adadi and Mohammed Berrada. 2018. Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI). IEEE Access 6 (2018), 52138–52160. https://doi.org/10.1109/ACCESS.2018.2870052 [5] Marco Almada. 2019. Human Intervention in Automated Decision-making: Toward the Construction of Contestable Systems. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law (ICAIL ’19). 2–11. Manuscript submitted to ACM 20 Henrietta Lyons, Eduardo Velloso, & Tim Miller [6] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014. Power to the People: The Role of Humans in Interactive Machine Learning. AI Magazine 35, 4 (Dec. 2014), 105–120. https://doi.org/10.1609/aimag.v35i4.2513 [7] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine Bias. https://www.propublica.org/article/machine-bias-risk- assessments-in-criminal-sentencing [8] Emre Bayamlıoğlu. 2018. Contesting Automated Decisions. European Data Protection Law Review 4 (2018), 433–446. [9] Reuben Binns. 2018. Algorithmic accountability and public reason. Philosophy & Technology 31, 4 (2018), 543–556. [10] Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. “It’s Reducing a Human Being to a Percentage”: Perceptions of Justice in Algorithmic Decisions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI ’18). ACM, 1–14. https://doi.org/10.1145/3173574.3173951 [11] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative Research in Psychology 3 (2006), 77–101. [12] Kiel Brennan-Marquez and Stephen Henderson. 2019. Artificial Intelligence and Role-Reversible Judgment. Journal of Criminal Law and Criminology 109, 1, Article 2 (2019), 27 pages. [13] Jenna Burrell. 2016. How the machine ‘thinks’: Understanding opacity in machine learning algorithms. Big Data and Society (2016), 1–12. [14] Terry Carney. 2019. Robo-debt illegality: The seven veils of failed guarantees of the rule of law? Alternative Law Journal 44, 1 (2019), 4–10. [15] Rich Caruana, Mohamed Elhawary, Nam Nguyen, and Casey Smith. 2006. Meta clustering. In Sixth International Conference on Data Mining. IEEE, 107–118. [16] Danielle Keats Citron and Frank Pasquale. 2014. The scored society: Due process for automated predictions. Wash. L. Rev. 89 (2014), 1. [17] David Cohn, Rich Caruana, and Andrew McCallum. 2003. Semi-supervised clustering with user feedback. Constrained Clustering: Advances in Algorithms, Theory, and Applications 4, 1 (2003), 17–32. [18] European Data Protection Supervisor & Garante per la protezione dei dati personali Commission Nationale de l’Informatique et des Libertés. 2018. Declaration on Ethics and Data Protection in Artificial Intelligence. https://edps.europa.eu/sites/edp/files/publication/icdppc-40th_ai- declaration_adopted_en_0.pdf [19] D Dawson, E Schleiger, J Horton, J McLaughlin, C Robinson, G Quezada, J Scowcroft J, and S Hajkowicz. 2019. Artificial Intelligence: Australia’s Ethics Framework. Data61 CSIRO, Australia (2019). https://consult.industry.gov.au/strategic-policy/artificial-intelligence-ethics-framework/ supporting_documents/ArtificialIntelligenceethicsframeworkdiscussionpaper.pdf [20] Virginia Dignum. 2019. Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way. Springer. [21] Jonathan Dodge, Q. Vera Liao, Yunfeng Zhang, Rachel K. E. Bellamy, and Casey Dugan. 2019. Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment. Proceedings of the 24th International Conference on Intelligent User Interfaces (2019). http://arxiv.org/abs/ 1901.07694 [22] Virginia Eubanks. 2018. Automating Inequality: How High-Tech Tools Profile, Police and Punish the Poor. St Martin’s Publishing Group, Hillsdale, NJ. [23] World Economic Forum. 2018. White Paper: How to Prevent Discriminatory Outcomes in Machine Learning. http://www3.weforum.org/docs/ WEF_40065_White_Paper_How_to_Prevent_Discriminatory_Outcomes_in_Machine_Learning.pdf [24] Oscar H Gandy. 2010. Engaging rational discrimination: exploring reasons for placing regulatory constraints on decision support systems. Ethics and Information Technology 12, 1 (2010), 29–42. [25] UNI Global. 2017. Top 10 Principles for Ethical Artificial Intelligence. http://www.thefutureworldofwork.org/media/35420/uni_ethical_ai.pdf [26] GA Gobry. 1973. Computer-assisted clinical decision-making. Methods of Information in Medicine 12, 01 (1973), 45–51. [27] Google. 2017. Artificial Intelligence and Machine Learning: Policy Paper. https://www.internetsociety.org/resources/doc/2017/artificial-intelligence- and-machine-learning-policy-paper/ [28] Nina Grgić-Hlača, Elissa M. Redmiles, Krishna P. Gummadi, and Adrian Weller. 2018. Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction. In Proceedings of the 2018 World Wide Web Conference (WWW ’18). International World Wide Web Conferences Steering Committee, 903–912. https://doi.org/10.1145/3178876.3186138 [29] Mireille Hildebrandt. 2019. Privacy as protection of the incomputable self: From agnostic to agonistic machine learning. Theoretical Inquiries in Law 20, 1 (2019), 83–121. [30] Tad Hirsch, Kritzia Merced, Shrikanth Narayanan, Zac E. Imel, and David C. Atkins. 2017. Designing Contestability: Interaction Design, Machine Learning, and Mental Health. In Proceedings of the 2017 Conference on Designing Interactive Systems. ACM, 95–99. [31] AI Now Institute. 2018. Litigating Algorithms: Challenging Government Use of Algorithmic Decision Systems. https://ainowinstitute.org/litigatingalgorithms.pdf. [32] Anna Jobin, Marcello Ienca, and Effy Vayena. 2019. The global landscape of AI ethics guidelines. Nature Machine Intelligence 1 (2019), 389–399. [33] Margot Kaminski. 2019. Binary Governance: Lessons from the GDPR’s Approach to Algorithmic Accountability. Southern California Law Review 92, 6 (2019), 1529–1616. [34] Olga Kharif. 2016. No Credit History? No Problem. Lenders Are Looking at Your Phone Data. Retrieved January 31, 2020 from https://www.bloomberg.com/news/articles/2016-11-25/no-credit-history-no-problem-lenders-now-peering-at-phone-data. [35] Daniel Kluttz and Deirdre K Mulligan. 2020. Automated decision support technologies and the legal profession. Berkeley Technology Law Journal 34, 3 (2020). [36] Joshua A Kroll, Joanna Huey, Solon Barocas, Edward W Felten, Joel R Reidenberg, David G Robinson, and Harlan Yu. 2017. Accountable algorithms. University of Pennsylvania Law Review 165 (2017), 633. Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 21 [37] Jonathan Lazar, Julio Abascal, Simone Barbosa, Jeremy Barksdale, Batya Friedman, Jens Grossklags, Jan Gulliksen, Jeff Johnson, Tom McEwan, Loïc Martínez-Normand, Wibke Michalk, Janice Tsai, Gerrit van der Veer, Hans von Axelson, Ake Walldius, Gill Whitney, Marco Winckler, Volker Wulf, Elizabeth F. Churchill, Lorrie Cranor, Janet Davis, Alan Hedge, Harry Hochheiser, Juan Pablo Hourcade, Clayton Lewis, Lisa Nathan, Fabio Paterno, Blake Reid, Whitney Quesenbery, Ted Selker, and Brian Wentz. 2016. Human–Computer Interaction and International Public Policymaking: A Framework for Understanding and Taking Future Actions. Found. Trends Hum.-Comput. Interact. 9, 2 (May 2016), 69–149. https://doi.org/10.1561/1100000062 [38] Min Kyung Lee, Anuraag Jain, Hea Jin Cha, Shashank Ojha, and Daniel Kusbit. 2019. Procedural Justice in Algorithmic Fairness: Leveraging Transparency and Outcome Control for Fair Algorithmic Mediation. Proceedings ACM Hum.-Comput. Interact. 3, CSCW, Article 182 (Nov. 2019), 26 pages. https://doi.org/10.1145/3359284 [39] Gianclaudio Malgieri. 2019. Automated decision-making in the EU Member States: The right to explanation and other “suitable safeguards” in the national legislations. Computer Law & Security Review 35, 5 (2019), 105327. [40] Berta Melder. 2018. The Role of Artificial Intelligence (AI) in Recruitment. https://www.talentlyft.com/en/blog/article/207/the-role-of-artificial- intelligence-ai-in-recruitment [41] Isak Mendoza and Lee A. Bygrave. 2017. The Right not to be Subject to Automated Decisions based on Profiling. In EU Internet Law: Regulation and Enforcement, Tatiani Synodinou, Philippe Jougleux, Christiana Markou, and Thalia Prastitou (Eds.). Springer. [42] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence 267 (2019), 1–38. [43] Tim Miller, Piers Howe, and Liz Sonenberg. 2017. Explainable AI: Beware of Inmates Running the Asylum. In Proceedings of the 2017 IJCAI Workshop on Explainable Artificial Intelligence (XAI) (IJCAI ’17). 36–42. http://people.eng.unimelb.edu.au/tmiller/pubs/explanation-inmates.pdf [44] Brent Mittelstadt. 2019. Principles alone cannot guarantee ethical AI. Nature Machine Intelligence (2019), 1–7. [45] Meredith Ringel Morris, Andreea Danielescu, Steven Drucker, Danyel Fisher, Bongshin Lee, m. c. schraefel, and Jacob O. Wobbrock. 2014. Reducing Legacy Bias in Gesture Elicitation Studies. Interactions 21, 3 (May 2014), 40–45. https://doi.org/10.1145/2591689 [46] Deirdre K. Mulligan, Daniel Kluttz, and Nitin Kohli. (Forthcoming 2020). Shaping Our Tools: Contestability as a Means to Promote Responsible Algorithmic Decision Making in the Professions. In After the Digital Tornado: Networks, Algorithms, Humanity, Kevin Werbach (Ed.). New York: Cambridge University Press. [47] Sarah Myers West. 2018. Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms. New Media & Society 2, 11 (2018), 4366–4383. [48] Australian Government Department of Industry Innovation and Science. 2019. AI Ethics Framework. https://www.industry.gov.au/data-and- publications/building-australias-artificial-intelligence-capability/ai-ethics-framework [49] Office of the Victorian Information Commissioner. 2019. Closer to the machine. https://ovic.vic.gov.au/wp-content/uploads/2019/08/closer-to-the- machine-web.pdf [50] Independent High-Level Expert Group on Artificial Intelligence. 2019. Ethics Guidelines for Trustworthy AI. https://ec.europa.eu/digital-single- market/en/news/ethics-guidelines-trustworthy-ai [51] European Group on Ethics in Science and New Technologies. 2018. Statement on Artificial Intelligence, Robotics and ‘Autonomous’ Systems. http://ec.europa.eu/research/ege/pdf/ege_ai_statement_2018.pdf [52] Article 29 Data Protection Working Party. 2017. Guidelines on Automated individual decision-making and Profiling for the purposes of Regulation 2016/679. https://ec.europa.eu/newsroom/article29/item-detail.cfm?item_id=612053 [53] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “Why Should I Trust You?\": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’16). ACM, 1135–1144. [54] Rashida Richardson, Jason M. Schultz, and Vincent M. Southerland. 2019. Litigating Algorithms 2019 US Report: New Challenges to Government Use of Algorithmic Decision Systems (AI Now Institute). https://ainowinstitute.org/litigatingalgorithms-2019-us.pdf. [55] Claudio Sarra. 2020. Put Dialectics into the Machine: Protection against Automatic-decision-making through a Deeper Understanding of Contestability by Design. Global Jurist Published online ahead of print (2020). [56] Internet Society. 2017. Artificial Intelligence and Machine Learning: Policy Paper. https://www.internetsociety.org/resources/doc/2017/artificial- intelligence-and-machine-learning-policy-paper/ [57] Software and Information Industry Association. 2017. Ethical Principles for Artificial Intelligence and Data Analytics. https://www.siia.net/Press/SIIA- Releases-Ethical-Principles-for-Artificial-Intelligence-and-Data-Analytics-with-Support-from-the-Future-of-Privacy-Forum-and-the- Information-Accountability-Foundation [58] Vanessa Thomas, Christian Remy, Mike Hazas, and Oliver Bates. 2017. HCI and Environmental Public Policy: Opportunities for Engagement. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA) (CHI ’17). Association for Computing Machinery, New York, NY, USA, 6986–6992. https://doi.org/10.1145/3025453.3025579 [59] Andrea Aler Tubella, Andreas Theodorou, Virginia Dignum, and Loizos Michael. 2020. Contestable Black Boxes. arXiv:2006.05133 [cs.AI] [60] Berk Ustun, Alexander Spangher, and Yang Liu. 2019. Actionable Recourse in Linear Classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* ’19). Association for Computing Machinery, New York, NY, USA, 10–19. https: //doi.org/10.1145/3287560.3287566 [61] Kristen Vaccaro, Karrie Karahalios, Deirdre K. Mulligan, Daniel Kluttz, and Tad Hirsch. 2019. Contestability in Algorithmic Systems. In Conference Companion Publication of the 2019 on Computer Supported Cooperative Work and Social Computing (Austin, TX, USA). ACM, 523–527. https: Manuscript submitted to ACM 22 Henrietta Lyons, Eduardo Velloso, & Tim Miller //doi.org/10.1145/3311957.3359435 [62] Michael Veale, Max Van Kleek, and Reuben Binns. 2018. Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI ’18). Association for Computing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/3173574.3174014 [63] Eduardo Velloso, Tilman Dingler, Frank Vetere, Sam Horman, Harriet McDougall, and Kasia Mierzejewska. 2018. Challenges of Emerging Technologies for Human-Centred Design: Bridging the Gap between Inquiry and Invention. In Proceedings of the 30th Australian Conference on Computer-Human Interaction (Melbourne, Australia) (OzCHI ’18). Association for Computing Machinery, New York, NY, USA, 609–611. https: //doi.org/10.1145/3292147.3293451 [64] Suresh Venkatasubramanian and Mark Alfano. 2020. The Philosophical Basis of Algorithmic Recourse. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* ’20). Association for Computing Machinery, New York, NY, USA, 284–293. https://doi.org/10.1145/3351095.3372876 [65] Jesse Vig, Shilad Sen, and John Riedl. 2011. Navigating the Tag Genome. In Proceedings of the 16th International Conference on Intelligent User Interfaces (Palo Alto, CA, USA) (IUI ’11). Association for Computing Machinery, New York, NY, USA, 93–102. https://doi.org/10.1145/1943403.1943418 [66] Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2018. Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. Harvard Journal of Law and Technology 31, 2 (2018), 841–887. [67] Ari Ezra Waldman. 2019. Power, Process, and Automated Decision-Making. Fordham L. Rev. 88 (2019), 613. [68] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y Lim. 2019. Designing Theory-Driven User-Centric Explainable AI. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, Article 601, 15 pages. [69] Human Rights Watch. 2018. The Toronto declaration: protecting the right to equality and non-discrimination in machine learning systems. https://www.hrw.org/news/2018/07/03/toronto-declaration-protecting-rights-equality-and-non-discrimination-machine [70] Allison Woodruff, Sarah E. Fox, Steven Rousso-Schindler, and Jeffrey Warshaw. 2018. A Qualitative Exploration of Perceptions of Algorithmic Fairness. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI ’18). Association for Computing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/3173574.3174230 [71] Karen Yeung and Adrian Weller. 2018. How is ‘Transparency’ Understood by Legal Scholars and the Machine Learning Community. In Being Profiled, Emre Bayamlioglu, Irina Baraliuc, Lissa Janssens, and Mireille Hildebrandt (Eds.). Amsterdam University Press. [72] Robert B Zajonc. 1968. Attitudinal effects of mere exposure. Journal of Personality and Social Psychology 9, 2 (1968), 1–27. [73] John Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. 2019. Transparency in algorithmic and human decision-making: Is there a double standard? Philosophy & Technology 32, 4 (2019), 661–683. Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 23 A APPENDIX A.1 Core Principles for AI [19] Core Principles for AI Principle 1 Generates net-benefits The AI system must generate benefits for people that are greater than the costs. Principle 2 Do no harm Civilian AI systems must not be designed to harm or deceive people and should be implemented in ways that minimise any negative outcomes. Principle 3 Regulatory and legal com- pliance The AI system must comply with all relevant international, Australian local, State/Territory and Federal government obligations, regulations and laws. Principle 4 Privacy protection Any system, including AI systems, must ensure people’s private data is protected and kept confidential plus prevent data breaches which could cause reputational, psychologi- cal, financial, professional or other types of harm. Principle 5 Fairness The development or the use of the AI system must not result in unfair discrimination against individuals, communities or groups. This requires particular attention to ensure the “training data” is free from bias or characteristics which may cause the algorithm to behave unfairly. Principle 6 Transparency and Explain- ability People must be informed when an algorithm is being used that impacts them and they should be provided with infor- mation about what information the algorithm uses to make decisions. Principle 7 Contestability When an algorithm impacts a person there must be an efficient process to allow that person to challenge the use or output of the algorithm. Principle 8 Accountability People and organisations responsible for the creation and implementation of AI algorithms should be identifiable and accountable for the impacts of that algorithm, even if the impacts are unintended. Manuscript submitted to ACM 24 Henrietta Lyons, Eduardo Velloso, & Tim Miller A.2 Discussion questions posed by Australia’s Department of Industry, Innovation and Science Discussion Questions Are the principles put forward in the discussion paper the right ones? Is anything missing? Do the principles put forward in the discussion paper sufficiently reflect the values of the Australian public? As an organisation, if you designed or implemented an AI system based on these principles, would this meet the needs of your customers and/or suppliers? What other principles might be required to meet the needs of your customers and/or suppliers? Would the proposed tools enable you or your organisation to implement the core principles for ethical AI? What other tools or support mechanisms would you need to be able to implement principles for ethical AI? Are there already best-practice models that you know of in related fields that can serve as a template to follow in the practical application of ethical AI? Are there additional ethical issues related to AI that have not been raised in the discussion paper? What are they and why are they important? Do you have any further comments? Manuscript submitted to ACM Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions 25 Manuscript submitted to ACM 26 Henrietta Lyons, Eduardo Velloso, & Tim Miller A.3 Authors of the submissions analysed Adobe Systems AI Now Institute Amara Bains, Bains Consulting Anand Tamboli, KNEWRON Technologies Artificial Intelligence Collaborative Network Australasian College of Dermatologists Australian Academy of Health and Medical Sciences Australian Academy of Science and the Australian National University Australian Academy of Technology and Engineering Australian Industry Group Australian Information Security Association Australian Library and Information Association Baker McKenzie BSA The Software Alliance Communications Alliance Consumer Policy Research Centre D2D CRC submission datanomics Deakin University Dr Carolyn McKay, University of Sydney Law School Dr Monika Zalnieriute and Olivia Gould-Fensom Dr. Jolyon Ford Electronic Frontiers Australia Eliot Palmer Free TV Australia Future AI Forum (KPMG) Gradient Institute John Harvey Juxi Leitner Keith Dodds Consulting KPMG Law Council of Australia Matthew Phillipps Matthew Thomas and Katie Miller Maurice Blackburn Microsoft National Australia Bank National Health and Medical Research Coun- cil NSW Young Lawyers Office of the Information Commissioner Queensland Office of the Victorian Information Commis- sioner Otzma Analytics Pty Ltd Pax Republic Professionals Australia Professor Felicity Gerry QC Professor Kimberlee Weatherall, Libby Young, Dr. Theresa Dirndorfer, Anderson, Associate Professor Julia Powles Raymond Sheh, Intelligent Robots Group, Curtin University RMIT University RMIT University - Graduate School of Busi- ness and Law Rod Jamieson Roger Clarke Xamax Consultancy Sarah Kaur Portable Skanda Kallur, GradientRisk Stephen Finch Telstra The Automated Society Working Group, Monash University The Pearcey Institute The Royal Australian and New Zealand Col- lege of Radiologists The University of Queensland, Centre for Policy Futures ThinkPlace ThoughtWorks Australia Toby Lightheart University of Melbourne Values in Defence & Security Technology Group, University of New South Wales Wendy Rogers, Macquarie University Manuscript submitted to ACM","libVersion":"0.0.0","langs":"","hash":"","size":0}