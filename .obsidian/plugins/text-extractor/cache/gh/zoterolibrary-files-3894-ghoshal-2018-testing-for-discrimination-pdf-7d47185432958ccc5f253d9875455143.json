{"path":".obsidian/plugins/text-extractor/cache/gh/zoterolibrary-files-3894-ghoshal-2018-testing-for-discrimination-pdf-7d47185432958ccc5f253d9875455143.json","text":"https://doi.org/10.1177/0092055X18789216 Teaching Sociology 2018, Vol. 46(4) 309 –323 © American Sociological Association 2018 DOI: 10.1177/0092055X18789216 ts.sagepub.com Article In the wake of civil rights advances of the 1960s, researchers developed audit studies as a means of test- ing for hidden discrimination. In these studies, two matched individuals armed with equal qualifications, usually one white and one African American, were sent to apply for the same set of housing or job oppor- tunities. Systematic differences that emerged were suggestive of ongoing discrimination. Numerous in- person audit studies from the 1970s through the early 2000s in the United States and elsewhere uncovered continuing hiring and housing discrimination by race (Daniel 1968; Pager 2007; Yinger 1995) and other characteristics (Neumark, Bank, and Van Nort 1996; Riach and Rich 2002). Though expensive and com- plicated to conduct (Pager 2007), audit studies pro- vided valuable direct evidence of covert unequal treatment that would have remained invisible in sur- veys or opinion polls. In the past 15 years, the movement online of many processes once conducted by mail or tele- phone has dramatically lowered audit studies’ cost and complexity. Now, researchers can develop near-identical resumes or profiles that vary only by one significant trait of interest and use these identi- ties to apply to opportunities online in a fraction of the time it once took in person. As a result, web- based and email-based audits have become a bur- geoning method in sociology. No longer restricted to white/black comparisons or housing and job markets, audit studies have addressed unequal treatment by multiple categories of race, gender, class, sexual orientation, disability status, and more (Gaddis 2018). They have been used to study ille- gal discrimination in the traditional areas of renting apartments and seeking jobs but have addressed 789216 TSOXXX10.1177/0092055X18789216Teaching SociologyGhoshal research-article2018 1Elon University, Elon, NC, USA Corresponding Author: Raj Ghoshal, Sociology & Anthropology Department, Elon University, 2035 Campus Box, Elon, NC 27244, USA. Email: rghoshal@elon.edu Testing for Discrimination: Teaching Audit Studies in Quantitative Methods Courses Raj Ghoshal 1 Abstract This article demonstrates a method for teaching students to conduct audit studies of discrimination. The assignment can be used in courses on quantitative methods, race, gender, or other topics. Audit studies test for unequal treatment by having otherwise identical pairs of people who vary on a single trait, such as race or gender, apply for the same sets of opportunities, such as apartment vacancies or job openings. Once intricate and expensive to conduct, the online shift of the past 15 years has streamlined the approach, enabling researchers to execute audits via email. I show how to lead students through designing and conducting original audit studies. I present evidence that this approach yields significant engagement, builds students’ abilities, and produces excellent work. Keywords quantitative methods, sociological research, data analysis, race and ethnicity, research methods 310 Teaching Sociology 46(4) many other arenas as well, sometimes including arenas in which discrimination is socially harmful but legally permitted or ambiguous (Besbris et al. 2015; Gaddis and Ghoshal 2015). In this article, I present an assignment that teaches students how to conduct online audit studies. The assignment enables students to collect and analyze original, real-world data on discriminatory behavior untainted by social desirability bias yet connected to core sociological concerns. It therefore simultane- ously addresses several best practices for assignments in quantitative research methods courses. The assign- ment might also be used in advanced courses on race, gender, or sexuality as it enables students to study discrimination and better understand how methods and substantive knowledge interrelate (Himes and Caffrey 2003; Sweet and Baker 2011). In the following pages, I first lay out some chal- lenges in teaching quantitative methods. I then argue that an audit studies assignment can power- fully address these challenges. I present the assign- ment, in which students reply to 75 or more housing listings online using two different identities, vary one characteristic such as race or sexual orienta- tion, and analyze differences in response rates to measure unequal treatment. I show that students regard the assignment as engaging and valuable, significantly grow in their ability to understand and produce audit studies, and produce excellent work. EffECTIVE TEAChING Of QUANTITATIVE METhODS Teaching quantitative methods poses some unique challenges and opportunities (Bridges et al. 1998; Markle 2017; Paxton 2006; cf. DeCesare 2007; Wisecup 2017, who argue that faculty overestimate anxiety about the course). On the challenges side, students may feel little emotional connection to a course that does not directly engage substantive topics like courses on race, gender, class, or health do. Students with weaker math backgrounds may find material difficult, and this problem can com- pound as the material scaffolds. And even engaged students may face challenges of relevance (Paxton 2006). Meanwhile, instructors face the challenge of maintaining learning across a wide range of ability levels. While this may be a concern in any course, it is especially likely to arise in courses that involve significant scaffolding. Despite these challenges, quantitative methods classes can be exceptional educational environ- ments. First, most students recognize that quantita- tive proficiency enhances their ability to understand the world (Wilder 2009). In addition, students are likely to see quantitative prowess as a route to interesting and productive employment (Senter 2017). And finally, the course is unlikely to be redundant. It may be the only or one of only two quantitative courses most sociology majors take. By contrast, our majors are likely to discuss core substantive issues repeatedly throughout many courses. For these reasons, quantitative methods courses that offer realistic challenges and help develop valued abilities have the potential to inspire significant intellectual excitement and learning (Bain 2011). Scholarship on methods instruction is clear that approaches in which students do research offer more realistic and valued challenges than in-class testing alone (Persell, Pfeiffer, and Syed 2008; Strangfeld 2013). Beyond this general consensus, existing scholarship suggests three traits of effec- tive methods assignments: They draw on real- world data, address topics students care about, and deeply immerse students in the data they engage. Methods courses whose assignments include all or some of these features will lead students to feel more connected to their work, generate greater motivation, and yield longer term retention of material (McKinney et al. 2004; Wilder 2009; Zull 2002) than those that do not. I address each aspect in turn. First, assignments should involve real-world data. Assignments that involve calculations divorced from real social context are less likely to be “meaningful and memorable” for many students (Steen 2001), and skills developed in such assign- ments may not be retained (Atkinson, Czaja, and Brewster 2006; Godfrey 1988; Steen 2001). Further, not all data will feel equally “real.” For instance, students frequently have questions about the validity, reliability, and accuracy of survey data on topics like sexual behavior, racial attitudes, and voting intentions (Babbie 2015; Bonilla-Silva 2006). Students’ awareness of social desirability bias may be especially high in the wake of an elec- tion in which many people believe, correctly or incorrectly, that this bias distorted pre-election polling (Enten 2016). Conversely, data will feel most compelling when students see it as measuring actual behaviors and conditions. Second, assignments should engage topics that students care about. Many sociology majors have strong interests in race, gender, sexuality, and other axes of inequality that can serve as engaging con- texts for methods assignments. Instructors might also leverage students’ personal connections to Ghoshal 311 their local area or life stage as college students in choosing topics (Lindner 2012; Paxton 2006). However, as students’ interests are not fully pre- dictable, assignments that give students some con- trol to choose the substantive context can generate more buy-in and a greater sense of autonomy (Strangfeld 2013; Whitley and Dietz 2018) than those whose focus is fully dictated in advance. Third, students should know the data intimately and engage it as actively as possible. Many real- world research challenges students encounter do not come with prepackaged data. Further, students are more likely to believe that data reflect reality when they have played a role in collecting it; this may hold especially true for students predisposed to skepticism about official information (Shepard 2017). And the process of data collection may well be interesting to students in ways that simply downloading preexisting data is not. This interest can fuel motivation, a core component of lasting learning (Paxton 2006). Existing methods assignments address these challenges in various ways. For instance, students might analyze real-world data from historic disas- ters such as the Challenger accident (Schumm et al. 2002) or from the General Social Survey or census, sometimes with local or interest-based cus- tomization (Burdette and McLoughlin 2010). Lindner’s (2012) students learn regression through an analysis of their own quiz and exam scores, while Paxton (2006) connects to students’ career interests by using salary data on various careers sociology majors might choose. And many pub- lished exercises ask students to collect original data (Lovekamp, Soboroff, and Gillespie 2017; Senter 2017; Strangfeld 2013; Whitley and Dietz 2018). Finally, several published quantitative assignments combine real-world data, topics of interest to stu- dents, and original data collection by having stu- dents perform content analyses of personal ads, high school yearbook pictures, or other media (Marx 1995; Rushing and Winfield 1999). All these assignments draw on at least one tech- nique likely to generate significant engagement. That said, even those that use multiple techniques face limitations in generating valid and original real-world data about important aspects of human behavior. Students know that survey responses about issues like race and gender may not reflect real attitudes, let alone behaviors. Similarly, con- tent analyses provide excellent information about our media environment and culture but usually cannot establish direct causal links between this environment and people’s daily behavior (Babbie 2015). Therefore, even those exercises that facili- tate students’ collection of real-world data on top- ics of interest may not come as close to measuring “the world out there” as students would like. When students believe that research cannot get at social truths, their intrinsic motivation to learn is likely to decline (Bain 2011), as is their confidence in social science more generally. In this article, I present an assignment that largely surmounts these challenges. The audit method easily engages issues like racism, sexism, classism, ableism, and sexual orientation discrimi- nation that students will find of interest. Audits can also address a wide arrange of traits, contexts, and geographic areas, offering many opportunities for personalization. Finally, audit studies face very lit- tle risk of social desirability bias or unconnected dots between context and behavior (Pager 2007), and doubts that might arise about the reliability of official statistics are not a concern. AUDIT STUDIES Audit studies have flourished in sociology, psy- chology, economics, and political science in the past 15 years. Audits are a subcategory of field experiments, the general term for an approach that combines a real-world setting with an experimental stimulus and data collection (for the only Teaching Sociology articles I know of on field experiments, see Burwell 1987 and Reid 2018 on the “lost let- ters” technique). After many forms of overt dis- crimination were banned in the 1960s, employers, landlords, and individuals became more reluctant to admit any racial bias or discrimination. Therefore, the U.S. federal government and allied nonprofit agencies would sometimes send one white and one black “tester” out to apply for the same set of housing vacancies, noting differences in treatment, as a means of assessing racial dis- crimination and enforcing fair housing laws (Yinger 1995). This method of in-person audits was used by academic social scientists at times throughout the next several decades, sometimes with striking results. For instance, Pager (2003) sent young white and African American men out in Milwaukee to apply for entry-level jobs, armed with equally good mock resumes. She found that employers were more likely to call white job appli- cants back and even slightly preferred white ex- felons to black applicants with no criminal record. By the mid-2000s, the Internet had emerged as the principal means employers and landlords would use to post job openings and housing vacancies. 312 Teaching Sociology 46(4) Around this time, Bertrand and Mullainathan (2004) deployed the first major modern correspon- dence audit—that is, an audit conducted by mail or online rather than in person (Gaddis 2018; Pager 2007). They created fake identities with distinc- tively white- and black-sounding names such as Emily and Lakisha, designed equal quality fake resumes to pair with these identities, and replied to 1,300 job openings in the Boston and Chicago metro areas. They found significant discrimination: Overall, their white identities received 50 percent more replies than their black ones despite equal qualifications. In the subsequent decade and a half, email- based audits have been used to test for unequal treatment by an array of social characteristics in many different arenas. Landmark findings abound. Audits have uncovered unequal treatment by race (Quillian et al. 2017), gender (Öblom and Antfolk 2017), motherhood status (Correll, Benard, and Paik 2007), family structure (Lauster and Easterbrook 2011), age (Baert et al. 2016), sexual orientation (Tilcsik 2011), neighborhood reputa- tion (Besbris et al. 2015), disability status (Ameri et al. 2018), class background (Kugelmass 2016), religion (Wright et al. 2013), and other characteris- tics. The approach has mostly been used to exam- ine hiring and housing discrimination but has also gauged responsiveness by elected officials (Butler and Broockman 2011), graduate school mentors (Milkman, Akinola, and Chugh 2015), churches (Wright et al. 2015), therapists (Kugelmass 2016), and bureaucrats (White, Nathan, and Faller 2015); treatment on sites like AirBnB (Edelman, Luca, and Svirsky 2017), Facebook (Hebl et al. 2012), and Uber (Ge et al. 2016); and other outcomes. Many audits have been U.S.-based, but several hundred now have addressed unequal group treat- ment in other countries (e.g., Ahmed and Hammarstedt 2009; Bonnet et al. 2016; Hogan and Berry 2011; Lauster and Easterbrook 2011). Simultaneously with these substantive develop- ments, researchers have pushed forward scholar- ship on methodological questions like what names to use to signal race (Gaddis 2017a, 2017b), how to separate race and class background signals (Butler and Homola 2015; Gaddis 2017a), and what level of similarity/difference in messages or resumes is ideal to send equivalent signals but escape being detected (Lahey and Beasley 2018; Vuolo, Uggen, and Lageson 2018). Despite their growing sophistication, a core appeal of audit studies is that they produce clear findings that address important questions but do not require highly complex or assumption-ridden statistical techniques to produce or understand. For instance, Pager’s (2003) study of hiring discrimination received significant media attention and is among the best known socio- logical studies of the past 20 years, but at its center are a simple binary predictor (race), binary control vari- able (criminal record), and binary outcome (job call- back) measured using a small to medium-sized sample. Her study is easily understood by readers with no statistical background: Like many audits, its main results can be effectively conveyed in a single bar graph. More recent audits often add more vari- ables, incorporate many more data points, and use multivariate regression. But with appropriate instruc- tion, designing interesting and original online audit studies is within undergraduate students’ capabilities. ThE ASSIGNMENT: CONDUCTING AN EMAIl- BASED AUDIT Objectives, Context, and Overview This section presents information on the assignment. Its main purpose is to teach students how to conduct email-based audits; secondary goals are building more general quantitative proficiency and broader critical thinking skills. I explain these goals in depth in the following. I use the assignment as the last of four projects in a semester-long quantitative methods course at Elon University, a midsized, somewhat selective private university in the Southeast United States. Students are almost all sociology or anthropol- ogy majors and mostly juniors but with some sopho- mores and seniors. Some have taken one required general education statistics course previously. Almost all students attend full-time and do not hold full-time jobs; average entering SAT scores are slightly over 1200 (Princeton Review 2018). Before I present the assignment itself, I address three aspects of context as instructors might modify the assignment depending on their institution characteristics, comfort with audits, and local Institutional Review Board (IRB) requirements. First, student and class characteristics may influence the assignment’s scope and design. In my course, this project is the semester’s largest assign- ment. Design and data collection require focused attention. I check in with each group of students several times, and the presentation and final paper take the place of a traditional in-class final exam. My small class size, students’ generally strong prior academic preparation, and the high share of full-time students facilitate this somewhat Ghoshal 313 intensive setup. Because working with larger classes or students from less privileged back- grounds may require additional focus, I suggest several potential ways instructors can shorten the assignment yet still capture its main benefits after I describe the full version. Second, instructors should consider their own experience and comfort with audits while planning. Though this assignment can be led by an instructor with no experience in the method, those who have not carried out audits themselves can use this arti- cle as a template for carrying out the design phase and perhaps also the data collection of a mini-audit of their own before launching the assignment in class. Alternatively, those unable to test the method on their own before class might assign a slimmer version the first time. Those interested in a detailed guide to conducting audits should consult relevant chapters in Gaddis (2018). Third, at my institution, the IRB exempts class projects that are not intended to become theses or published research from review and has approved my own audit studies without difficulty. In my own research, IRBs have uniformly come to think that the deception in audit studies is extremely minor and worth the cost given that the majority of all interac- tions on the online marketplace used in this project are dropped and that the lost time cost to ad posters is very low. Instructors should discern their local IRB’s standards and plan accordingly. Assignment Steps: Full Version Instructors should devote three to four weeks of class mainly to the project, which constitutes 20 percent of the course grade; trimmed versions will require less time. Students work in groups through- out the assignment, but I inform them that individ- ual grades may vary based on a group evaluation form that every student will fill out individually at the end. In-class time is a mix of lecture, discus- sion, and group work. Outside of class, students complete assigned readings, decide key features of their project, and conduct most data collection, analysis, and writing. The schedule as described presumes twice-weekly class meetings; instructors on other schedules should adjust as needed. All handouts and instructions for the assignment are available at the author’s website or by emailing the author. When the section begins, students have previ- ously read one short news article that describes an audit (Storrs 2016), and some are aware that audits of hiring discrimination exist, but few have in-depth knowledge of audits’ design and execu- tion. Therefore, as homework for the first day, stu- dents watch a five-minute video clip explaining one audit much like the one students will conduct (Ghoshal 2015), read a published audit on housing discrimination (Carpusor and Loges 2006), and read a set of 10 abstracts from recent published audits. The abstracts assigned vary by semester but always include 10 recent studies that test a variety of characteristics in an array of different domains to give students a sense of how audits work and the range of arenas and traits they can involve. To help prepare students to design their own studies, in class, I explain some key features of audits, give an overview lecture on audit design, and show students how Craigslist’s regional mar- ketplaces of housing ads are set up. I tell students that most groups will use unequal treatment in roommate searches as the arena of interest. I explain that individual choices to rule out or pre- fer people of particular races, sexual orientations, or other statuses as roommates are not illegal in most contexts but are socially consequential. That is, patterns of (un)willingness to live with groups give us information about where social boundar- ies are drawn and how close groups feel to others, with far-reaching personal and network implica- tions (Bogardus 1933; Shiao 2013) and also with macro-level impacts on segregation/integration and racial formation (Omi and Winant 2014). I also mention that there have been far fewer audit studies in social realms than formal arenas such as hiring or renting full apartments, where most kinds of discrimination are illegal, so the potential for students’ projects to uncover genuinely new knowledge is higher. I also give students the option of choosing another outcome, such as rent- ing full apartments, if they discuss and clear their design with me first. As homework for the second day, students are given a handout (email author or see author’s web- site) that asks them to pick one or two partners, decide what social characteristic they want to study, pick names for their fake identities, and develop preliminary drafts of email texts. I also show them an Excel or Google Sheets template for tracking data collection (see Figure 1). I provide information on signals of race sent by some first and last names in the United States (Gaddis 2017a, 2017b). Students have some time in class to begin collaborating on Step 1, and I check in with groups as they work. Each group completes this first step of the assignment in a Google Doc, making it easy to collaborate and for me to post comments back. 314 Teaching Sociology 46(4) Through all periods of in-class work, students can use their laptops freely. As reading for Day 2, I assign clips from articles that discuss audit meth- odology (Gaddis & Ghoshal 2015, pp. 282–83 and 287–90 only) and name selection (Gaddis 2017a, pp. 43 and 54–58 only) and recommend that stu- dents read relevant parts of their textbook chapter (Babbie 2015: chapter 8) on experiments. In the second class period devoted to this assignment, each group informally presents its progress so far so that other students and I can offer feedback on design. A second handout provides the foundation for the remainder of class time and explains the next stage of homework. It asks stu- dents to finalize their pilot design and actually reply to at least 20 different ads using their two identities, gathering 40 data points by the start of the third class period of this section. I explain that these pilot data will not be used in tabulating actual results but that a pilot run is often priceless in iden- tifying design flaws and gaining familiarity with mechanical and workflow aspects of conducting email-based audits. The class period then includes additional time for student groups to hone their research design and plan pilot data collection. At this point or earlier, I provide students with a copy of the full assignment sheet for the final write-up, due in several weeks (email author or see author’s website). I also advise students to not reply back to any messages they receive in either the pilot wave or full wave. Most IRBs do not ask auditors to reply back, and most researchers prefer not to, as doing so would cost ad posters additional time for no clear benefit and could make future studies of discrimination more difficult (Crabtree 2018; Gaddis 2018). Instructors should mention these rationales for not replying back; they may also wish to mention the related point that accu- rately measuring hidden discrimination is virtually impossible without deception. Giving students time after the assignment to write and reflect on their own views on non-notification, deception, or both may be worthwhile as well. In the third class meeting, groups report back to the full class with their pilot findings. I provide some time for groups to work together and also consult each group individually. Students complete one more pre- liminary step, writing up any alterations and more detailed plans, for the fourth class period. I remind students that they should leave 36 to 48 hours between sending their final emails and tabulating results to allow almost all responses to come in. They then complete the full data collection, replying to at least 75 additional ads, by the fifth class period. In this class, we discuss findings so far, and I give groups time to conduct required statistical analyses and work on a five-minute poster presentation, which they con- tinue as homework. If I have not already done so for a previous project, I provide a template for posters and explain key features of poster presentations. Groups need not print their posters but instead are asked to create them electronically, post them to the class web- site, and project them on the main screen while pre- senting. In the sixth class, following each presentation, other students and I ask questions and make sugges- tions as appropriate. Students also have time in that class period to continue analyses, develop their mini literature reviews, and draft other parts of the paper. The final written assignment is due the following week, in the place of a traditional semester-ending exam. After submitting their papers, students submit a brief evaluation of their group. Abridged Versions For those who wish to trim the assignment, a smaller version can easily be designed. Here are several possibilities. First, instructors could pro- vide student groups with predesigned emails and names to choose from (for some sample texts, see Gaddis and Ghoshal 2015; Ghoshal and Gaddis 2015). Second, students could write a brief report documenting their findings rather than a full final paper. Third, the entire class could address the same trait and even use the same collectively designed email messages, breaking into groups only to target different regions. Response data across locations could then be pooled before analy- sis. Other options for trimming include having stu- dents reply to fewer ads, reducing the number of ID#CityURL EmailAdAd posted Picture? PriceNotes ReplySendversion order Response? Posi\u0000veNotes Send versionorder Response?Posi\u0000ve Notes addressdate\u0000me date \u0000meresponse? \u0000meresponse? AD ITSELF IDENTITY 2IDENTITY 1 Figure 1. Sample Excel template for data collection. Ghoshal 315 statistical analyses required, or having students work in Excel rather than SPSS. Regardless, since audit studies require coordinated sending of emails from two different identities, they naturally work best in groups of two or three at the undergraduate level, which reduces grading challenges that might otherwise arise in larger classes. DATA COllECTION ON EffECTIVENESS I received IRB clearance to collect data on assign- ment outcomes and students’ perceptions for this article. All data in the following section are drawn from two courses, one in the 2016–2017 and the other in the 2017–2018 academic year. The first had 20 students; the second had 10. Because stu- dents worked in groups, a total of 12 different proj- ects were completed. In presenting evidence of outcomes, I draw on two sources of evidence. First, I consider students’ proj- ects themselves. Projects were assessed relative to five objectives, including two to four specific criteria per objective. The first two objectives were tightly focused on audit design and data collection, while the other three objectives addressed students demonstrat- ing proficiency in several broader data analysis tasks, critically evaluating their own projects’ strengths and limitations, and integrating their work with existing scholarship. Objectives are fully explained in the fol- lowing. Each project received a rating on each objec- tive; here I collapse those ratings to exceeds expectations, meets expectations, and does not meet expectations. Beyond assessment results, I also pro- vide excerpts from students’ work to illustrate their performance on various objectives. Second, both semesters, I administered an anonymous follow-up survey focused on students’ perceptions of learning that included both qualitative and quantitative items. Twenty-nine of 30 students filled out this survey, for a response rate of 97 percent. RESUlTS The assignment effectively led students to meet its objectives while generating significant intellectual excitement. It performed especially well in its first two objectives, which were the most tightly linked to audits in particular. It was also successful in helping students build broader quantitative, ana- lytical, and critical thinking skills. In terms of grades, six projects scored in the A range, while four received Bs, and two received Cs. Success was in part due to the assignment enabling students to collect original data that they cared about and believed to accurately measure real-world behavior. Student Projects: Assessment Results and Illustrative Excerpts The first objective addressed design: students were to create appropriate stimuli for investigating a research question about unequal treatment. Specific criteria for this objective were (a) clearly identify- ing a research question around a trait that could be productively studied using the audit methodology and (b) designing two well-balanced, compelling messages and identities that did not conflate mul- tiple traits into a single comparison unless approved. Projects that exceeded or met expecta- tions here generally settled on a single point of dif- ference (though I note two exceptions in the following), such as varying race and specific mes- sage wording while including signals that effec- tively held gender, message quality, and message length constant. Eight of the 12 projects exceeded expectations for this objective, while 4 met expec- tations, and none were below expectations. Students’ work illustrates some ways they excelled here. Across the two sections, five groups chose to study sexual orientation, with three groups comparing treatment of a straight woman to a les- bian and two groups comparing a gay man to a straight man. Three groups focused on race alone: They contrasted treatment of a white man to a Latino man, a white man to a Native American man, and a white woman to woman whose name suggested an Arab or Middle Eastern origin. One group varied signals of age between two white men, while another group compared two African American male identities whose messages sug- gested differing sociability or extroversion. Finally, two groups deliberately varied race and message quality simultaneously. Because I had mentioned that differential treatment of Arab and black men in housing searches is already well established (e.g., Carpusor and Loges 2006), these groups asked whether high-quality messages can make up for the “race penalty” that black and Arab men face, analo- gous to parts of studies that contrast well-creden- tialed African American job candidates with lesser credentialed white candidates (Gaddis 2015; Pager 2003). Students showed creativity and balance in cre- ating messages with realistic yet clear signals. For instance, one group studying sexual orientation designed two female identities with common white 316 Teaching Sociology 46(4) names, Caroline Baker and Caitlin Walsh. Each identity’s reply to the ad mentioned spending some evenings at their romantic partner’s apartment, but one reply signaled that partner was female while the other signaled a male partner. The messages held age, education, employment, and race signals constant, successfully near-isolating sexual orien- tation as the one clear difference. Similarly, for the two groups that varied race and message quality simultaneously, the white male identity’s message used nonstandard grammar and was written with unusual informality, while the other message was designed to be clear and more appealing. Because all groups had conducted a pilot mini-audit, they were able to modify their messages as needed in response to feedback before launching the full data collection. Figure 2 presents the message wordings in the three examples discussed here. The second objective was for students to collect data by systematically sending messages and track- ing responses; criteria included (a) finding 75+ appropriate ads to respond to while excluding inap- propriate ones, (b) responding promptly and bal- ancing the timing of messages, and (c) clearly tabulating all data in a spreadsheet. Successful projects responded to enough ads, explained their criteria for including/excluding ads, and had gener- ally balanced schemes for timing responses. Exceptional projects additionally articulated very compelling criteria for their inclusion/exclusion scheme, balanced their sending order nearly per- fectly, and sent all messages promptly. Seven proj- ects exceeded baseline expectations here, while five met them, and none were inadequate. Some examples illustrate projects’ thoughtful- ness and thoroughness here. To find ads, most groups targeted two metro areas. Often, location choices reflected a desire to make regional com- parisons, especially comparing areas thought to be more conservative with more liberal ones. Other times, rationales for choices were more specific. One group explained in their write-up’s methods section: The cities used in our research were Boston, Detroit, and Washington, D.C. We decided to look at rooms in cities that may hold generalized stereotypes against Middle Eastern individuals due to events involving . . . terrorist groups. For example, in 2013, two brothers planned and executed . . . the Boston Marathon Bombing. Furthermore, we suspected that studying an area (Detroit) with a large Muslim population could potentially yield interesting results, as well as studying an area (Washington, D.C.) that has racially and politically changed as a result of the President- elect. (Student paper, citation redacted) Similarly, within cities, projects did not simply respond to every ad posted but limited their sample to postings that seemed legitimate and appropriate. Another group described their selection process (guided by instructions from the handouts) as follows: In determining which ads we wanted to respond to, we first searched the respective pages for [our cities of interest]. We went to the housing section and clicked the “rooms/ shared” button. Next we checked the box labeled “bundle duplicates.” We entered our price range as between $300 and $1200. While initially we decided to only select the 40 ads for each city that were posted within the last 24 hours, we determined that we needed to broaden our time frame to gather a large enough sample. . . . We responded to ads that appeared to be listed by a person, and not a company or apartment leasing office. (Student paper, citation redacted) Two students further explained their email-sending process, showing a level of organization suggestive of significant engagement: At around 3:00 p.m., we sent 40 emails from [Identity 1] and 40 emails from [Identity 2] to their corresponding listings. Approximately four hours later, we sent 40 emails from [each identity] to the opposite listings. We waited to send our second round of emails to control for the possibility that room listers might become suspicious if similar emails are sent to [sic] close together. To record our data, we created a spreadsheet in which we entered the location of the listing, the URL of the listing, the contact email address of the individual renting out a room, the ad date, the price, the order emails were sent, whether our identities received a response, if that response was positive or negative, and notes. (Student paper, citation redacted) Once students had sent all their messages, they waited two days for responses to accumulate before executing tasks related to the next three objectives. Ghoshal 317 Identity signaled: lesbian woman Subject line: Room for Rent hello! I am responding to your ad on craigslist regarding the open room. My name is Caitlin Walsh, I recently graduated from college, and I work full-time in the area. I would be sleeping at my fiancé Jenny’s house a couple times a week. I would love to come check out the place sometime soon. let me know if the room is still available and when is a good time for us to meet in person! Best, Caitlin Identity signaled: Straight woman Subject line: Room listing hello- I’m responding to your room listing on Craigslist. I’m 21 years old and recently out of college. I work full-time so I’d be gone during the day Monday-friday and I typically spend two nights a week at my boy- friend’s place. It would be great to meet and see the place! Thanks, Caroline Identity: Arab male, higher-quality message Subject: Room availability Dear Sir or Madam, I am writing to you regarding your most recent post on Craigslist for an available room. My name is Ahmed Najjar, I am a male in my mid 20’s and am looking for a room while I complete my graduate de- gree. If your offer still stands, I would love to meet you to discuss the details as well as view the room. Thank you for your consideration. Sincerely, Ahmed Identity: White male, lower-quality message Subject: Craigslist room hey, I saw your room ad on craigslist. Are u still looking for a roommate? I’m 25 & looking for a change of scenery. let me know if you’re interested -Jake Sent from my iPhone. Identity: Black male, higher-quality message Subject: Room for rent hi, I noticed your ad on Craigslist and I am interested in getting more information about the room you are looking to rent. I am a 23 year old male looking for housing in the area due to a new job. Would it be possible to set up a time to meet and take a look at the space? Thank you for your help and I look forward to hearing from you. Best, Jamal Jackson Identity: White male, lower-quality message Subject: Craigslist ad hi, Im looking to rent a room and saw your ad. Im a 24 year old guy and just landed a job nearby. Wanna set up a time to talk? let me know what works. Thanks. Jacob Anderson Figure 2. Email texts from three example projects. 318 Teaching Sociology 46(4) Because these objectives all concern broader ana- lytical skills rather than solely audit-specific tasks, I present information on them together. The third objective was for students to appropriately analyze their data, including conducting, presenting, and correctly interpreting (a) a simple bivariate com- parison of their two identities’ response rates with a table, graph, and calculation of a response ratio; (b) a t test; (c) the lambda or gamma statistic; and (d) a multivariate comparison with a table and two to four calculations of response ratios. Projects that excelled here completed all these tasks success- fully, while those that met expectations did so for nearly all. Fourth, students were to critically evalu- ate both the (a) benefits/usefulness and (b) limita- tions of their specific design, the audit method, and/or quantitative methods more broadly. The fifth and final objective was for students to inte- grate their findings with prior literature in (a) a focused mini literature review and (b) a discussion/ conclusion section that related their findings back to prior work. On these last two outcomes, projects that met expectations correctly identified key strengths and limitations and noted connections between their work and prior research. Projects that exceeded expectations went further, thoroughly evaluating the seriousness of their projects’ strengths and weaknesses and articulating ways their work might build on, challenge, or qualify prior research. Students’ performance on these three objectives was strong, albeit slightly weaker overall than for the two prior objectives. On the data analysis tasks, six projects exceeded expectations, five met expec- tations, and one failed to meet expectations. The groups did about as well in evaluating strengths and limitations: Five projects exceeded standards, five met them, and two were inadequate. Finally, the most intellectually complex, big-picture tasks involved integration with prior literature and the- ory. Here, four groups exceeded expectations, eight met them, and none were inadequate. For brevity, I do not provide examples of quan- titative analyses, literature reviews, assessment of strengths, or conclusions here. Instead, I focus on one piece of the critical evaluation outcome at which groups excelled: discussions of projects’ limitations. Early researchers can struggle with how to think about limitations of research designs, potentially defaulting to the view that limitations are simply either trivial or fatal. But discussion sec- tions showed nuance in students’ treatment of limi- tations. For instance, the two groups that compared a white man’s poorly written message to a well-written message from a man of color both noted correctly that their findings might vary if the gap in message quality had been greater or lesser. The group that compared a straight woman and a lesbian seeking a roommate wrote that their signal of lesbian status—“my fiancé Jenny”—might have inflated responses if readers saw engagement as signaling greater maturity than the identity that simply mentioned “my boyfriend” or if misspelling “fiancée” was read as evidence of heterosexuality. Most groups who found nonsignificant differences between groups correctly noted that a larger sample or different significance threshold might have yielded a different interpretation and mentioned that measures of association such as lambda and gamma provide information that should be consid- ered especially with relatively small samples. The students who contrasted an extroverted candidate with a self-described introvert identity, expecting ad posters to prefer a more outgoing roommate, wrote that “after examining the results, we actually found out that the results reveal our expectations were flipped backward from reality”—and went on to show a solid understanding of why their expecta- tions had been flawed. Yet all of these groups were able to persuasively articulate why their findings were nonetheless revealing and important. Far from invalidating these studies, then, these reactions show students developing abilities to reflect on their own work and grapple with the imperfections inherent in all research. These abili- ties were stronger in this project than earlier ones for the course, where papers’ discussions of limita- tions were more binary. That this project immedi- ately and deeply immersed students in the real-world tradeoffs that all audit studies inherently face, such as how to clearly but credibly signify the trait of interest and how similar or different mes- sages should be, appeared especially helpful in building students’ sophistication in thinking about limitations. This ability could prove useful in many different future contexts. Students’ Perceptions of Learning Finally, I present quantitative and qualitative evi- dence that students saw the assignment as effective and educational. In the first two questions, students report on their knowledge of how to conduct audits “before this class” and “now.” The mean level of knowledge prior to the course was 2.5 on a 1 to 6 Likert scale. The mean score after doing the assign- ment was 4.9, a large and significant improvement (p < .01). While 21 of 29 students reported their Ghoshal 319 knowledge of conducting audits fell in the lower half of the scale before this class, 28 of 29 reported that their knowledge fell in the top half of answer choices after the assignment. Further, that more students saw their newly gained ability to conduct audits as good (5) rather than high (6) suggests most realized they had improved significantly but not yet become experts, balancing confidence with an awareness of their own limitations in a way that may aid additional learning (Goleman 2006). In addition, students saw the project as broadly beneficial. On a 5-point Likert scale, 28 of 29 stu- dents, or 97 percent, strongly or somewhat agreed that “after having done this project, a bright and motivated student who was interested would be in a good position to plan and eventually carry out a larger research project (such as a senior thesis) using the method.” Similar numbers agreed that the assignment was a better way to learn audit methods than simply reading/hearing and being tested about them, while only one student was neutral. And 26 of 29 students responded affirmatively when asked if the assignment was worthwhile, while 3 were neutral. The overall mean ratings for these three items on 5-point scales were 4.7, 4.7, and 4.3. Students’ open-ended responses to two survey questions were also quite positive. First, I asked students to explain in a paragraph what they learned from the assignment. Responses included that they enhanced their abilities to design realistic yet bal- anced messages, send messages in a way that would avoid detection, organize and interpret data that came back, and make changes in project design or update expected conclusions as needed in response to unanticipated findings. I also asked what the best parts of the assign- ment were. Typical responses here included that “we were able to choose the variables/identities that interested us most,” “choosing our topic of interest and creating identities based on personal interest,” and “it was very hands on and we were learning about real life occurrences.” Overall, com- ments suggested that students responded positively to the authenticity of the assignment, felt connec- tion and ownership of their projects because they were able to choose their trait of interest and collect their own data, and saw the assignment as helping them perform real research. As one student wrote, As someone particularly interested in qualitative research, I don’t usually enjoy quantitative projects or I wish I could supplement them. However, I both enjoyed and saw the value in this project. It was clearly a more effective way to learn about online field experiments than just learning about it [sic] from readings and lectures. DISCUSSION In the remainder of this article, I discuss consider- ations in responding to students’ work and then note two potential variations. First, instructors should keep in mind that their main goals are to help students improve their abil- ity to conduct audits and become intellectually excited about the process, not produce published research. Given this, students should be encour- aged to pursue interesting questions without regard to whether their design is theoretically novel or results will be statistically significant. For exam- ple, the group studying sexual orientation ulti- mately found no significant difference in treatment of “Caitlin” and “Caroline”: Response rates were 61 percent and 60 percent, respectively. But the finding that the lesbian candidate faced no discrim- ination in her roommate search (see also Ahmed and Hammarstedt 2009; Urban Institute 2017) is as interesting and noteworthy as the opposite finding, and the project remains just as valuable in helping students learn the method. Of course, instructors should also bear in mind and remind students that relatively small sample sizes make significant find- ings less likely. More broadly, instructors should remember that imperfection and uncertainty is a part of audit stud- ies, especially when performed by first-timers. Creating two messages that are substantively iden- tical yet different enough to escape detection while credibly signaling the trait of interest is a delicate balancing act. Similarly, randomizing which iden- tity goes first while alternating cities may be diffi- cult for beginning researchers, even after the pilot wave. Having access to students’ fake identity email accounts (see author handouts) and devoting class time to check-ins will mitigate some design issues, but even the most talented students will not design an airtight audit on their first real attempt. Instructors should therefore be reasonable about the standards they bring to judging student work. I now address design variations. The assign- ment presented here asks students to perform a matched-pairs audit in which each ad poster to whom students respond receives emails from both fake identities. This is usually regarded as method- ologically superior to a simple random assignment setup in which the identities respond to two differ- ent, randomly selected groups of ads (Vuolo et al. 320 Teaching Sociology 46(4) 2018; but see Weichselbaumer 2015 on risks of detection in matched-pairs audits). However, it requires more attention to detail, which can yield mistakes, especially in projects where students alternate their email texts between different-named identities. Instructors who are concerned their stu- dents may not be able to track the audit approach closely enough could therefore use the nonmatched approach. However, they should be aware of the higher potential for unobserved confounds to skew findings and let students know about this possibil- ity to reduce the risk that students will make sweep- ing conclusions based on data quirks that are more likely in this approach. Alternatively, instructors could vary whether the assignment asks students to reply to roommate wanted ads, vacancies for full apartments, or even job postings or involves some other arena, such as contacting religious institutions or government rep- resentatives using different identities—or offers several options. I use the roommate approach as the default and ask those interested in another arena to get my approval, but the most natural default may vary by instructor. Whatever design variations instructors choose, the preceding evidence shows that with sufficient guidance, undergraduate students can learn to con- duct simple yet sound online audit studies rela- tively quickly as part of a quantitative methods course. In addition, students feel the assignment is realistic, experience a sense of connection and ownership, and further develop their research skills as they conduct it. The assignment therefore has the potential not merely to help students become better at conducting audits but to increase their engage- ment and facility with quantitative research more broadly. Online audits can be a powerful tool in a sociology instructor’s toolkit. ACkNOWlEDGMENTS This work was partly supported through reassigned- time and research funding from Elon University, including its Center for the Advancement of Teaching and Learning, Dean’s and Provost’s Offices, and Faculty Research and Development Committee. The article benefitted from thoughtful feedback from the editor and three anonymous reviewers. I also thank Claire Whitlinger, Michael Gaddis, and Baris Kesgin for helpful comments. EDITOR’S NOTE Reviewers for this manuscript were, in alphabetical order, Maxine Atkinson, Andrew Lindner, and Gail Markle. REfERENCES Ahmed, Ali, and Mats Hammarstedt. 2009. “Detecting Discrimination against Homosexuals: Evidence from a Field Experiment on the Internet.” Economica 76(303):588–97. Ameri, Mason, Lisa Schur, Meera Adya, F. Scott Bentley, Patrick McKay, and Douglas Kruse. 2018. “The Disability Employment Puzzle: A Field Experiment on Employer Hiring Behavior.” ILR Review. Retrieved May 10, 2018 (https://doi .org/10.1177/0019793917717474). Atkinson, Maxine, Ronald Czaja, and Zachary Brewster. 2006. “Integrating Sociological Research into Large Introductory Courses: Learning Content and Increasing Quantitative Literacy.” Teaching Sociology 34(1):54–64. Babbie, Earl. 2015. The Practice of Social Research. Toronto, ON: Nelson Education. Baert, Stijn, Jennifer Norga, Yannick Thuy, and Marieke Van Hecke. 2016. “Getting Grey Hairs in the Labour Market. An Alternative Experiment on Age Discrimination.” Journal of Economic Psychology 57:86–101. Bain, Ken. 2011. What the Best College Teachers Do. Cambridge, MA: Harvard University Press. Bertrand, Marianne, and Sendhil Mullainathan. 2004. “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” The American Economic Review 94(4):991–1013. Besbris, Max, Jacob William Faber, Peter Rich, and Patrick Sharkey. 2015. “Effect of Neighborhood Stigma on Economic Transactions.” Proceedings of the National Academy of Sciences 112(16):4994–98. Bogardus, Emory Stephen. 1933. “A Social Distance Scale.” Sociology & Social Research 17:265–71. Bonilla-Silva, Eduardo. 2006. Racism without Racists: Color-blind Racism and the Persistence of Racial Inequality in America. Lanham, MD: Rowman & Littlefield. Bonnet, François, Etienne Lalé, Mirna Safi, and Etienne Wasmer. 2016. “Better Residential Than Ethnic Discrimination! Reconciling Audit and Interview Findings in the Parisian Housing Market.” Urban Studies 53(13):2815–33. Bridges, George S., Gerald M. Gillmore, Jana L. Pershing, and Kristin A. Bates. 1998. “Teaching Quantitative Research Methods: A Quasi-experimental Analysis.” Teaching Sociology 26(1):14–28. Burdette, Amy M., and Kerry McLoughlin. 2010. “Using Census Data in the Classroom to Increase Quantitative Literacy and Promote Critical Sociological Thinking.” Teaching Sociology 38(3):247–57. Burwell, Ronald J. 1987. “The Lost Letter Experiment: A Class Exercise for Research Methods.” Teaching Sociology 15(2):195–96. Butler, Daniel, and David Broockman. 2011. “Do Politicians Racially Discriminate against Constituents? A Field Ghoshal 321 Experiment on State Legislators.” American Journal of Political Science 55(3):463–77. Butler, Daniel M., and Jonathan Homola. 2017. “An Empirical Justification for the Use of Racially Distinctive Names to Signal Race in Experiments.” Political Analysis 25(1):122–30. Carpusor, Adrian G., and William E. Loges. 2006. “Rental Discrimination and Ethnicity in Names.” Journal of Applied Social Psychology 36(4):934–52. Correll, Shelley J., Stephen Benard, and In Paik. 2007. “Getting a Job: Is There a Motherhood Penalty?” American Journal of Sociology 112(5):1297–338. Crabtree, Charles. 2018. “An Introduction to Conducting Email Audit Studies.” Pp. 103–17 in Audit Studies: Behind the Scenes with Theory, Method, and Nuance, edited by S. M. Gaddis. New York: Springer International Publishing. Daniel, William Wentworth. 1968. Racial Discrimination in England. Middlesex: Penguin Books. DeCesare, Michael. 2007. “‘Statistics Anxiety’ among Sociology Majors: A First Diagnosis and Some Treatment Options.” Teaching Sociology 35(4):360– 67. Edelman, Benjamin, Michael Luca, and Dan Svirsky. 2017. “Racial Discrimination in the Sharing Economy: Evidence from a Field Experiment.” American Economic Journal: Applied Economics 9(2):1–22. Enten, Harry. 2016. “‘Shy’ Voters Probably Aren’t Why the Polls Missed Trump.” FiveThirtyEight, November 16. Retrieved May 10, 2018 (https:// fivethirtyeight.com/features/shy-voters-probably- arent-why-the-polls-missed-trump/). Gaddis, S. Michael. 2015. “Discrimination in the Credential Society: An Audit Study of Race and College Selectivity in the Labor Market.” Social Forces 93(4):1451–79. Gaddis, S. Michael. 2017a. “How Black Are Lakisha and Jamal? Racial Perceptions from Names Used in Correspondence Audit Studies.” Sociological Science 4(19):469–89. Gaddis, S. Michael. 2017b. “Racial/Ethnic Perceptions from Hispanic Names: Selecting Names to Test for Discrimination.” Socius 3(1):1–11. Gaddis, S. Michael. 2018. Audit Studies: Behind the Scenes with Theory, Method, and Nuance. New York: Springer International Publishing. Gaddis, S. Michael, and Raj Ghoshal. 2015. “Arab American Housing Discrimination, Ethnic Competition, and the Contact Hypothesis.” The Annals of the American Academy of Political and Social Science 660(1): 282–99. Ge, Yanbo, Christopher R. Knittel, Don MacKenzie, and Stephen Zoepf. 2016. “Racial and Gender Discrimination in Transportation Network Companies.” NBER Working Papers. Retrieved May 10, 2018 (http://www.nber.org/papers/w22776). Ghoshal, Raj. 2015. “Ignite 16: Finding a Roommate on Craigslist.” Talk at Ignite Baltimore, June 13. YouTube. Retroeved May 10, 2018 (https://www .youtube.com/watch?v=AdlQkQK218o). Ghoshal, Raj, and S. Michael Gaddis. 2015. “Finding a Roommate on Craigslist: Racial Discrimination and Residential Segregation.” Social Science Research Network. Retrieved May 10, 2018 (https://ssrn.com/ abstract=2605853). Godfrey, A. Blanton. 1988. “Should Mathematicians Teach Statistics?” The College Mathematics Journal 19(1):8–10. Goleman, Daniel. 2006. Emotional Intelligence. New York: Bantam. Hebl, Michelle R., Melissa J. Williams, Jane M. Sundermann, Harrison J. Kell, and Paul G. Davies. 2012. “Selectively Friending: Racial Stereotypicality and Social Rejection.” Journal of Experimental Social Psychology 48(6):1329–35. Himes, Christine L., and Christine Caffrey. 2003. “Linking Social Gerontology with Quantitative Skills: A Class Project Using US Census Data.” Teaching Sociology 31(1):85–94. Hogan, Bernie, and Brent Berry. 2011. “Racial and Ethnic Biases in Rental Housing: An Audit Study of Online Apartment Listings.” City & Community 10(4):351–72. Kugelmass, Heather. 2016. “‘Sorry, I’m Not Accepting New Patients’: An Audit Study of Access to Mental Health Care.” Journal of Health and Social Behavior 57(2):168–83. Lahey, Joanna, and Ryan Beasley. 2018. “Technical Aspects of Correspondence Studies.” Pp. 81–101 in Audit Studies: Behind the Scenes with Theory, Method, and Nuance, edited by S. M.l Gaddis. New York: Springer International Publishing. Lauster, Nathanael, and Adam Easterbrook. 2011. “No Room for New Families? A Field Experiment Measuring Rental Discrimination against Same- sex Couples and Single Parents.” Social Problems 58(3):389–409. Lindner, Andrew M. 2012. “Teaching Quantitative Literacy through a Regression Analysis of Exam Performance.” Teaching Sociology 40(1):50–59. Lovekamp, William E., Shane D. Soboroff, and Michael D. Gillespie. 2017. “Engaging Students in Survey Research Projects across Research Methods and Statistics Courses.” Teaching Sociology 45(1):65–72. Markle, Gail. 2017. “Factors Influencing Achievement in Undergraduate Social Science Research Methods Courses: A Mixed Methods Analysis.” Teaching Sociology 45(2):105–15. Marx, Jonathan. 1995. “‘Mona Lisa, Mona Lisa, Men Have Named You’: Smiles as a Social Fact.” Teaching Sociology 23(3):274–79. McKinney, Kathleen, Carla B. Howery, Kerry J. Strand, Edward L. Kain, and Catherine White Berheide. 2004. Liberal Learning and the Sociology Major Updated: Meeting the Challenge of Teaching Sociology in the Twenty-first Century. Washington, DC: American Sociological Association. 322 Teaching Sociology 46(4) Milkman, Katherine L., Modupe Akinola, and Dolly Chugh. 2015. “What Happens before? A Field Experiment Exploring How Pay and Representation Differentially Shape Bias on the Pathway into Organizations.” Journal of Applied Psychology 100(6):1678–712. Neumark, David, Roy J. Bank, and Kyle D. Van Nort. 1996. “Sex Discrimination in Hiring in the Restaurant Industry: An Audit Study.” Quarterly Journal of Economics 111(3):915–42. Öblom, Annamaria, and Jan Antfolk. 2017. “Ethnic and Gender Discrimination in the Private Rental Housing Market in Finland: A Field Experiment.” PloS One 12(8):e0183344. Omi, Michael, and Howard Winant. 2014. Racial Formation in the United States. New York: Routledge. Pager, Devah. 2003. “The Mark of a Criminal Record.” American Journal of Sociology 108(5):937–75. Pager, Devah. 2007. “The Use of Field Experiments for Studies of Employment Discrimination: Contributions, Critiques, and Directions for the Future.” The Annals of the American Academy of Political and Social Science 609:104–33. Paxton, Pamela. 2006. “Dollars and Sense: Convincing Students That They Can Learn and Want to Learn Statistics.” Teaching Sociology 34(1):65–70. Persell, Caroline Hodges, Kathryn M. Pfeiffer, and Ali Syed. 2008. “How Sociological Leaders Teach: Some Key Principles.” Teaching Sociology 36(2):108–24. Princeton Review. 2018. “Elon University.” Retrieved May 10, 2018 (https://www.princetonreview.com/ college/elon-university-1023974). Quillian, Lincoln, Devah Pager, Ole Hexel, and Arnfinn Midtboen. 2017. “Meta-analysis of Field Experiments Shows No Change in Racial Discrimination in Hiring over Time.” Proceedings of the National Academy of Sciences 114(41):10870–75. Reid, Matt. 2018. “Lost Letters: Using the Lost-letter Technique to Teach Social Research Methods.” Teaching Sociology. Retrieved May 26, 2018 (https:// doi.org/10.1177/0092055X18776728). Riach, Peter, and Judth Rich. 2002. “Field Experiments of Discrimination in the Market Place.” The Economic Journal 112: F480–518. Retrieved May 10, 2018 (http://doi:10.1111/1468-0297.00080). Rushing, Beth, and Idee Winfield. 1999. “Learning about Sampling and Measurement by Doing Content Analysis of Personal Advertisements.” Teaching Sociology 27(2):159–66. Schumm, Walter R., Farrell J. Webb, Carlos S. Castelo, Cynthia G. Akagi, Erick J. Jensen, Rose M. Ditto, Elaine Spencer-Carver, and Beverlyn F. Brown. 2002. “The Space Shuttle Challenger, Pearl Harbor, and the RMS Titanic.” Teaching Sociology 30(3):361–75. Senter, Mary Scheuer. 2017. “Integrating Program Assessment and a Career Focus into a Research Methods Course.” Teaching Sociology 45(2):131–41. Shepard, Steven. 2017. “Poll: 46 Percent Think Media Make up Stories.” Politico, October 18. Retrieved May 10, 2018 (https://www.politico .com/story/2017/10/18/trump-media-fake-news- poll-243884). Shiao, Jiannbin. 2013. “The Influence of Interracial Friendships on the Likelihood of Interracial Intimacy.” Paper presented at the American Sociological Association annual meeting, New York, NY. Steen, Lynn A. 2001. “The Case for Quantitative Literacy.” Pp. 1–22 in Mathematics and Democracy, edited by L. A. Steen. Princeton, NJ: National Council on Education and the Discipline. Strangfeld, Jennifer A. 2013. “Promoting Active Learning: Student-led Data Gathering in Undergraduate Statistics.” Teaching Sociology 41(2):199–206. Storrs, Carolina, 2016. “Therapists Often Discriminate against Black and Poor Patients, Study Finds.” CNN, June 1. Retrieved May 10, 2018 (https://www.cnn .com/2016/06/01/health/mental-health-therapists- race-class-bias/index.html). Sweet, Stephen, and Kimberly M. Baker. 2011. “Who Has the Advantages in My Intended Career? Engaging Students in the Identification of Gender and Racial Inequalities.” Teaching Sociology 39(1):1–15. Tilcsik, András. 2011. “Price and Prejudice: Employment Discrimination against Openly Gay Men in the United States.” American Journal of Sociology 117(2):586–626. Urban Institute. 2017. “A Paired-testing Pilot Study of Housing Discrimination against Same-sex Couples and Transgender Individuals.” Retrieved May 10, 2018 (https://www.urban.org/sites/default/files/pub- lication/91486/hds_lgt_final_report_3.pdf). Vuolo, Mike, Christopher Uggen, and Sarah Lageson. 2018. “To Match or Not to Match? Statistical and Substantive Considerations in Audit Design and Analysis.” Pp. 119–40 in Audit Studies: Behind the Scenes with Theory, Method, and Nuance, edited by S. M. Gaddis. New York: Springer International Publishing. Weichselbaumer, Doris. 2015. “Testing for Discrimination against Lesbians of Different Marital Status: A Field Experiment.” Industrial Relations: A Journal of Economy and Society 54(1):131–61. White, Ariel R., Noah L. Nathan, and Julie K. Faller. 2015. “What Do I Need to Vote? Bureaucratic Discretion and Discrimination by Local Election Officials.” American Political Science Review 109(1):129–42. Whitley, Cameron T., and Thomas Dietz. 2018. “Turking Statistics: Student-generated Surveys Increase Student Engagement and Performance.” Teaching Sociology 46(1):44–53. Wilder, Esther Isabelle. 2009. “Responding to the Quantitative Literacy Gap among Students in Sociology Courses.” Teaching Sociology 37(2):151–70. Wisecup, Allison K. “Take It or Leave It: Students’ Attitudes about Research Methods.” Teaching Sociology 45(1):73–79. Wright, Bradley, Michael Wallace, John Bailey, and Allen Hyde. 2013. “Religious Affiliation and Ghoshal 323 Hiring Discrimination in New England: A Field Experiment.” Research in Social Stratification and Mobility 34(1):111–26. Wright, Bradley R. E., Michael Wallace, Annie Scola Wisnesky, Christopher M. Donnelly, Stacy Missari, and Christine Zozula. 2015. “Religion, Race, and Discrimination: A Field Experiment of How American Churches Welcome Newcomers.” Journal for the Scientific Study of Religion 54(2):185–204. Yinger, John. 1995. Closed Doors, Opportunities Lost: The Continuing Costs of Housing Discrimination. New York: Russell Sage Foundation. Zull, James Ellwood. 2002. The Art of Changing the Brain: Enriching Teaching by Exploring the Biology of Learning. Sterling, VA: Stylus. AUThOR BIOGRAPhy Raj Ghoshal is an assistant professor of sociology at Elon University in North Carolina. He studies race, politics, and culture in the United States. Prior research has been pub- lished in The American Sociological Review, Cultural Sociology, Theory & Society, and elsewhere. Website: www.rajghoshal.com.","libVersion":"0.0.0","langs":"","hash":"","size":0}